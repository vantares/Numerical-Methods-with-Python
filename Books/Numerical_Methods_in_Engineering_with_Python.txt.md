 #Numerical Methods in Engineering with Python

 Second Edition


 Numerical Methods in Engineering with Python, Second Edition, is a
 text for engineering students and a reference for practicing engineers,
 especially those who wish to explore Python. This new edition fea-
 tures 18 additional exercises and the addition of rational function in-
 terpolation. Brent’s method of root finding was replaced by Ridder’s
 method, and the Fletcher–Reeves method of optimization was dropped
 in favor of the downhill simplex method. Each numerical method is
 explained in detail, and its shortcomings are pointed out. The ex-
 amples that follow individual topics fall into two categories: hand
 computations that illustrate the inner workings of the method and
 small programs that show how the computer code is utilized in solv-
 ing a problem. This second edition also includes more robust com-
 puter code with each method, which is available on the book Web site
 (www.cambridge.org/kiusalaaspython). This code is made simple and
 easy to understand by avoiding complex bookkeeping schemes, while
 maintaining the essential features of the method.

 Jaan Kiusalaas is a Professor Emeritus in the Department of Engineer-
 ing Science and Mechanics at Pennsylvania State University. He has
 taught computer methods, including finite element and boundary el-
 ement methods, for more than 30 years. He is also the co-author of four
 other books – Engineering Mechanics: Statics, Engineering Mechanics:
 Dynamics, Mechanics of Materials, and an alternate version of this work
 with MATLAB® code.



 NUMERICAL
 METHODS IN
 ENGINEERING
 WITH PYTHON

 Second Edition


 Jaan Kiusalaas
 Pennsylvania State University


CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore,
São Paulo, Delhi, Dubai, Tokyo

Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK

Published in the United States of America by Cambridge University Press, New York

www.cambridge.org
Information on this title: www.cambridge.org/9780521191326
© Jaan Kiusalaas 2010


This publication is in copyright. Subject to statutory exception and to the
provision of relevant collective licensing agreements, no reproduction of any part
may take place without the written permission of Cambridge University Press.
First published in print format 2010

ISBN-13    978-0-511-67694-9       eBook (NetLibrary)
ISBN-13    978-0-521-19132-6       Hardback



Cambridge University Press has no responsibility for the persistence or accuracy
of urls for external or third-party internet websites referred to in this publication,
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.
P1: PHB


#Contents

* Preface to the First Edition
* Preface to the Second Edition

1. Introduction to Python 
   1. General Information 
   2. Core Python 
   3. Functions and Modules 
   4  Mathematics Modules 
   5. numpy Module
   6. Scoping of Variables
   7. Writing and Running Programs
2. Systems of Linear Algebraic Equations
   1. Introduction
   2. Gauss Elimination Method
   3. LU Decomposition Methods
   4. Problem Set 2.1.
   5. Symmetric and Banded Coefficient Matrices 
   6. Pivoting
   7. Problem Set 2.2.
   8. Matrix Inversion
   9. Iterative Methods 
   10. Problem Set 2.3.
   11. Other Methods 
3. Interpolation and Curve Fitting
   1. Introduction 
   2. Polynomial Interpolation.
   3. Interpolation with Cubic Spline
   4. Problem Set 3.1 
   5. Least-Squares Fit
   6. Problem Set 3.2 

4 Roots of Equations 
 1 Introduction. 
 2 Incremental Search Method 
 3 Method of Bisection.
 4 Methods Based on Linear Interpolation 
 5 Newton–Raphson Method 
 6 Systems of Equations
 Problem Set 4.1 
 4.7 Zeroes of Polynomials 
 Problem Set 4.2 

5 Numerical Differentiation 
 1 Introduction.
 2 Finite Difference Approximations.
 3 Richardson Extrapolation 
 4 Derivatives by Interpolation 
Problem Set 5.1 

6 Numerical Integration.
 1 Introduction. 
 2 Newton–Cotes Formulas 
 3 Romberg Integration.
Problem Set 6.1 
 4 Gaussian Integration
Problem Set 6.2 
 5 Multiple Integrals. 
Problem Set 6.3 

7 Initial Value Problems
 1 Introduction.
 2 Taylor Series Method 
 3 Runge–Kutta Methods
Problem Set 7.1 
 4 Stability and Stiffness
 5 Adaptive Runge–Kutta Method
 6 Bulirsch–Stoer Method
Problem Set 7.2 
 7 Other Methods

8 Two-Point Boundary Value Problems 
 1 Introduction.
 2 Shooting Method
Problem Set 8.1 
 3 Finite Difference Method 
Problem Set 8.2 

9 Symmetric Matrix Eigenvalue Problems
 1 Introduction.
 2 Jacobi Method
 3 Power and Inverse Power Methods
Problem Set 9.1 
 4 Householder Reduction to Tridiagonal Form
 5 Eigenvalues of Symmetric Tridiagonal Matrices
 Problem Set 9.2
 6 Other Methods 

10 Introduction to Optimization
 1 Introduction.
 2 Minimization along a Line
 3 Powell’s Method
 4 Downhill Simplex Method
 Problem Set 10.1
 5 Other Methods
 A1 Taylor Series 
 A2 Matrix Algebra. 


List of Program Modules (by Chapter) 416

Index.


          Preface to the First Edition




                    This book is targeted primarily toward engineers and engineering students of ad-
                    vanced standing (juniors, seniors, and graduate students). Familiarity with a com-
                    puter language is required; knowledge of engineering mechanics (statics, dynamics,
                    and mechanics of materials) is useful, but not essential.
                         The text attempts to place emphasis on numerical methods, not programming.
                    Most engineers are not programmers, but problem solvers. They want to know what
                    methods can be applied to a given problem, what are their strengths and pitfalls, and
                    how to implement them. Engineers are not expected to write computer code for basic
                    tasks from scratch; they are more likely to utilize functions and subroutines that have
                    been already written and tested. Thus, programming by engineers is largely confined
                    to assembling existing bits of code into a coherent package that solves the problem
                    at hand.
                         The “bit” of code is usually a function that implements a specific task. For the
                    user the details of the code are unimportant. What matters is the interface (what goes
                    in and what comes out) and an understanding of the method on which the algorithm
                    is based. Since no numerical algorithm is infallible, the importance of understanding
                    the underlying method cannot be overemphasized; it is, in fact, the rationale behind
                    learning numerical methods.
                         This book attempts to conform to the views outlined above. Each numerical
                    method is explained in detail and its shortcomings are pointed out. The examples
                    that follow individual topics fall into two categories: hand computations that illus-
                    trate the inner workings of the method, and small programs that show how the com-
                    puter code is utilized in solving a problem. Problems that require programming are
                    marked with .
                         The material consists of the usual topics covered in an engineering course on
                    numerical methods: solution of equations, interpolation and data fitting, numerical
                    differentiation and integration, and solution of ordinary differential equations and
                    eigenvalue problems. The choice of methods within each topic is tilted toward rel-
                    evance to engineering problems. For example, there is an extensive discussion of
                    symmetric, sparsely populated coefficient matrices in the solution of simultaneous
                    equations. In the same vein, the solution of eigenvalue problems concentrates on
                    methods that efficiently extract specific eigenvalues from banded matrices.

           viii
P1: PHB

CUUS884-Kiusalaas   CUUS884-FM     978 0 521 19132 6                                December 16, 2009    15:4




               ix     Preface to the First Edition

                          An important criterion used in the selection of methods was clarity. Algorithms
                     requiring overly complex bookkeeping were rejected regardless of their efficiency and
                     robustness. This decision, which was taken with great reluctance, is in keeping with
                     the intent to avoid emphasis on programming.
                          The selection of algorithms was also influenced by current practice. This disqual-
                     ified several well-known historical methods that have been overtaken by more recent
                     developments. For example, the secant method for finding roots of equations was
                     omitted as having no advantages over Ridder’s method. For the same reason, the mul-
                     tistep methods used to solve differential equations (e.g., Milne and Adams methods)
                     were left out in favor of the adaptive Runge–Kutta and Bulirsch–Stoer methods.
                          Notably absent is a chapter on partial differential equations. It was felt that
                     this topic is best treated by finite element or boundary element methods, which
                     are outside the scope of this book. The finite difference model, which is commonly
                     introduced in numerical methods texts, is just too impractical in handling multi-
                     dimensional boundary value problems.
                          As usual, the book contains more material than can be covered in a three-credit
                     course. The topics that can be skipped without loss of continuity are tagged with an
                     asterisk (*).
                          The programs listed in this book were tested with Python 2.5 under Win-
                     dows XP and Red Hat Linux. The source code is available on the Web site
                     http://www.cambridge.org/kiusalaaspython.
P1: PHB

CUUS884-Kiusalaas    CUUS884-FM      978 0 521 19132 6                                December 16, 2009     15:4




                    Preface to the Second Edition




                    The major change in the second edition is the replacement of NumArray (a Python
                    extension that implements array objects) with NumPy. As a consequence, most rou-
                    tines listed in the text required some code changes. The reason for the changeover is
                    the imminent discontinuance of support for NumArray and its predecessor Numeric.
                         We also took the opportunity to make a few changes in the material covered:

                     • Rational function interpolation was added to Chapter 3.
                     • Brent’s method of root finding in Chapter 4 was replaced by Ridder’s method.
                       The full-blown algorithm of Brent is a complicated procedure involving elaborate
                       bookkeeping (a simplified version was presented in the first edition). Ridder’s
                       method is as robust and almost as efficient as Brent’s method, but much easier
                       to understand.
                     • The Fletcher–Reeves method of optimization was dropped in favor of the down-
                       hill simplex method in Chapter 10. Fletcher–Reeves is a first-order method that
                       requires knowledge of the gradients of the merit function. Because there are few
                       practical problems where the gradients are available, the method is of limited
                       utility. The downhill simplex algorithm is a very robust (but slow) zero-order
                       method that often works where faster methods fail.




           x
P1: PHB

CUUS884-Kiusalaas   CUUS884-01       978 0 521 19132 6                                      December 16, 2009      15:4




              1      Introduction to Python




              1.1    General Information
                     Quick Overview
                     This chapter is not a comprehensive manual of Python. Its sole aim is to provide suf-
                     ficient information to give you a good start if you are unfamiliar with Python. If you
                     know another computer language, and we assume that you do, it is not difficult to
                     pick up the rest as you go.
                          Python is an object-oriented language that was developed in the late 1980s as
                     a scripting language (the name is derived from the British television show Monty
                     Python’s Flying Circus). Although Python is not as well known in engineering circles
                     as some other languages, it has a considerable following in the programming com-
                     munity – in fact, Python is used by more programmers than Fortran. Python may be
                     viewed as an emerging language, because it is still being developed and refined. In
                     the current state, it is an excellent language for developing engineering applications –
                     Python’s facilities for numerical computation are as good as those of Fortran or
                                R
                     MATLAB.
                          Python programs are not compiled into machine code, but are run by an in-
                     terpreter.1 The great advantage of an interpreted language is that programs can be
                     tested and debugged quickly, allowing the user to concentrate more on the princi-
                     ples behind the program and less on programming itself. Because there is no need
                     to compile, link, and execute after each correction, Python programs can be devel-
                     oped in a much shorter time than equivalent Fortran or C programs. On the negative
                     side, interpreted programs do not produce stand-alone applications. Thus, a Python
                     program can be run only on computers that have the Python interpreter installed.
                          Python has other advantages over mainstream languages that are important in a
                     learning environment:

                         • Python is open-source software, which means that it is free; it is included in most
                           Linux distributions.


                     1   The Python interpreter also compiles byte code, which helps to speed up execution somewhat.



               1
P1: PHB

CUUS884-Kiusalaas    CUUS884-01           978 0 521 19132 6                         December 16, 2009      15:4




           2        Introduction to Python

                     • Python is available for all major operating systems (Linux, Unix, Windows, Mac
                       OS, etc.). A program written on one system runs without modification on all
                       systems.
                     • Python is easier to learn and produces more readable code than do most lan-
                       guages.
                     • Python and its extensions are easy to install.

                        Development of Python was clearly influenced by Java and C++, but there is also
                    a remarkable similarity to MATLAB (another interpreted language, very popular in
                    scientific computing). Python implements the usual concepts of object-oriented lan-
                    guages such as classes, methods, and inheritance. We will not use object-oriented
                    programming in this text. The only object that we need is the N-dimensional array
                    available in the NumPy module (the NumPy module is discussed later in this
                    chapter).
                        To get an idea of the similarities between MATLAB and Python, let us look at the
                    codes written in the two languages for solution of simultaneous equations Ax = b by
                    Gauss elimination. Here is the function written in MATLAB:

                    function x] = gaussElimin(a,b)
                    n = length(b);
                    for k = 1:n-1
                          for i= k+1:n
                                  if a(i,k) ˜= 0
                                         lam = a(i,k)/a(k,k);
                                         a(i,k+1:n) = a(i,k+1:n) - lam*a(k,k+1:n);
                                         b(i)= b(i) - lam*b(k);
                                   end
                            end
                    end
                    for k = n:-1:1
                          b(k) = (b(k) - a(k,k+1:n)*b(k+1:n))/a(k,k);
                    end
                    x = b;

                          The equivalent Python function is:

                    from numpy import dot
                    def gaussElimin(a,b):
                          n = len(b)
                          for k in range(0,n-1):
                                  for i in range(k+1,n):
                                     if a[i,k] != 0.0:
                                            lam = a [i,k]/a[k,k]
                                            a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
                                            b[i] = b[i] - lam*b[k]
P1: PHB

CUUS884-Kiusalaas   CUUS884-01    978 0 521 19132 6                                   December 16, 2009    15:4




               3      1.2 Core Python

                          for k in range(n-1,-1,-1):
                                 b[k] = (b[k] - dot(a[k,k+1:n],b[k+1:n]))/a[k,k]
                          return b


                          The command from numpy import dot instructs the interpreter to load the
                     function dot (which computes the dot product of two vectors) from the module
                     numpy. The colon (:) operator, known as the slicing operator in Python, works the
                     same way it does in MATLAB and Fortran90 – it defines a slice of an array.
                          The statement for k = 1:n-1 in MATLAB creates a loop that is executed with
                     k = 1, 2, . . . , n − 1. The same loop appears in Python as for k in range(n-1).
                     Here the function range(n-1) creates the list [0, 1, . . . , n − 2]; k then loops over the
                     elements of the list. The differences in the ranges of k reflect the native offsets used
                     for arrays. In Python, all sequences have zero offset, meaning that the index of the first
                     element of the sequence is always 0. In contrast, the native offset in MATLAB is 1.
                          Also note that Python has no end statements to terminate blocks of code (loops,
                     subroutines, etc.). The body of a block is defined by its indentation; hence indenta-
                     tion is an integral part of Python syntax.
                          Like MATLAB, Python is case sensitive. Thus, the names n and N would represent
                     different objects.



                     Obtaining Python
                     The Python interpreter can be downloaded from the Python Language Website
                     www.python.org. It normally comes with a nice code editor called Idle that allows
                     you to run programs directly from the editor. For scientific programming, we also
                     need the NumPy module, which contains various tools for array operations. It is ob-
                     tainable from the NumPy home page http://numpy.scipy.org/. Both sites also
                     provide documentation for downloading. If you use Linux, it is very likely that Python
                     is already installed on your machine (but you must still download NumPy).
                          You should acquire other printed material to supplement the on-line doc-
                     umentation. A commendable teaching guide is Python by Chris Fehly (Peachpit
                     Press, CA, 2002). As a reference, Python Essential Reference by David M. Beazley
                     (New Riders Publishing, 2001) is recommended. By the time you read this, newer
                     editions may be available. A useful guide to NumPy is found at http://www.
                     scipy.org/Numpy Example List.




              1.2    Core Python
                     Variables
                     In most computer languages the name of a variable represents a value of a given type
                     stored in a fixed memory location. The value may be changed, but not the type. This
P1: PHB

CUUS884-Kiusalaas       CUUS884-01   978 0 521 19132 6                                 December 16, 2009       15:4




           4        Introduction to Python

                    it not so in Python, where variables are typed dynamically. The following interactive
                    session with the Python interpreter illustrates this (>>> is the Python prompt):

                    >>> b = 2             # b is integer type
                    >>> print b
                    2
                    >>> b = b*2.0      # Now b is float type
                    >>> print b
                    4.0


                         The assignment b = 2 creates an association between the name b and the in-
                    teger value 2. The next statement evaluates the expression b*2.0 and associates the
                    result with b; the original association with the integer 2 is destroyed. Now b refers to
                    the floating point value 4.0.
                         The pound sign (#) denotes the beginning of a comment – all characters between
                    # and the end of the line are ignored by the interpreter.


                    Strings
                    A string is a sequence of characters enclosed in single or double quotes. Strings are
                    concatenated with the plus (+) operator, whereas slicing (:) is used to extract a por-
                    tion of the string. Here is an example:

                    >>> string1 = ’Press return to exit’
                    >>> string2 = ’the program’
                    >>> print string1 + ’ ’ + string2             # Concatenation
                    Press return to exit the program
                    >>> print string1[0:12]                       # Slicing
                    Press return


                        A string is an immutable object – its individual characters cannot be modified
                    with an assignment statement, and it has a fixed length. An attempt to violate im-
                    mutability will result in TypeError, as shown here:

                    >>> s = ’Press return to exit’
                    >>> s[0] = ’p’
                    Traceback (most recent call last):
                        File ’’<pyshell#1>’’, line 1, in ?
                          s[0] = ’p’
                    TypeError: object doesn’t support item assignment



                    Tuples
                    A tuple is a sequence of arbitrary objects separated by commas and enclosed in
                    parentheses. If the tuple contains a single object, a final comma is required; for
P1: PHB

CUUS884-Kiusalaas   CUUS884-01    978 0 521 19132 6                                December 16, 2009    15:4




               5      1.2 Core Python

                     example, x = (2,). Tuples support the same operations as strings; they are also im-
                     mutable. Here is an example where the tuple rec contains another tuple (6,23,68):

                     >>> rec = (’Smith’,’John’,(6,23,68))                # This is a tuple
                     >>> lastName,firstName,birthdate = rec              # Unpacking the tuple
                     >>> print firstName
                     John
                     >>> birthYear = birthdate[2]
                     >>> print birthYear
                     68
                     >>> name = rec[1] + ’ ’ + rec[0]
                     >>> print name
                     John Smith
                     >>> print rec[0:2]
                     (’Smith’, ’John’)



                     Lists
                     A list is similar to a tuple, but it is mutable, so that its elements and length can be
                     changed. A list is identified by enclosing it in brackets. Here is a sampling of opera-
                     tions that can be performed on lists:

                     >>> a = [1.0, 2.0, 3.0]               # Create a list
                     >>> a.append(4.0)                     # Append 4.0 to list
                     >>> print a
                     [1.0, 2.0, 3.0, 4.0]
                     >>> a.insert(0,0.0)                   # Insert 0.0 in position 0
                     >>> print a
                     [0.0, 1.0, 2.0, 3.0, 4.0]
                     >>> print len(a)                      # Determine length of list
                     5
                     >>> a[2:4] = [1.0, 1.0, 1.0] # Modify selected elements
                     >>> print a
                     [0.0, 1.0, 1.0, 1.0, 1.0, 4.0]

                         If a is a mutable object, such as a list, the assignment statement b = a does not
                     result in a new object b, but simply creates a new reference to a. Thus any changes
                     made to b will be reflected in a. To create an independent copy of a list a, use the
                     statement c = a[:], as shown here:

                     >>> a = [1.0, 2.0, 3.0]
                     >>> b = a                        # ’b’ is an alias of ’a’
                     >>> b[0] = 5.0                   # Change ’b’
                     >>> print a
                     [5.0, 2.0, 3.0]                  # The change is reflected in ’a’
                     >>> c = a[:]                     # ’c’ is an independent copy of ’a’
P1: PHB

CUUS884-Kiusalaas       CUUS884-01   978 0 521 19132 6                               December 16, 2009      15:4




           6        Introduction to Python

                    >>> c[0] = 1.0                   # Change ’c’
                    >>> print a
                    [5.0, 2.0, 3.0]                  # ’a’ is not affected by the change

                         Matrices can be represented as nested lists with each row being an element of
                    the list. Here is a 3 × 3 matrix a in the form of a list:

                    >>> a = [[1, 2, 3], \
                                [4, 5, 6], \
                                [7, 8, 9]]
                    >>> print a[1]                   # Print second row (element 1)
                    [4, 5, 6]
                    >>> print a[1][2]                # Print third element of second row
                    6

                        The backslash (\) is Python’s continuation character. Recall that Python se-
                    quences have zero offset, so that a[0] represents the first row, a[1] the second row,
                    and so forth. With very few exceptions, we do not use lists for numerical arrays. It
                    is much more convenient to employ array objects provided by the NumPy module.
                    Array objects are discussed later.


                    Arithmetic Operators
                    Python supports the usual arithmetic operators:

                                                     +    Addition
                                                     −    Subtraction
                                                      ∗   Multiplication
                                                      /   Division
                                                     ∗∗   Exponentiation
                                                     %    Modular division

                    Some of these operators are also defined for strings and sequences as illustrated
                    here:

                    >>> s = ’Hello ’
                    >>> t = ’to you’
                    >>> a = [1, 2, 3]
                    >>> print 3*s                         # Repetition
                    Hello Hello Hello
                    >>> print 3*a                         # Repetition
                    [1, 2, 3, 1, 2, 3, 1, 2, 3]
                    >>> print a + [4, 5]                  # Append elements
                    [1, 2, 3, 4, 5]
                    >>> print s + t                       # Concatenation
                    Hello to you
                    >>> print 3 + s                       # This addition makes no sense
P1: PHB

CUUS884-Kiusalaas   CUUS884-01   978 0 521 19132 6                                     December 16, 2009   15:4




               7      1.2 Core Python

                     Traceback (most recent call last):
                         File ’’<pyshell#9>’’, line 1, in ?
                           print n + s
                     TypeError: unsupported operand types for +: ’int’ and ’str’


                          Python 2.0 and later versions also have augmented assignment operators, such as
                     a+ = b, that are familiar to the users of C. The augmented operators and the equiva-
                     lent arithmetic expressions are shown in the following table.

                                                          a += b     a = a + b
                                                          a -= b     a = a - b
                                                          a *= b     a = a*b
                                                          a /= b     a = a/b
                                                          a **= b    a = a**b
                                                          a %= b     a = a%b




                     Comparison Operators
                     The comparison (relational) operators return 1 for true and 0 for false. These opera-
                     tors are:

                                                 <          Less than
                                                 >          Greater than
                                                 <=         Less than or equal to
                                                 >=         Greater than or equal to
                                                     ==     Equal to
                                                     !=     Not equal to

                     Numbers of different type (integer, floating point, etc.) are converted to a common
                     type before the comparison is made. Otherwise, objects of different type are consid-
                     ered to be unequal. Here are a few examples:

                     >>> a = 2                # Integer
                     >>> b = 1.99             # Floating point
                     >>> c = ’2’              # String
                     >>> print a > b
                     1
                     >>> print a == c
                     0
                     >>> print (a > b) and (a != c)
                     1
                     >>> print (a > b) or (a == b)
                     1
P1: PHB

CUUS884-Kiusalaas    CUUS884-01      978 0 521 19132 6                                 December 16, 2009       15:4




           8        Introduction to Python

                    Conditionals
                    The if construct

                                                          if   condition:
                                                                block

                    executes a block of statements (which must be indented) if the condition returns true.
                    If the condition returns false, the block is skipped. The if conditional can be followed
                    by any number of elif (short for “else if”) constructs

                                                         elif    condition:
                                                               block

                    which work in the same manner. The else clause

                                                            else:
                                                                  block

                    can be used to define the block of statements that are to be executed if none of
                    the if-elif clauses is true. The function sign of a illustrates the use of the
                    conditionals:
                    def sign_of_a(a):
                         if a < 0.0:
                              sign = ’negative’
                         elif a > 0.0:
                              sign = ’positive’
                         else:
                              sign = ’zero’
                         return sign


                    a = 1.5
                    print ’a is ’ + sign_of_a(a)

                        Running the program results in the output
                    a is positive



                    Loops
                    The while construct

                                                         while condition:
                                                               block

                    executes a block of (indented) statements if the condition is true. After execution of
                    the block, the condition is evaluated again. If it is still true, the block is executed
P1: PHB

CUUS884-Kiusalaas   CUUS884-01     978 0 521 19132 6                                    December 16, 2009     15:4




               9      1.2 Core Python

                     again. This process is continued until the condition becomes false. The else clause

                                                                else:
                                                                    block

                     can be used to define the block of statements that are to be executed if the condition
                     is false. Here is an example that creates the list [1, 1/2, 1/3, . . .]:
                     nMax = 5
                     n = 1
                     a = []                        # Create empty list
                     while n < nMax:
                          a.append(1.0/n)          # Append element to list
                          n = n + 1
                     print a

                         The output of the program is

                     [1.0, 0.5, 0.33333333333333331, 0.25]

                         We met the for statement before in Section 1.1. This statement requires a tar-
                     get and a sequence (usually a list) over which the target loops. The form of the
                     construct is

                                                       for    tar get in sequence:
                                                             block

                     You may add an else clause that is executed after the for loop has finished. The
                     previous program could be written with the for construct as
                     nMax = 5
                     a = []
                     for n in range(1,nMax):
                          a.append(1.0/n)
                     print a

                          Here n is the target and the list [1, 2, . . . , nMax − 1], created by calling the range
                     function, is the sequence.
                          Any loop can be terminated by the break statement. If there is an else cause
                     associated with the loop, it is not executed. The following program, which searches
                     for a name in a list, illustrates the use of break and else in conjunction with a for
                     loop:
                     list = [’Jack’, ’Jill’, ’Tim’, ’Dave’]
                     name = eval(raw_input(’Type a name: ’))                  # Python input prompt
                     for i in range(len(list)):
                          if list[i] == name:
                                 print name,’is number’,i + 1,’on the list’
                                 break
P1: PHB

CUUS884-Kiusalaas    CUUS884-01      978 0 521 19132 6                                 December 16, 2009       15:4




           10       Introduction to Python

                    else:
                         print name,’is not on the list’

                        Here are the results of two searches:

                    Type a name: ’Tim’
                    Tim is number 3 on the list


                    Type a name: ’June’
                    June is not on the list

                        The

                                                          continue


                    statement allows us to skip a portion of the statements in an iterative loop. If the
                    interpreter encounters the continue statement, it immediately returns to the begin-
                    ning of the loop without executing the statements below continue. The following
                    example compiles a list of all numbers between 1 and 99 that are divisible by 7.

                    x = []                           # Create an empty list
                    for i in range(1,100):
                       if i%7!= 0: continue # If not divisible by 7, skip rest of loop
                       x.append(i)                   # Append i to the list
                    print x



                        The printout from the program is

                    [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98]



                    Type Conversion
                    If an arithmetic operation involves numbers of mixed types, the numbers are au-
                    tomatically converted to a common type before the operation is carried out. Type
                    conversions can also be achieved by the following functions:

                                        int(a)             Converts a to integer
                                        long(a)            Converts a to long integer
                                        float(a)           Converts a to floating point
                                        complex(a)         Converts to complex a + 0 j
                                        complex(a,b)       Converts to complex a + bj

                         The foregoing functions also work for converting strings to numbers as long as
                    the literal in the string represents a valid number. Conversion from a float to an inte-
                    ger is carried out by truncation, not by rounding off. Here are a few examples:

                    >>> a = 5
                    >>> b = -3.6
P1: PHB

CUUS884-Kiusalaas   CUUS884-01    978 0 521 19132 6                                  December 16, 2009   15:4




               11     1.2 Core Python

                     >>> d = ’4.0’
                     >>> print a + b
                     1.4
                     >>> print int(b)
                     -3
                     >>> print complex(a,b)
                     (5-3.6j)
                     >>> print float(d)
                     4.0
                     >>> print int(d)        # This fails: d is not Int type
                     Traceback (most recent call last):
                        File ’’<pyshell#7>’’, line 1, in ?
                           print int(d)
                     ValueError: invalid literal for int(): 4.0



                     Mathematical Functions
                     Core Python supports only a few mathematical functions:

                                        abs(a)            Absolute value of a
                                        max(sequence)     Largest element of sequence
                                        min(sequence)     Smallest element of sequence
                                        round(a,n)        Round a⎧to n decimal places
                                                                  ⎪ −1 if a < b
                                                                  ⎨
                                        cmp(a,b)          Returns     0 if a = b
                                                                  ⎪
                                                                  ⎩
                                                                      1 if a > b

                     The majority of mathematical functions are available in the math module.

                     Reading Input
                     The intrinsic function for accepting user input is

                                                      raw input(prompt)


                     It displays the prompt and then reads a line of input that is converted to a string. To
                     convert the string into a numerical value, use the function

                                                         eval(string)


                     The following program illustrates the use of these functions:

                     a = raw_input(’Input a: ’)
                     print a, type(a)                   # Print a and its type
                     b = eval(a)
                     print b,type(b)                    # Print b and its type
P1: PHB

CUUS884-Kiusalaas    CUUS884-01      978 0 521 19132 6                                   December 16, 2009     15:4




           12       Introduction to Python

                       The function type(a) returns the type of the object a; it is a very useful tool in
                    debugging. The program was run twice with the following results:
                    Input a: 10.0
                    10.0 <type ’str’>
                    10.0 <type ’float’>


                    Input a: 11**2
                    11**2 <type ’str’>
                    121 <type ’int’>

                        A convenient way to input a number and assign it to the variable a is

                                              a = eval(raw input(prompt))




                    Printing Output
                    Output can be displayed with the print statement:

                                                    printobject1, object2, . . .


                    which converts object1, object2, and so on to strings and prints them on the same line,
                    separated by spaces. The newline character ’\n’ can be used to force a new line. For
                    example,

                    >>> a = 1234.56789
                    >>> b = [2, 4, 6, 8]
                    >>> print a,b
                    1234.56789 [2, 4, 6, 8]
                    >>> print ’a =’,a, ’\nb =’,b
                    a = 1234.56789
                    b = [2, 4, 6, 8]

                        The modulo operator (%) can be used to format a tuple. The form of the conver-
                    sion statement is

                                             ’%format1 %format2        · · · ’ % tuple

                    where format1, format2 · · · are the format specifications for each object in the tuple.
                    Typically used format specifications are:

                                                wd       Integer
                                                w.d f    Floating point notation
                                                w .d e   Exponential notation

                    where w is the width of the field and d is the number of digits after the decimal point.
                    The output is right-justified in the specified field and padded with blank spaces
P1: PHB

CUUS884-Kiusalaas   CUUS884-01     978 0 521 19132 6                                   December 16, 2009   15:4




               13     1.2 Core Python

                     (there are provisions for changing the justification and padding). Here are a couple
                     of examples:

                     >>> a = 1234.56789
                     >>> n = 9876
                     >>> print ’%7.2f’ % a
                     1234.57
                     >>> print ’n = %6d’ % n             # Pad with spaces
                     n =    9876
                     >>> print ’n = %06d’ % n # Pad with zeroes
                     n = 009876
                     >>> print ’%12.4e %6d’ % (a,n)
                      1.2346e+003         9876


                     Opening and Closing a File
                     Before a data file can be accessed, you must create a file object with the command

                                                file object = open(filename, action)

                     where filename is a string that specifies the file to be opened (including its path if
                     necessary) and action is one of the following strings:

                                 ’r’    Read from an existing file.
                                 ’w’    Write to a file. If filename does not exist, it is created.
                                 ’a’    Append to the end of the file.
                                 ’r+’   Read to and write from an existing file.
                                 ’w+’   Same as ’r+’, but filename is created if it does not exist.
                                 ’a+’   Same as ’w+’, but data is appended to the end of the file.

                         It is good programming practice to close a file when access to it is no longer re-
                     quired. This can be done with the method

                                                         file object.close()


                     Reading Data from a File
                     There are three methods for reading data from a file. The method

                                                         file object.read(n)

                     reads n characters and returns them as a string. If n is omitted, all the characters in
                     the file are read.
                          If only the current line is to be read, use

                                                       file object.readline(n)
P1: PHB

CUUS884-Kiusalaas     CUUS884-01      978 0 521 19132 6                                       December 16, 2009   15:4




           14       Introduction to Python

                    which reads n characters from the line. The characters are returned in a string that
                    terminates in the newline character \n. Omission of n causes the entire line to be
                    read.
                        All the lines in a file can be read using

                                                     file object.readlines()

                    This returns a list of strings, each string being a line from the file ending with the
                    newline character.

                    Writing Data to a File
                    The method

                                                          file object.write()

                    writes a string to a file, whereas

                                                    file object.writelines()

                    is used to write a list of strings. Neither method appends a newline character to the
                    end of a line.
                         The print statement can also be used to write to a file by redirecting the output
                    to a file object:
                                          print    >> file object , object1, object2, . . .
                    Apart from the redirection, this statement works just like the regular print com-
                    mand.

                    Error Control
                    When an error occurs during execution of a program, an exception is raised and the
                    program stops. Exceptions can be caught with try and except statements:

                                                          try:
                                                             do something
                                                          except error :
                                                             do something else

                    where error is the name of a built-in Python exception. If the exception error is not
                    raised, the try block is executed; otherwise, the execution passes to the except
                    block. All exceptions can be caught by omitting error from the except statement.
                         Here is a statement that raises the exception ZeroDivisionError:
                    >>> c = 12.0/0.0
                    Traceback (most recent call last):
                      File ’’<pyshell#0>’’, line 1, in ?
                         c = 12.0/0.0
                    ZeroDivisionError: float division
P1: PHB

CUUS884-Kiusalaas   CUUS884-01     978 0 521 19132 6                               December 16, 2009   15:4




               15     1.3 Functions and Modules

                           This error can be caught by

                     try:
                            c = 12.0/0.0
                     except ZeroDivisionError:
                            print ’Division by zero’




              1.3    Functions and Modules
                     Functions
                     The structure of a Python function is

                                              def    func name(param1, param2,. . .):
                                                    statements
                                                    return return values


                     where param1, param2,. . . are the parameters. A parameter can be any Python ob-
                     ject, including a function. Parameters may be given default values, in which case the
                     parameter in the function call is optional. If the return statement or return values
                     are omitted, the function returns the null object.
                          The following example computes the first two derivatives of arctan(x) by finite
                     differences:

                     from math import atan
                     def     finite_diff(f,x,h=0.0001):            # h has a default value
                            df =(f(x+h) - f(x-h))/(2.0*h)
                            ddf =(f(x+h) - 2.0*f(x) + f(x-h))/h**2
                            return df,ddf


                     x = 0.5
                     df,ddf = finite_diff(atan,x)                # Uses default value of h
                     print ’First derivative             =’,df
                     print ’Second derivative =’,ddf


                         Note that atan is passed to finite diff as a parameter. The output from the
                     program is

                     First derivative         = 0.799999999573
                     Second derivative = -0.639999991892


                         The number of input parameters in a function definition may be left arbitrary.
                     For example, in the function definition

                                                       def func(x1,x2,*x3)
P1: PHB

CUUS884-Kiusalaas     CUUS884-01     978 0 521 19132 6                                   December 16, 2009       15:4




           16       Introduction to Python

                    x1 and x2 are the usual parameters, also called positional parameters, whereas x3 is a
                    tuple of arbitrary length containing the excess parameters. Calling this function with

                                                         func(a,b,c,d,e)

                    results in the following correspondence between the parameters:

                                         a   ←→ x1, b ←→ x2, (c,d,e) ←→ x3

                    The positional parameters must always be listed before the excess parameters.
                        If a mutable object, such as a list, is passed to a function where it is modified, the
                    changes will also appear in the calling program. Here is an example:

                    def squares(a):
                         for i in range(len(a)):
                              a[i] = a[i]**2


                    a = [1, 2, 3, 4]
                    squares(a)
                    print a

                         The output is

                    [1, 4, 9, 16]



                    Lambda Statement
                    If the function has the form of an expression, it can be defined with the lambda state-
                    ment

                                   func name = lambda param1, param2,...: expression

                    Multiple statements are not allowed.
                       Here is an example:

                    >>> c = lambda x,y : x**2 + y**2
                    >>> print c(3,4)
                    25



                    Modules
                    It is sound practice to store useful functions in modules. A module is simply a file
                    where the functions reside; the name of the module is the name of the file. A module
                    can be loaded into a program by the statement

                                                from     module name import *

                    Python comes with a large number of modules containing functions and methods
                    for various tasks. Some of the modules are described briefly in the following section.
P1: PHB

CUUS884-Kiusalaas   CUUS884-01    978 0 521 19132 6                                  December 16, 2009   15:4




               17     1.4 Mathematics Modules

                     Additional modules, including graphics packages, are available for downloading on
                     the Web.



              1.4    Mathematics Modules
                     math   Module
                     Most mathematical functions are not built into core Python, but are available by load-
                     ing the math module. There are three ways of accessing the functions in a module.
                     The statement

                                                      from math import *


                     loads all the function definitions in the math module into the current function or
                     module. The use of this method is discouraged because it not only is wasteful, but
                     can also lead to conflicts with definitions loaded from other modules.
                         You can load selected definitions by

                                              from math import     func1, func2, . . .

                     as illustrated here:

                     >>> from math import log,sin
                     >>> print log(sin(0.5))
                     -0.735166686385

                        The third method, which is used by the majority of programmers, is to make the
                     module available by

                                                         import math


                     The module can then be accessed by using the module name as a prefix:

                     >>> import math
                     >>> print math.log(math.sin(0.5))
                     -0.735166686385

                         The contents of a module can be printed by calling dir(module). Here is how to
                     obtain a list of the functions in the math module:

                     >>> import math
                     >>> dir(math)
                     [’__doc__’, ’__name__’, ’acos’, ’asin’, ’atan’,
                      ’atan2’, ’ceil’, ’cos’, ’cosh’, ’e’, ’exp’, ’fabs’,
                      ’floor’, ’fmod’, ’frexp’, ’hypot’, ’ldexp’, ’log’,
                      ’log10’, ’modf’, ’pi’, ’pow’, ’sin’, ’sinh’, ’sqrt’,
                      ’tan’, ’tanh’]
P1: PHB

CUUS884-Kiusalaas       CUUS884-01       978 0 521 19132 6                                        December 16, 2009         15:4




           18       Introduction to Python

                        Most of these functions are familiar to programmers. Note that the module in-
                    cludes two constants: π and e.

                    cmath      Module
                    The cmath module provides many of the functions found in the math module, but
                    these accept complex numbers. The functions in the module are:

                    [’__doc__’, ’__name__’, ’acos’, ’acosh’, ’asin’, ’asinh’,
                        ’atan’, ’atanh’, ’cos’, ’cosh’, ’e’, ’exp’, ’log’,
                        ’log10’, ’pi’, ’sin’, ’sinh’, ’sqrt’, ’tan’, ’tanh’]

                          Here are examples of complex arithmetic:

                    >>> from cmath import sin
                    >>> x = 3.0 -4.5j
                    >>> y = 1.2 + 0.8j
                    >>> z = 0.8
                    >>> print x/y
                    (-2.56205313375e-016-3.75j)
                    >>> print sin(x)
                    (6.35239299817+44.5526433649j)
                    >>> print sin(z)
                    (0.7173560909+0j)



          1.5       numpy      Module
                    General Information
                    The NumPy module2 is not a part of the standard Python release. As pointed out
                    before, it must be obtained separately and installed (the installation is very easy).
                    The module introduces array objects that are similar to lists, but can be manipulated
                    by numerous functions contained in the module. The size of an array is immutable,
                    and no empty elements are allowed.
                        The complete set of functions in numpy is far too long to be printed in its entirety.
                    The following list is limited to the most commonly used functions:
                          [’complex’, ’float’, ’abs’, ’append’, arccos’,
                          ’arccosh’, ’arcsin’, ’arcsinh’, ’arctan’, ’arctan2’,
                          ’arctanh’, ’argmax’, ’argmin’, ’cos’, ’cosh’, ’diag’,
                          ’diagonal’, ’dot’, ’e’, ’exp’, ’floor’, ’identity’,
                          ’inner, ’inv’, ’log’, ’log10’, ’max’, ’min’,
                          ’ones’,’outer’, ’pi’, ’prod’ ’sin’, ’sinh’, ’size’,
                          ’solve’,’sqrt’, ’sum’, ’tan’, ’tanh’, ’trace’,
                          ’transpose’, ’zeros’, ’vectorize’]
                    2   NumPy is the successor of older Python modules called Numeric and NumArray. Their interfaces
                        and capabilities are very similar. Although Numeric and NumArray are still available, they are no
                        longer supported.
P1: PHB

CUUS884-Kiusalaas   CUUS884-01       978 0 521 19132 6                                  December 16, 2009   15:4




               19     1.5   numpy   Module

                     Creating an Array
                     Arrays can be created in several ways. One of them is to use the array function to
                     turn a list into an array:

                                                 array(list,dtype =    type specification)

                     Here are two examples of creating a 2 × 2 array with floating-point elements:

                     >>> from numpy import array,float
                     >>> a = array([[2.0, -1.0],[-1.0, 3.0]])
                     >>> print a
                     [[ 2. -1.]
                      [-1.      3.]]
                     >>> b = array([[2, -1],[-1, 3]],dtype = float)
                     >>> print b
                     [[ 2. -1.]
                      [-1.      3.]]

                            Other available functions are

                                          zeros((dim1,dim2),dtype =         type specification)

                     which creates a dim1 × dim2 array and fills it with zeroes, and

                                           ones((dim1,dim2),dtype =        type specification)

                     which fills the array with ones. The default type in both cases is float.
                        Finally, there is the function

                                                         arange(from,to,increment)


                     which works just like the range function, but returns an array rather than a list. Here
                     are examples of creating arrays:
                     >>> from numpy import *
                     >>> print arange(2,10,2)
                     [2 4 6 8]
                     >>> print arange(2.0,10.0,2.0)
                     [ 2.      4.    6.    8.]
                     >>> print zeros(3)
                     [ 0.      0.    0.]
                     >>> print zeros((3),dtype=int)
                     [0 0 0]
                     >>> print ones((2,2))
                     [[ 1.      1.]
                      [ 1.      1.]]
P1: PHB

CUUS884-Kiusalaas     CUUS884-01         978 0 521 19132 6                               December 16, 2009       15:4




           20       Introduction to Python

                    Accessing and Changing Array Elements
                    If a is a rank-2 array, then a[i,j] accesses the element in row i and column j,
                    whereas a[i] refers to row i. The elements of an array can be changed by assign-
                    ment:

                    >>> from numpy import *
                    >>> a = zeros((3,3),dtype=int)
                    >>> print a
                    [[0 0 0]
                     [0 0 0]
                     [0 0 0]]
                    >>> a[0] = [2,3,2]                 # Change a row
                    >>> a[1,1] = 5                     # Change an element
                    >>> a[2,0:2] = [8,-3]              # Change part of a row
                    >>> print a
                    [[ 2    3     2]
                     [ 0    5     0]
                     [ 8 -3       0]]



                    Operations on Arrays
                    Arithmetic operators work differently on arrays than they do on tuples and lists – the
                    operation is broadcast to all the elements of the array; that is, the operation is applied
                    to each element in the array. Here are examples:

                    >>> from numpy import array
                    >>> a = array([0.0, 4.0, 9.0, 16.0])
                    >>> print a/16.0
                    [ 0.              0.25       0.5625      1.    ]
                    >>> print a - 4.0
                    [ -4.        0.     5.      12.]

                        The mathematical functions available in NumPy are also broadcast:

                    >>> from numpy import array,sqrt,sin
                    >>> a = array([1.0, 4.0, 9.0, 16.0])
                    >>> print sqrt(a)
                    [ 1.    2.     3.     4.]
                    >>> print sin(a)
                    [ 0.84147098 -0.7568025                  0.41211849 -0.28790332]

                        Functions imported from the math module will work on the individual elements,
                    of course, but not on the array itself. Here is an example:

                    >>> from numpy import array
                    >>> from math import sqrt
P1: PHB

CUUS884-Kiusalaas   CUUS884-01         978 0 521 19132 6                          December 16, 2009   15:4




               21     1.5    numpy    Module

                     >>> a = array([1.0, 4.0, 9.0, 16.0])
                     >>> print sqrt(a[1])
                     2.0
                     >>> print sqrt(a)
                     Traceback (most recent call last):

                            ..
                             .

                     TypeError: only length-1 arrays can be converted to Python scalars



                     Array Functions
                     There are numerous functions in NumPy that perform array operations and other
                     useful tasks. Here are a few examples:

                     >>> from numpy import *
                     >>> A = array([[4,-2,1],[-2,4,-2],[1,-2,3]],dtype=float)
                     >>> b = array([1,4,3],dtype=float)
                     >>> print diagonal(A)                 # Principal diagonal
                     [ 4.        4.    3.]
                     >>> print diagonal(A,1)               # First subdiagonal
                     [-2. -2.]
                     >>> print trace(A)                    # Sum of diagonal elements
                     11.0
                     >>> print argmax(b)                   # Index of largest element
                     1
                     >>> print argmin(A,axis=0)            # Indecies of smallest col. elements
                     [1 0 1]
                     >>> print identity(3)                 # Identity matrix
                     [[ 1.        0.    0.]
                      [ 0.        1.    0.]
                      [ 0.        0.    1.]]

                          There are three functions in NumPy that compute array products. They are illus-
                     trated by the program listed below For more details, see Appendix A2.

                     from numpy import *
                     x = array([7,3])
                     y = array([2,1])
                     A = array([[1,2],[3,2]])
                     B = array([[1,1],[2,2]])


                     # Dot product
                     print "dot(x,y) =\n",dot(x,y)                # {x}.{y}
                     print "dot(A,x) =\n",dot(A,x)                # [A]{x}
                     print "dot(A,B) =\n",dot(A,B)                # [A][B]
P1: PHB

CUUS884-Kiusalaas    CUUS884-01      978 0 521 19132 6                             December 16, 2009     15:4




           22       Introduction to Python

                    # Inner product
                    print "inner(x,y) =\n",inner(x,y)          # {x}.{y}
                    print "inner(A,x) =\n",inner(A,x)          # [A]{x}
                    print "inner(A,B) =\n",inner(A,B)          # [A][B_transpose]


                    # Outer product
                    print "outer(x,y) =\n",outer(x,y)
                    print "outer(A,x) =\n",outer(A,x)
                    print "Outer(A,B) =\n",outer(A,B)


                         The output of the program is

                    dot(x,y) =
                    17
                    dot(A,x) =
                    [13 27]
                    dot(A,B) =
                    [[5 5]
                     [7 7]]
                    inner(x,y) =
                    17
                    inner(A,x) =
                    [13 27]
                    inner(A,B) =
                    [[ 3     6]
                     [ 5 10]]
                    outer(x,y) =
                    [[14     7]
                     [ 6     3]]
                    outer(A,x) =
                    [[ 7     3]
                     [14     6]
                     [21     9]
                     [14     6]]
                    Outer(A,B) =
                    [[1 1 2 2]
                     [2 2 4 4]
                     [3 3 6 6]
                     [2 2 4 4]]




                    Linear Algebra Module
                    NumPy comes with a linear algebra module called linalg that contains routine tasks
                    such as matrix inversion and solution of simultaneous equations. For example:
P1: PHB

CUUS884-Kiusalaas   CUUS884-01       978 0 521 19132 6                                  December 16, 2009   15:4




               23     1.5   numpy   Module

                     >>> from numpy import array
                     >>> from numpy.linalg import inv,solve
                     >>> A = array([[ 4.0, -2.0,                  1.0], \
                                             [-2.0,      4.0, -2.0], \
                                             [ 1.0, -2.0,       3.0]])
                     >>> b = array([1.0, 4.0, 2.0])
                     >>> print inv(A)                                            # Matrix inverse
                     [[ 0.33333333           0.16666667      0.             ]
                      [ 0.16666667           0.45833333      0.25           ]
                      [ 0.                   0.25            0.5            ]]
                     >>> print solve(A,b)                                        # Solve [A]{x} = {b}
                     [ 1. ,      2.5,     2. ]


                     Copying Arrays
                     We explained before that if a is a mutable object, such as a list, the assignment state-
                     ment b = a does not result in a new object b, but simply creates a new reference to
                     a, called a deep copy. This also applies to arrays. To make an independent copy of an
                     array a, use the copy method in the NumPy module:

                                                             b = a.copy()



                     Vectorizing Algorithms
                     Sometimes the broadcasting properties of the mathematical functions in the NumPy
                     module can be utilized to replace loops in the code. This procedure is known as vec-
                     torization. Consider, for example, the expression
                                                               100
                                                                     iπ      iπ
                                                          s=             sin
                                                                     100     100
                                                               i=0

                     The direct approach is to evaluate the sum in a loop, resulting in the following “scalar”
                     code:

                     from math import sqrt,sin,pi
                     x=0.0; sum = 0.0
                     for i in range(0,101):
                            sum = sum + sqrt(x)*sin(x)
                            x = x + 0.01*pi
                     print sum

                            The vectorized version of algorithm is

                     from numpy import sqrt,sin,arange
                     from math import pi
                     x = arrange(0.0,1.001*pi,0.01*pi)
                     print sum(sqrt(x)*sin(x))
P1: PHB

CUUS884-Kiusalaas    CUUS884-01     978 0 521 19132 6                                 December 16, 2009       15:4




           24       Introduction to Python

                         Note that the first algorithm uses the scalar versions of sqrt and sin functions
                    in the math module, whereas the second algorithm imports these functions from the
                    numpy. The vectorized algorithm is faster, but uses more memory.




          1.6       Scoping of Variables

                    Namespace is a dictionary that contains the names of the variables and their values.
                    The namespaces are automatically created and updated as a program runs. There are
                    three levels of namespaces in Python:

                     • Local namespace, which is created when a function is called. It contains the vari-
                       ables passed to the function as arguments and the variables created within the
                       function. The namespace is deleted when the function terminates. If a variable
                       is created inside a function, its scope is the function’s local namespace. It is not
                       visible outside the function.
                     • A global namespace is created when a module is loaded. Each module has its own
                       namespace. Variables assigned in a global namespace are visible to any function
                       within the module.
                     • Built-in namespace is created when the interpreter starts. It contains the func-
                       tions that come with the Python interpreter. These functions can be accessed by
                       any program unit.

                         When a name is encountered during execution of a function, the interpreter tries
                    to resolve it by searching the following in the order shown: (1) local namespace,
                    (2) global namespace, and (3) built-in namespace. If the name cannot be resolved,
                    Python raises a NameError exception.
                         Because the variables residing in a global namespace are visible to functions
                    within the module, it is not necessary to pass them to the functions as arguments
                    (although is good programming practice to do so), as the following program illus-
                    trates:

                    def divide():
                          c = a/b
                          print ’a/b =’,c


                    a = 100.0
                    b = 5.0
                    divide()


                    >>>
                    a/b = 20.0

                        Note that the variable c is created inside the function divide and is thus not
                    accessible to statements outside the function. Hence an attempt to move the print
                    statement out of the function fails:
P1: PHB

CUUS884-Kiusalaas   CUUS884-01    978 0 521 19132 6                                December 16, 2009    15:4




               25     1.7 Writing and Running Programs

                     def divide():
                           c = a/b


                     a = 100.0
                     b = 5.0
                     divide()
                     print ’a/b =’,c


                     >>>
                     Traceback (most recent call last):
                        File ’’C:\Python22\scope.py’’, line 8, in ?
                           print c
                     NameError: name ’c’ is not defined




              1.7    Writing and Running Programs

                     When the Python editor Idle is opened, the user is faced with the prompt >>>, in-
                     dicating that the editor is in interactive mode. Any statement typed into the editor is
                     immediately processed upon pressing the enter key. The interactive mode is a good
                     way to learn the language by experimentation and to try out new programming ideas.
                          Opening a new window places Idle in the batch mode, which allows typing and
                     saving of programs. One can also use a text editor to enter program lines, but Idle
                     has Python-specific features, such as color coding of keywords and automatic inden-
                     tation, that make work easier. Before a program can be run, it must be saved as a
                     Python file with the .py extension, for example, myprog.py. The program can then
                     be executed by typing python myprog.py; in Windows, double-clicking on the pro-
                     gram icon will also work. But beware: the program window closes immediately after
                     execution, before you get a chance to read the output. To prevent this from happen-
                     ing, conclude the program with the line


                                                 raw input(’press return’)


                     Double-clicking the program icon also works in Unix and Linux if the first line
                     of the program specifies the path to the Python interpreter (or a shell script
                     that provides a link to Python). The path name must be preceded by the sym-
                     bols #!. On my computer the path is /usr/bin/python, so that all my programs
                     start with the line #!/usr/bin/python. On multiuser systems the path is usually
                     /usr/local/bin/python.
                          When a module is loaded into a program for the first time with the import state-
                     ment, it is compiled into bytecode and written in a file with the extension .pyc.
                     The next time the program is run, the interpreter loads the bytecode rather than the
                     original Python file. If in the meantime changes have been made to the module, the
P1: PHB

CUUS884-Kiusalaas    CUUS884-01      978 0 521 19132 6                              December 16, 2009    15:4




           26       Introduction to Python

                    module is automatically recompiled. A program can also be run from Idle using the
                    Run/Run Module menu.
                        It is a good idea to document your modules by adding a docstring at the begin-
                    ning of each module. The docstring, which is enclosed in triple quotes, should ex-
                    plain what the module does. Here is an example that documents the module error
                    (we use this module in several of our programs):

                    ## module error
                    ’’’ err(string).
                          Prints ’string’ and terminates program.
                    ’’’
                    import sys
                    def err(string):
                          print string
                          raw_input(’Press return to exit’)
                          sys.exit()

                          The docstring of a module can be printed with the statement


                                                 printmodule   name. doc


                    For example, the docstring of error is displayed by

                    >>> import error
                    >>> print error.__doc__
                     err(string).
                          Prints ’string’ and terminates program.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02       978 0 521 19132 6                                          December 16, 2009    15:4




              2      Systems of Linear Algebraic Equations




                                                Solve the simultaneous equations Ax = b




              2.1    Introduction

                     In this chapter we look at the solution of n linear, algebraic equations in n unknowns.
                     It is by far the longest and arguably the most important topic in the book. There is a
                     good reason for this – it is almost impossible to carry out numerical analysis of any
                     sort without encountering simultaneous equations. Moreover, equation sets arising
                     from physical problems are often very large, consuming a lot of computational re-
                     sources. It is usually possible to reduce the storage requirements and the run time
                     by exploiting special properties of the coefficient matrix, such as sparseness (most
                     elements of a sparse matrix are zero). Hence, there are many algorithms dedicated to
                     the solution of large sets of equations, each one being tailored to a particular form of
                     the coefficient matrix (symmetric, banded, sparse, etc.). A well-known collection of
                     these routines is LAPACK – Linear Algebra PACKage, originally written in Fortran77.1
                           We cannot possibly discuss all the special algorithms in the limited space avail-
                     able. The best we can do is to present the basic methods of solution, supplemented
                     by a few useful algorithms for banded and sparse coefficient matrices.


                     Notation
                     A system of algebraic equations has the form

                                                     A 11 x1 + A 12 x2 + · · · + A 1n xn = b1

                                                     A 21 x1 + A 22 x2 + · · · + A 2n xn = b2                       (2.1)
                                                                                          ..
                                                                                           .

                                                    A n1 x1 + A n2 x2 + · · · + A nn xn = bn

                     1   LAPACK is the successor of LINPACK, a 1970s and 80s collection of Fortran subroutines.

               27
P1: PHB

CUUS884-Kiusalaas     CUUS884-02     978 0 521 19132 6                                 December 16, 2009       15:4




           28       Systems of Linear Algebraic Equations

                    where the coefficients A ij and the constants b j are known, and xi represent the un-
                    knowns. In matrix notation the equations are written as
                                          ⎡                          ⎤⎡ ⎤ ⎡ ⎤
                                            A 11 A 12 · · · A 1n        x1       b1
                                          ⎢A             · · ·       ⎥ ⎢x ⎥ ⎢b ⎥
                                          ⎢ 21     A  22       A  2n ⎥ ⎢ 2 ⎥    ⎢ 2⎥
                                          ⎢ .                    .. ⎥  ⎢ ⎥ ⎢ ⎥
                                          ⎢ .        ..   ..         ⎥ ⎢ .. ⎥ = ⎢ .. ⎥               (2.2)
                                          ⎣ .         .      .    . ⎦⎣ . ⎦ ⎣ . ⎦
                                            A n1 A n2 · · · A nn        xn       bn
                    or, simply,

                                                            Ax = b                                     (2.3)

                         A particularly useful representation of the equations for computational purposes
                    is the augmented coefficient matrix obtained by adjoining the constant vector b to the
                    coefficient matrix A in the following fashion:
                                                       ⎡                            ⎤
                                                         A 11 A 12 · · · A 1n b1
                                                       ⎢A                           ⎥
                                                       ⎢ 21 A 22 · · · A 2n b2 ⎥
                                                A b =⎢ ⎢ ..      ..   ..    ..   .. ⎥
                                                                                    ⎥                (2.4)
                                                       ⎣ .        .      .   .    .⎦
                                                         A n1 A n2 · · · A n3 bn


                    Uniqueness of Solution
                    A system of n linear equations in n unknowns has a unique solution, provided that
                    the determinant of the coefficient matrix is nonsingular; that is, |A| = 0. The rows and
                    columns of a nonsingular matrix are linearly independent in the sense that no row (or
                    column) is a linear combination of other rows (or columns).
                        If the coefficient matrix is singular, the equations may have an infinite number of
                    solutions, or no solutions at all, depending on the constant vector. As an illustration,
                    take the equations

                                                   2x + y = 3    4x + 2y = 6

                    Because the second equation can be obtained by multiplying the first equation by 2,
                    any combination of x and y that satisfies the first equation is also a solution of the
                    second equation. The number of such combinations is infinite. On the other hand,
                    the equations

                                                   2x + y = 3    4x + 2y = 0

                    have no solution because the second equation, being equivalent to 2x + y = 0, con-
                    tradicts the first one. Therefore, any solution that satisfies one equation cannot sat-
                    isfy the other one.


                    Ill Conditioning
                    The obvious question is: what happens when the coefficient matrix is almost singu-
                    lar, that is, if |A| is very small? In order to determine whether the determinant of the
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                        December 16, 2009     15:4




               29     2.1 Introduction

                     coefficient matrix is “small,” we need a reference against which the determinant can
                     be measured. This reference is called the norm of the matrix and is denoted by A .
                     We can then say that the determinant is small if

                                                            |A| << A

                     Several norms of a matrix have been defined in existing literature, such as the eu-
                     clidean norm

                                                                       n      n
                                                        A   e   =                  A ij2                       (2.5a)
                                                                      i=1 j =1


                     and the row-sum norm, also called the infinity norm

                                                                             n
                                                       A    ∞   = max               A ij                       (2.5b)
                                                                    1≤i≤n
                                                                            j =1


                     A formal measure of conditioning is the matrix condition number, defined as

                                                      cond(A) = A                  A−1                         (2.5c)

                     If this number is close to unity, the matrix is well conditioned. The condition number
                     increases with the degree of ill-conditioning, reaching infinity for a singular matrix.
                     Note that the condition number is not unique, but depends on the choice of the ma-
                     trix norm. Unfortunately, the condition number is expensive to compute for large
                     matrices. In most cases it is sufficient to gauge conditioning by comparing the deter-
                     minant with the magnitudes of the elements in the matrix.
                          If the equations are ill conditioned, small changes in the coefficient matrix result
                     in large changes in the solution. As an illustration, take the equations

                                                  2x + y = 3         2x + 1.001y = 0

                     that have the solution x = 1501.5, y = −3000. Because |A| = 2(1.001) − 2(1) = 0.002
                     is much smaller than the coefficients, the equations are ill conditioned. The effect of
                     ill-conditioning can be verified by changing the second equation to 2x + 1.002y = 0
                     and re-solving the equations. The result is x = 751.5, y = −1500. Note that a 0.1%
                     change in the coefficient of y produced a 100% change in the solution!
                          Numerical solutions of ill-conditioned equations are not to be trusted. The rea-
                     son is that the inevitable roundoff errors during the solution process are equiva-
                     lent to introducing small changes into the coefficient matrix. This in turn introduces
                     large errors into the solution, the magnitude of which depends on the severity of ill-
                     conditioning. In suspect cases the determinant of the coefficient matrix should be
                     computed so that the degree of ill-conditioning can be estimated. This can be done
                     during or after the solution with only a small computational effort.
P1: PHB

CUUS884-Kiusalaas     CUUS884-02      978 0 521 19132 6                                    December 16, 2009        15:4




           30       Systems of Linear Algebraic Equations

                    Linear Systems
                    Linear, algebraic equations occur in almost all branches of numerical analysis. But
                    their most visible application in engineering is in the analysis of linear systems
                    (any system whose response is proportional to the input is deemed to be linear).
                    Linear systems include structures, elastic solids, heat flow, seepage of fluids, elec-
                    tromagnetic fields, and electric circuits, that is, most topics taught in an engineering
                    curriculum.
                         If the system is discrete, such as a truss or an electric circuit, then its analysis
                    leads directly to linear algebraic equations. In the case of a statically determinate
                    truss, for example, the equations arise when the equilibrium conditions of the joints
                    are written down. The unknowns x1 , x2 , . . . , xn represent the forces in the members
                    and the support reactions, and the constants b1 , b2 , . . . , bn are the prescribed external
                    loads.
                         The behavior of continuous systems is described by differential equations, rather
                    than algebraic equations. However, because numerical analysis can deal only with
                    discrete variables, it is first necessary to approximate a differential equation with a
                    system of algebraic equations. The well-known finite difference, finite element, and
                    boundary element methods of analysis work in this manner. They use different ap-
                    proximations to achieve the “discretization,” but in each case the final task is the
                    same: to solve a system (often a very large system) of linear, algebraic equations.
                         In summary, the modeling of linear systems invariably gives rise to equations
                    of the form Ax = b, where b is the input and x represents the response of the sys-
                    tem. The coefficient matrix A, which reflects the characteristics of the system, is in-
                    dependent of the input. In other words, if the input is changed, the equations have
                    to be solved again with a different b, but the same A. Therefore, it is desirable to have
                    an equation-solving algorithm that can handle any number of constant vectors with
                    minimal computational effort.



                    Methods of Solution
                    There are two classes of methods for solving systems of linear, algebraic equations:
                    direct and iterative methods. The common characteristic of direct methods is that
                    they transform the original equations into equivalent equations (equations that have
                    the same solution) that can be solved more easily. The transformation is carried out
                    by applying the three operations listed here. These so-called elementary operations
                    do not change the solution, but they may affect the determinant of the coefficient
                    matrix as indicated in parentheses.

                     • Exchanging two equations (changes sign of |A|).
                     • Multiplying an equation by a non-zero constant (multiplies |A| by the same con-
                       stant).
                     • Multiplying an equation by a nonzero constant and then subtracting it from an-
                       other equation (leaves |A| unchanged).
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                        December 16, 2009   15:4




               31     2.1 Introduction

                          Iterative or indirect methods start with a guess at the solution x, and then re-
                     peatedly refine the solution until a certain convergence criterion is reached. Itera-
                     tive methods are generally less efficient than their direct counterparts because of the
                     large number of iterations required. But they do have significant computational ad-
                     vantages if the coefficient matrix is very large and sparsely populated (most coeffi-
                     cients are zero).



                     Overview of Direct Methods
                     Table 2.1 lists three popular direct methods, each of which uses elementary opera-
                     tions to produce its own final form of easy-to-solve equations.
                          In the table, U represents an upper triangular matrix, L is a lower triangular ma-
                     trix, and I denotes the identity matrix. A square matrix is called triangular if it con-
                     tains only zero elements on one side of the leading diagonal. Thus, a 3 × 3 upper
                     triangular matrix has the form
                                                          ⎡                       ⎤
                                                           U11        U12     U13
                                                          ⎢                       ⎥
                                                        U=⎣ 0         U22     U23 ⎦
                                                            0          0      U33

                     and a 3 × 3 lower triangular matrix appears as
                                                             ⎡                     ⎤
                                                              L 11     0       0
                                                            ⎢                      ⎥
                                                        L = ⎣ L 21    L 22     0 ⎦
                                                              L 31    L 32    L 33

                          Triangular matrices play an important role in linear algebra, because they sim-
                     plify many computations. For example, consider the equations Lx = c, or

                                                                         L 11 x1 = c1

                                                               L 21 x1 + L 22 x2 = c2

                                                      L 31 x1 + L 32 x2 + L 33 x3 = c3

                     If we solve the equations forward, starting with the first equation, the computations
                     are very easy, because each equation contains only one unknown at a time. The



                                               Method                   Initial form     Final form
                                        Gauss elimination                    Ax = b        Ux = c
                                        LU decomposition                     Ax = b       LUx = b
                                     Gauss-Jordan elimination                Ax = b        Ix = c

                                   Table 2.1
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                       December 16, 2009   15:4




           32       Systems of Linear Algebraic Equations

                    solution would thus proceed as follows:

                                                  x1 = c1 /L 11

                                                  x2 = (c2 − L 21 x1 )/L 22

                                                  x3 = (c3 − L 31 x1 − L 32 x2 )/L 33

                    This procedure is known as forward substitution. In a similar way, Ux = c, encoun-
                    tered in Gauss elimination, can easily be solved by back substitution, which starts
                    with the last equation and proceeds backward through the equations.
                        The equations LUx = b, which are associated with LU decomposition, can also
                    be solved quickly if we replace them with two sets of equivalent equations: Ly = b
                    and Ux = y. Now Ly = b can be solved for y by forward substitution, followed by the
                    solution of Ux = y by means of back substitution.
                        The equations Ix = c, which are the produced by Gauss-Jordan elimination, are
                    equivalent to x = c (recall the identity Ix = x), so that c is already the solution.

                    EXAMPLE 2.1
                    Determine whether the following matrix is singular:
                                                    ⎡                    ⎤
                                                      2.1 −0.6       1.1
                                                    ⎢                    ⎥
                                                A = ⎣3.2      4.7 −0.8⎦
                                                      3.1 −6.5       4.1

                    Solution Laplace’s development of the determinant (see Appendix A2) about the first
                    row of A yields

                                            4.7    −0.8          3.2 −0.8       3.2  4.7
                               |A| = 2.1                − (−0.6)          + 1.1
                                           −6.5     4.1          3.1  4.1       3.1 −6.5
                                   = 2.1(14.07) + 0.6(15.60) + 1.1(35.37) = 0

                    Because the determinant is zero, the matrix is singular. It can be verified that the
                    singularity is due to the following row dependency: (row 3) = (3 × row 1) − (row 2).

                    EXAMPLE 2.2
                    Solve the equations Ax = b, where
                                               ⎡                     ⎤           ⎡      ⎤
                                                  8 −6             2                 28
                                               ⎢                     ⎥            ⎢     ⎥
                                           A = ⎣−4 11             −7⎦         b = ⎣ −40 ⎦
                                                  4 −7             6                 33

                    knowing that the LU decomposition of the coefficient matrix is (you should verify
                    this)
                                                 ⎡            ⎤⎡             ⎤
                                                    2    0 0     4 −3      1
                                                 ⎢            ⎥⎢             ⎥
                                        A = LU = ⎣−1     2 0⎦ ⎣0       4 −3⎦
                                                    1 −1 1       0     0   2
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                       December 16, 2009    15:4




               33     2.2 Gauss Elimination Method

                     Solution We first solve the equations Ly = b by forward substitution:

                                          2y1 = 28        y1 = 28/2 = 14
                                 −y1 + 2y2 = −40          y2 = (−40 + y1 )/2 = (−40 + 14)/2 = −13
                                 y1 − y2 + y3 = 33        y3 = 33 − y1 + y2 = 33 − 14 − 13 = 6

                     The solution x is then obtained from Ux = y by back substitution:

                                       2x3 = y3         x3 = y3 /2 = 6/2 = 3
                                 4x2 − 3x3 = y2         x2 = (y2 + 3x3 )/4 = [−13 + 3(3)] /4 = −1
                            4x1 − 3x2 + x3 = y1         x1 = (y1 + 3x2 − x3 )/4 = [14 + 3(−1) − 3] /4 = 2

                                                               T
                     Hence, the solution is x = 2 −1 3             .




              2.2    Gauss Elimination Method
                     Introduction
                     Gauss elimination is the most familiar method for solving simultaneous equations. It
                     consists of two parts: the elimination phase and the solution phase. As indicated in
                     Table 2.1, the function of the elimination phase is to transform the equations into the
                     form Ux = c. The equations are then solved by back substitution. In order to illustrate
                     the procedure, let us solve the equations

                                                            4x1 − 2x2 + x3 = 11                                (a)

                                                        −2x1 + 4x2 − 2x3 = −16                                 (b)

                                                            x1 − 2x2 + 4x3 = 17                                 (c)


                     Elimination Phase
                     The elimination phase utilizes only one of the elementary operations listed in Table
                     2.1 – multiplying one equation (say, equation j ) by a constant λ and subtracting it
                     from another equation (equation i). The symbolic representation of this operation is

                                                      Eq. (i) ← Eq. (i) − λ × Eq. ( j )                       (2.6)

                     The equation being subtracted, namely, Eq. ( j ), is called the pivot equation.
                         We start the elimination by taking Eq. (a) to be the pivot equation and choosing
                     the multipliers λ so as to eliminate x1 from Eqs. (b) and (c):

                                               Eq. (b) ← Eq. (b) − ( − 0.5) × Eq. (a)

                                               Eq. (c) ← Eq. (c) − 0.25 × Eq. (a)
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                December 16, 2009         15:4




           34       Systems of Linear Algebraic Equations

                    After this transformation, the equations become

                                                        4x1 − 2x2 + x3 = 11                              (a)

                                                          3x2 − 1.5x3 = −10.5                            (b)

                                                   −1.5x2 + 3.75x3 = 14.25                               (c)

                    This completes the first pass. Now we pick (b) as the pivot equation and eliminate x2
                    from (c):

                                          Eq. (c) ← Eq. (c) − ( − 0.5) × Eq.(b)

                    which yields the equations

                                                    4x1 − 2x2 + x3 = 11                                  (a)

                                                          3x2 − 1.5x3 = −10.5                            (b)

                                                                 3x3 = 9                                 (c)

                    The elimination phase is now complete. The original equations have been replaced
                    by equivalent equations that can be easily solved by back substitution.
                         As pointed out before, the augmented coefficient matrix is a more convenient
                    instrument for performing the computations. Thus, the original equations would be
                    written as
                                                   ⎡                    ⎤
                                                      4 −2       1 11
                                                   ⎢                    ⎥
                                                   ⎣−2     4 −2 −16⎦
                                                      1 −2       4 17

                    and the equivalent equations produced by the first and the second passes of Gauss
                    elimination would appear as
                                                  ⎡                    ⎤
                                                   4 −2     1    11.00
                                                  ⎢                    ⎥
                                                  ⎣0   3   −1.5 −10.50⎦
                                                    0 −1.5 3.75  14.25

                                                   ⎡                       ⎤
                                                    4      −2    1    11.0
                                                   ⎢                       ⎥
                                                   ⎣0       3   −1.5 −10.5⎦
                                                    0       0    3     9.0

                        It is important to note that the elementary row operation in Eq. (2.6) leaves the
                    determinant of the coefficient matrix unchanged. This is rather fortunate, since the
                    determinant of a triangular matrix is very easy to compute – it is the product of the
                    diagonal elements (you can verify this quite easily). In other words,

                                              |A| = |U| = U11 × U22 × · · · × Unn                   (2.7)
P1: PHB

CUUS884-Kiusalaas   CUUS884-02       978 0 521 19132 6                                                     December 16, 2009     15:4




               35     2.2 Gauss Elimination Method

                     Back Substitution Phase
                     The unknowns can now be computed by back substitution in the manner described
                     in the previous section. Solving Eqs. (c), (b), and (a) in that order, we get


                                              x3 = 9/3 = 3

                                              x2 = (−10.5 + 1.5x3 )/3 = [−10.5 + 1.5(3)]/3 = −2

                                              x1 = (11 + 2x2 − x3 )/4 = [11 + 2(−2) − 3]/4 = 1



                     Algorithm for Gauss Elimination Method
                     Elimination Phase
                     Let us look at the equations at some instant during the elimination phase. Assume
                     that the first k rows of A have already been transformed to upper-triangular form.
                     Therefore, the current pivot equation is the kth equation, and all the equations be-
                     low it are still to be transformed. This situation is depicted by the augmented co-
                     efficient matrix shown next. Note that the components of A are not the coefficients
                     of the original equations (except for the first row), because they have been altered
                     by the elimination procedure. The same applies to the components of the constant
                     vector b.
                           ⎡                                                                                ⎤
                             A 11     A 12     A 13      ···   A 1k     ···    A 1j    ···   A 1n     b1
                           ⎢                                                                                ⎥
                           ⎢ 0        A 22     A 23      ···   A 2k     ···    A 2j    ···   A 2n     b2 ⎥
                           ⎢                                                                                ⎥
                           ⎢ 0
                           ⎢           0       A 33      ···   A 3k     ···    A 3j    ···   A 3n     b3 ⎥  ⎥
                           ⎢ .         ..       ..               ..             ..             ..      .. ⎥
                           ⎢ ..
                           ⎢            .        .                .              .              .       .⎥  ⎥
                           ⎢                                                                                ⎥
                           ⎢ 0         0        0        ···   A kk     ···    A kj    ···   A kn     bk ⎥ ← pivot row
                           ⎢                                                                                ⎥
                           ⎢ ..         ..       ..               ..             ..             ..      .. ⎥
                           ⎢ .
                           ⎢             .        .                .              .              .       .⎥ ⎥
                           ⎢ 0
                           ⎢           0        0        ···   A ik     ···    A ij    ···   A in     bi ⎥  ⎥ ← row being
                           ⎢ .
                           ⎢ .           ..       ..               ..             ..             ..      .. ⎥
                                                                                                            ⎥   transformed
                           ⎣ .            .        .                .              .              .       .⎦
                                 0     0        0        ···   A nk     ···    A nj    ···   A nn     bn

                         Let the ith row be a typical row below the pivot equation that is to be trans-
                     formed, meaning that the element A ik is to be eliminated. We can achieve this by
                     multiplying the pivot row by λ = A ik /A kk and subtracting it from the ith row. The
                     corresponding changes in the ith row are


                                                       A ij ← A ij − λA kj ,      j = k, k + 1, . . . , n                      (2.8a)

                                                        bi ← bi − λbk                                                          (2.8b)


                     In order to transform the entire coefficient matrix to upper-triangular form, k and
                     i in Eqs. (2.8) must have the ranges k = 1, 2, . . . , n − 1 (chooses the pivot row),
P1: PHB

CUUS884-Kiusalaas     CUUS884-02       978 0 521 19132 6                                                  December 16, 2009      15:4




           36       Systems of Linear Algebraic Equations

                    i = k + 1, k + 2 . . . , n (chooses the row to be transformed). The algorithm for the
                    elimination phase now almost writes itself:

                    for k in range(0,n-1):
                         for i in range(k+1,n):
                               if a[i,k] != 0.0:
                                      lam = a[i,k]/a[k,k]
                                      a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
                                      b[i] = b[i] - lam*b[k]


                        In order to avoid unnecessary operations, this algorithm departs slightly from
                    Eqs. (2.8) in the following ways:

                     • If A ik happens to be zero, the transformation of row i is skipped.
                     • The index j in Eq. (2.8a) starts with k + 1 rather than k. Therefore, A ik is not re-
                       placed by zero, but retains its original value. As the solution phase never accesses
                       the lower triangular portion of the coefficient matrix anyway, its contents are ir-
                       relevant.


                    Back Substitution Phase
                    After Gauss elimination the augmented coefficient matrix has the form
                                                           ⎡                                          ⎤
                                                         A 11        A 12     A 13   ···   A 1n   b1
                                                       ⎢ 0           A 22     A 23   ···   A 2n   b2 ⎥
                                                       ⎢                                              ⎥
                                                       ⎢                                              ⎥
                                            A      b = ⎢ 0            0       A 33   ···   A 3n   b3 ⎥
                                                       ⎢                                              ⎥
                                                       ⎢ ..           ..       ..            ..    .. ⎥
                                                       ⎣ .             .        .             .     .⎦
                                                          0           0        0     ···   A nn   bn

                    The last equation, A nn xn = bn , is solved first, yielding

                                                                  xn = bn /A nn                                          (2.9)

                        Consider now the stage of back substitution where xn , xn−1 , . . . , xk+1 have been
                    already been computed (in that order), and we are about to determine xk from the
                    kth equation

                                                A kk xk + A k,k+1 xk+1 + · · · + A kn xn = bk

                    The solution is
                                           ⎛                        ⎞
                                                       n
                                                                           1
                                      xk = ⎝bk −             A kj x j ⎠        , k = n − 1, n − 2, . . . , 1            (2.10)
                                                                          A kk
                                                    j =k+1


                    The corresponding algorithm for back substitution is:

                    for k in range(n-1,-1,-1):
                               x[k]=(b[k] - dot(a[k,k+1:n],x[k+1:n]))/a[k,k]
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                     December 16, 2009     15:4




               37     2.2 Gauss Elimination Method

                     Operation Count
                     The execution time of an algorithm depends largely on the number of long opera-
                     tions (multiplications and divisions) performed. It can be shown that Gauss elimi-
                     nation contains approximately n3 /3 such operations (n is the number of equations)
                     in the elimination phase, and n2 /2 operations in back substitution. These numbers
                     show that most of the computation time goes into the elimination phase. Moreover,
                     the time increases very rapidly with the number of equations.


                        gaussElimin

                     The function gaussElimin combines the elimination and the back substitution
                     phases. During back substitution b is overwritten by the solution vector x, so that
                     b contains the solution upon exit.

                     ## module gaussElimin
                     ’’’ x = gaussElimin(a,b).
                           Solves [a]{b} = {x} by Gauss elimination.
                     ’’’
                     from numpy import dot


                     def gaussElimin(a,b):
                           n = len(b)
                        # Elimination phase
                           for k in range(0,n-1):
                                 for i in range(k+1,n):
                                    if a[i,k] != 0.0:
                                          lam = a [i,k]/a[k,k]
                                          a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
                                          b[i] = b[i] - lam*b[k]
                        # Back substitution
                           for k in range(n-1,-1,-1):
                                 b[k] = (b[k] - dot(a[k,k+1:n],b[k+1:n]))/a[k,k]
                           return b



                     Multiple Sets of Equations
                     As mentioned before, it is frequently necessary to solve the equations Ax = b for sev-
                     eral constant vectors. Let there be m such constant vectors, denoted by b1 , b2 , . . . , bm ,
                     and let the corresponding solution vectors be x1 , x2 , . . . , xm . We denote multiple sets
                     of equations by AX = B, where

                                           X = x1 x2 · · · xm         B = b1 b2 · · · bm

                     are n × m matrices whose columns consist of solution vectors and constant vectors,
                     respectively.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02       978 0 521 19132 6                                December 16, 2009   15:4




           38       Systems of Linear Algebraic Equations

                         An economical way to handle such equations during the elimination phase is
                    to include all m constant vectors in the augmented coefficient matrix, so that they
                    are transformed simultaneously with the coefficient matrix. The solutions are then
                    obtained by back substitution in the usual manner, one vector at a time. It would
                    be quite easy to make the corresponding changes in gaussElimin. However, the LU
                    decomposition method, described in the next section, is more versatile in handling
                    multiple constant vectors.

                    EXAMPLE 2.3
                    Use Gauss elimination to solve the equations AX = B, where
                                            ⎡              ⎤         ⎡          ⎤
                                               6 −4       1           −14     22
                                            ⎢              ⎥         ⎢          ⎥
                                       A = ⎣−4       6 −4⎦       B = ⎣ 36 −18⎦
                                               1 −4       6               6    7

                    Solution The augmented coefficient matrix is
                                             ⎡                              ⎤
                                                 6 −4      1 −14          22
                                             ⎢                              ⎥
                                             ⎣−4      6 −4 36            −18⎦
                                                 1 −4      6     6         7

                    The elimination phase consists of the following two passes:

                                                  row 2 ← row 2 + (2/3) × row 1

                                                  row 3 ← row 3 − (1/6) × row 1

                                               ⎡                             ⎤
                                                6   −4          1 −14     22
                                               ⎢                             ⎥
                                               ⎣0  10/3     −10/3 80/3 −10/3⎦
                                                0 −10/3      35/6 25/3  10/3

                    and

                                                      row 3 ← row 3 + row 2

                                                ⎡                             ⎤
                                                 6    −4      1   −14     22
                                                ⎢                             ⎥
                                                ⎣0   10/3   −10/3 80/3   −10/3⎦
                                                 0    0      5/2   35      0

                          In the solution phase, we first compute x1 by back substitution:

                                               35
                                      X 31 =       = 14
                                               5/2
                                               80/3 + (10/3)X 31   80/3 + (10/3)14
                                      X 21 =                     =                 = 22
                                                     10/3               10/3
                                               −14 + 4X 21 − X 31   −14 + 4(22) − 14
                                      X 11 =                      =                  = 10
                                                      6                    6
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                                                    December 16, 2009   15:4




               39     2.2 Gauss Elimination Method

                     Thus, the first solution vector is
                                                                                      T                                 T
                                             x1 = X 11           X 21         X 31        = 10            22 14

                           The second solution vector is computed next, also using back substitution:

                                           X 32 = 0
                                                    −10/3 + (10/3)X 32   −10/3 + 0
                                           X 22 =                      =           = −1
                                                          10/3             10/3
                                                    22 + 4X 22 − X 32   22 + 4(−1) − 0
                                           X 12 =                     =                =3
                                                           6                   6
                     Therefore,
                                                                                      T                                 T
                                              x2 = X 12            X 22        X 32           = 3         −1        0

                     EXAMPLE 2.4
                     An n × n Vandermode matrix A is defined by
                                                       n− j
                                           A ij = vi          , i = 1, 2, . . . , n,                j = 1, 2, . . . , n

                     where v is a vector. Use the function gaussElimin to compute the solution of Ax = b,
                     where A is the 6 × 6 the Vandermode matrix generated from the vector
                                                                                                                    T
                                                v = 1.0           1.2         1.4         1.6       1.8       2.0

                     and
                                                                                                          T
                                                         b= 0             1     0         1     0     1

                     Also evaluate the accuracy of the solution (Vandermode matrices tend to be ill con-
                     ditioned).

                     Solution

                     #!/usr/bin/python
                     ## example2_4
                     from numpy import zeros,array,prod,diagonal,dot
                     from gaussElimin import *


                     def vandermode(v):
                           n = len(v)
                           a = zeros((n,n))
                           for j in range(n):
                                 a[:,j] = v**(n-j-1)
                           return a


                     v = array([1.0, 1.2, 1.4, 1.6, 1.8, 2.0])
                     b = array([0.0, 1.0, 0.0, 1.0, 0.0, 1.0])
                     a = vandermode(v)
P1: PHB

CUUS884-Kiusalaas       CUUS884-02   978 0 521 19132 6                                  December 16, 2009       15:4




           40       Systems of Linear Algebraic Equations

                    aOrig = a.copy()           # Save original matrix
                    bOrig = b.copy()           # and the constant vector
                    x = gaussElimin(a,b)
                    det = prod(diagonal(a))
                    print ’x =\n’,x
                    print ’\ndet =’,det
                    print ’\nCheck result: [a]{x} - b =\n’,dot(aOrig,x) - bOrig
                    raw_input("\nPress return to exit")

                          The program produced the following results:

                    x =
                    [     416.66666667      -3125.00000004        9250.00000012 -13500.00000017
                         9709.33333345      -2751.00000003]


                    det = -1.13246207999e-006


                    Check result: [a]{x} - b =
                    [     4.54747351e-13     2.27373675e-12       4.09272616e-12        1.50066626e-11
                        -5.00222086e-12      6.04813977e-11]

                        As the determinant is quite small relative to the elements of A (you may want to
                    print A to verify this), we expect detectable roundoff error. Inspection of x leads us to
                    suspect that the exact solution is
                                                                                              T
                                  x = 1250/3     −3125   9250    −13500    29128/3    −2751

                    in which case the numerical solution would be accurate to about 10 decimal places.
                    Another way to gauge the accuracy of the solution is to compute Ax − b (the result
                    should be 0). The printout indicates that the solution is indeed accurate to at least 10
                    decimal places.



          2.3       LU Decomposition Methods
                    Introduction
                    It is possible to show that any square matrix A can be expressed as a product of a
                    lower triangular matrix L and an upper triangular matrix U:

                                                            A = LU                                     (2.11)

                    The process of computing L and U for a given A is known as LU decomposition or
                    LU factorization. LU decomposition is not unique (the combinations of L and U for
                    a prescribed A are endless), unless certain constraints are placed on L or U. These
                    constraints distinguish one type of decomposition from another. Three commonly
                    used decompositions are listed in Table 2.2.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                  December 16, 2009    15:4




               41     2.3 LU Decomposition Methods


                                      Name                          Constraints
                                      Doolittle’s decomposition     Lii = 1, i = 1, 2, ..., n
                                      Crout’s decomposition         Uii = 1, i = 1, 2, ..., n
                                      Choleski’s decomposition      L = UT

                                     Table 2.2


                         After decomposing A, it is easy to solve the equations Ax = b, as pointed out
                     in Section 2.1. We first rewrite the equations as LUx = b. Upon using the notation
                     Ux = y, the equations become

                                                             Ly = b

                     which can be solved for y by forward substitution. Then

                                                             Ux = y

                     will yield x by the back substitution process.
                          The advantage of LU decomposition over the Gauss elimination method is that
                     once A is decomposed, we can solve Ax = b for as many constant vectors b as we
                     please. The cost of each additional solution is relatively small, since the forward and
                     back substitution operations are much less time consuming than the decomposition
                     process.


                     Doolittle’s Decomposition Method
                     Decomposition Phase
                     Doolittle’s decomposition is closely related to Gauss elimination. In order to illustrate
                     the relationship, consider a 3 × 3 matrix A and assume that there exist triangular ma-
                     trices
                                             ⎡             ⎤           ⎡               ⎤
                                                1    0 0                U11 U12 U13
                                             ⎢             ⎥           ⎢               ⎥
                                         L = ⎣ L 21  1 0⎦         U = ⎣ 0 U22 U23 ⎦
                                               L 31 L 32 1               0     0 U33

                     such that A = LU. After completing the multiplication on the right-hand side, we get
                                       ⎡                                                       ⎤
                                        U11      U12                 U13
                                       ⎢                                                       ⎥
                                  A = ⎣U11 L 21 U12 L 21 + U22       U13 L 21 + U23            ⎦   (2.12)
                                        U11 L 31 U12 L 31 + U22 L 32 U13 L 31 + U23 L 32 + U33

                         Let us now apply Gauss elimination to Eq. (2.12). The first pass of the elimina-
                     tion procedure consists of choosing the first row as the pivot row and applying the
                     elementary operations

                                          row 2 ← row 2 − L 21 × row 1 (eliminatesA 21 )

                                          row 3 ← row 3 − L 31 × row 1 (eliminatesA 31 )
P1: PHB

CUUS884-Kiusalaas     CUUS884-02      978 0 521 19132 6                                   December 16, 2009        15:4




           42       Systems of Linear Algebraic Equations

                    The result is
                                                  ⎡                                  ⎤
                                                   U11      U12            U13
                                                  ⎢                                  ⎥
                                               A =⎣ 0       U22            U23       ⎦
                                                    0      U22 L 32   U23 L 32 + U33

                    In the next pass we take the second row as the pivot row and utilize the operation

                                         row 3 ← row 3 − L 32 × row 2 (eliminatesA 32 )

                    ending up with
                                                        ⎡                        ⎤
                                                         U11          U12    U13
                                                        ⎢                        ⎥
                                                   A =U=⎣ 0           U22    U23 ⎦
                                                          0            0     U33

                         The foregoing illustration reveals two important features of Doolittle’s decompo-
                    sition:

                     • The matrix U is identical to the upper triangular matrix that results from Gauss
                       elimination.
                     • The off-diagonal elements of L are the pivot equation multipliers used during
                       Gauss elimination, that is, Lij is the multiplier that eliminated A ij .

                        It is usual practice to store the multipliers in the lower triangular portion of the
                    coefficient matrix, replacing the coefficients as they are eliminated (Lij replacing A ij ).
                    The diagonal elements of L do not have to be stored, because it is understood that
                    each of them is unity. The final form of the coefficient matrix would thus be the fol-
                    lowing mixture of L and U:
                                                            ⎡                    ⎤
                                                             U11      U12    U13
                                                            ⎢                    ⎥
                                                    [L\U] = ⎣ L 21    U22    U23 ⎦                       (2.13)
                                                              L 31    L 32   U33

                        The algorithm for Doolittle’s decomposition is thus identical to the Gauss elimi-
                    nation procedure in gaussElimin, except that each multiplier λ is now stored in the
                    lower triangular portion of A :

                    for k in range(0,n-1):
                         for i in range(k+1,n):
                               if a[i,k] != 0.0:
                                     lam = a[i,k]/a[k,k]
                                     a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
                                     a[i,k] = lam
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                              December 16, 2009     15:4




               43     2.3 LU Decomposition Methods

                     Solution Phase
                     Consider now the procedure for the solution of Ly = b by forward substitution. The
                     scalar form of the equations is (recall that Lii = 1)

                                                                                         y1 = b1

                                                                               L 21 y1 + y2 = b2
                                                                                                 ..
                                                                                                  .

                                           L k1 y1 + L k2 y2 + · · · + L k,k−1 yk−1 + yk = bk
                                                                                                 ..
                                                                                                  .

                     Solving the kth equation for yk yields

                                                            k−1
                                               y k = bk −          L kj y j , k = 2, 3, ..., n                       (2.14)
                                                            j =1


                     Therefore, the forward substitution algorithm is

                     y[0] = b[0]
                     for k in range(1,n):
                           y[k] = b[k] - dot(a[k,0:k],y[0:k])


                         The back substitution phase for solving Ux = y is identical to what was used in
                     the Gauss elimination method.


                        LUdecomp

                     This module contains both the decomposition and solution phases. The decompo-
                     sition phase returns the matrix [L\U] shown in Eq. (2.13). In the solution phase, the
                     contents of b are replaced by y during forward substitution Similarly, the back sub-
                     stitution overwrites y with the solution x.


                     ## module LUdecomp
                     ’’’ a = LUdecomp(a).
                           LU decomposition: [L][U] = [a]. The returned matrix
                           [a] = [L\U] contains [U] in the upper triangle and
                           the nondiagonal terms of [L] in the lower triangle.


                           x = LUsolve(a,b).
                           Solves [L][U]{x} = b, where [a] = [L\U] is the matrix
                           returned from LUdecomp.
                     ’’’
                     from numpy import dot
P1: PHB

CUUS884-Kiusalaas     CUUS884-02       978 0 521 19132 6                                          December 16, 2009      15:4




           44       Systems of Linear Algebraic Equations

                    def LUdecomp(a):
                         n = len(a)
                         for k in range(0,n-1):
                               for i in range(k+1,n):
                                    if a[i,k] != 0.0:
                                           lam = a [i,k]/a[k,k]
                                           a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
                                           a[i,k] = lam
                         return a


                    def LUsolve(a,b):
                         n = len(a)
                         for k in range(1,n):
                               b[k] = b[k] - dot(a[k,0:k],b[0:k])
                         for k in range(n-1,-1,-1):
                              b[k] = (b[k] - dot(a[k,k+1:n],b[k+1:n]))/a[k,k]
                         return b



                    Choleski’s Decomposition Method
                    Choleski’s decomposition A = LLT has two limitations:

                     • Because LLT is always a symmetric matrix, Choleski’s decomposition requires A
                       to be symmetric.
                     • The decomposition process involves taking square roots of certain combinations
                       of the elements of A. It can be shown that in order to avoid square roots of nega-
                       tive numbers A must be positive definite.

                        Choleski’s decomposition contains approximately n3 /6 long operations plus n
                    square root computations. This is about half the number of operations required in
                    LU decomposition. The relative efficiency of Choleski’s decomposition is due to its
                    exploitation of symmetry.
                        Let us start by looking at Choleski’s decomposition

                                                                A = LLT                                         (2.15)

                    of a 3 × 3 matrix:
                                  ⎡                     ⎤ ⎡                      ⎤⎡                    ⎤
                                    A 11    A 12   A 13       L 11    0      0      L 11   L 21   L 31
                                  ⎢                     ⎥ ⎢                      ⎥⎢                    ⎥
                                  ⎣A 21     A 22   A 23 ⎦ = ⎣ L 21   L 22    0 ⎦⎣ 0        L 22   L 32 ⎦
                                    A 31    A 32   A 33       L 31   L 32   L 33     0      0     L 33

                    After completing the matrix multiplication on the right-hand side, we get
                           ⎡                ⎤ ⎡                                                        ⎤
                             A 11 A 12 A 13      L 211     L 11 L 21             L 11 L 31
                           ⎢                ⎥ ⎢                                                        ⎥
                           ⎣A 21 A 22 A 23 ⎦ = ⎣ L 11 L 21 L 221 + L 222         L 21 L 31 + L 22 L 32 ⎦        (2.16)
                             A 31 A 32 A 33      L 11 L 31 L 21 L 31 + L 22 L 32 L 231 + L 232 + L 233
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                                        December 16, 2009     15:4




               45     2.3 LU Decomposition Methods

                     Note that the right-hand-side matrix is symmetric, as pointed out before. Equating
                     the matrices A and LLT element by element, we obtain six equations (because of sym-
                     metry only lower or upper triangular elements have to be considered) in the six un-
                     known components of L. By solving these equations in a certain order, it is possible
                     to have only one unknown in each equation.
                          Consider the lower triangular portion of each matrix in Eq. (2.16) (the upper tri-
                     angular portion would do as well). By equating the elements in the first column, start-
                     ing with the first row and proceeding downward, we can compute L 11 , L 21 , and L 31
                     in that order:

                                                      A 11 = L 211              L 11 =     A 11

                                                      A 21 = L 11 L 21          L 21 = A 21 /L 11

                                                      A 31 = L 11 L 31          L 31 = A 31 /L 11

                     The second column, starting with second row, yields L 22 and L 32 :

                                       A 22 = L 221 + L 222                     L 22 =     A 22 − L 221

                                       A 32 = L 21 L 31 + L 22 L 32            L 32 = (A 32 − L 21 L 31 )/L 22

                     Finally, the third column, third row gives us L 33 :

                                        A 33 = L 231 + L 232 + L 233            L 33 =     A 33 − L 231 − L 232

                         We can now extrapolate the results for an n × n matrix. We observe that a typical
                     element in the lower triangular portion of LLT is of the form

                                                                                                   j
                                  (LLT )ij = Li1 L j 1 + Li2 L j 2 + · · · + Lij L j j =                Lik L j k , i ≥ j
                                                                                                  k=1

                     Equating this term to the corresponding element of A yields

                                               j
                                     A ij =         Lik L j k , i = j, j + 1, . . . , n,          j = 1, 2, . . . , n          (2.17)
                                              k=1

                     The range of indices shown limits the elements to the lower triangular part. For the
                     first column ( j = 1), we obtain from Eq. (2.17)

                                            L 11 =      A 11       Li1 = A i1 /L 11 , i = 2, 3, ..., n                         (2.18)

                     Proceeding to other columns, we observe that the unknown in Eq. (2.17) is Lij (the
                     other elements of L appearing in the equation have already been computed). Taking
                     the term containing Lij outside the summation in Eq. (2.17), we obtain

                                                                   j −1
                                                          A ij =          Lik L j k + Lij L j j
                                                                   k=1
P1: PHB

CUUS884-Kiusalaas     CUUS884-02           978 0 521 19132 6                                                    December 16, 2009      15:4




           46       Systems of Linear Algebraic Equations

                    If i = j (a diagonal term), the solution is

                                                                           j −1
                                                    L jj =        A jj −          L 2j k ,   j = 2, 3, ..., n                 (2.19)
                                                                           k=1

                    For a nondiagonal term we get
                          ⎛               ⎞
                                    j −1
                    Lij = ⎝A ij −          Lik L j k ⎠ /L j j ,   j = 2, 3, . . . , n − 1, i = j + 1, j + 2, . . . , n        (2.20)
                                    k=1



                      choleski

                    Before presenting the algorithm for Choleski’s decomposition, we make a useful ob-
                    servation: A ij appears only in the formula for Lij . Therefore, once Lij has been com-
                    puted, A ij is no longer needed. This makes it possible to write the elements of L
                    over the lower triangular portion of A as they are computed. The elements above the
                    leading diagonal of A will remain untouched. The function listed next implements
                    Choleski’s decomposition. If a negative diagonal term is encountered during decom-
                    position, an error message is printed and the program is terminated.
                        After the coefficient matrix A has been decomposed, the solution of Ax = b can
                    be obtained by the usual forward and back substitution operations. The function
                    choleskiSol (given here without derivation) carries out the solution phase.

                    ## module choleski
                    ’’’ L = choleski(a)
                          Choleski decomposition: [L][L]transpose = [a]


                          x = choleskiSol(L,b)
                          Solution phase of Choleski’s decomposition method
                    ’’’
                    from numpy import dot
                    from math import sqrt
                    import error


                    def choleski(a):
                          n = len(a)
                          for k in range(n):
                              try:
                                      a[k,k] = sqrt(a[k,k] - dot(a[k,0:k],a[k,0:k]))
                              except ValueError:
                                      error.err(’Matrix is not positive definite’)
                              for i in range(k+1,n):
                                      a[i,k] = (a[i,k] - dot(a[i,0:k],a[k,0:k]))/a[k,k]
                          for k in range(1,n): a[0:k,k] = 0.0
                          return a
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                 December 16, 2009    15:4




               47     2.3 LU Decomposition Methods

                     def choleskiSol(L,b):
                          n = len(b)
                        # Solution of [L]{y} = {b}
                          for k in range(n):
                                 b[k] = (b[k] - dot(L[k,0:k],b[0:k]))/L[k,k]
                        # Solution of [L_transpose]{x} = {y}
                          for k in range(n-1,-1,-1):
                                 b[k] = (b[k] - dot(L[k+1:n,k],b[k+1:n]))/L[k,k]
                          return b



                     Other Methods
                     Crout’s Decomposition
                     Recall that the various decompositions A = LU are characterized by the constraints
                     placed on the elements of L or U. In Doolittle’s decomposition, the diagonal elements
                     of L were set to 1. An equally viable method is Crout’s decomposition, where the 1’s lie
                     on the diagonal of U. There is little difference in the performance of the two methods.


                     Gauss–Jordan Elimination
                     The Gauss–Jordan method is essentially Gauss elimination taken to its limit. In the
                     Gauss elimination method only the equations that lie below the pivot equation are
                     transformed. In the Gauss–Jordan method the elimination is also carried out on
                     equations above the pivot equation, resulting in a diagonal coefficient matrix.
                         The main disadvantage of Gauss–Jordan elimination is that it involves about n3 /2
                     long operations, which is 1.5 times the number required in Gauss elimination.

                     EXAMPLE 2.5
                     Use Doolittle’s decomposition method to solve the equations Ax = b, where
                                                 ⎡            ⎤          ⎡ ⎤
                                                   1   4    1               7
                                                 ⎢            ⎥          ⎢ ⎥
                                            A = ⎣1     6 −1⎦         b = ⎣ 13 ⎦
                                                   2 −1     2               5

                     Solution We first decompose A by Gauss elimination. The first pass consists of the
                     elementary operations

                                          row 2 ← row 2 − 1 × row 1 (eliminates A 21 )

                                          row 3 ← row 3 − 2 × row 1 (eliminates A 31 )

                     Storing the multipliers L 21 = 1 and L 31 = 2 in place of the eliminated terms, we ob-
                     tain
                                                           ⎡              ⎤
                                                              1    4     1
                                                           ⎢              ⎥
                                                       A = ⎣1      2 −2⎦
                                                              2 −9       0
P1: PHB

CUUS884-Kiusalaas     CUUS884-02      978 0 521 19132 6                                     December 16, 2009   15:4




           48       Systems of Linear Algebraic Equations

                    The second pass of Gauss elimination uses the operation

                                       row 3 ← row 3 − (−4.5) × row 2 (eliminates A 32 )

                    Storing the multiplier L 32 = −4.5 in place of A 32 , we get
                                                              ⎡                  ⎤
                                                                1      4       1
                                                              ⎢                  ⎥
                                                 A = [L\U] = ⎣1        2     −2⎦
                                                                2 −4.5 −9

                    The decomposition is now complete, with
                                            ⎡           ⎤               ⎡               ⎤
                                             1   0     0                 1     4      1
                                            ⎢           ⎥               ⎢               ⎥
                                        L = ⎣1   1     0⎦           U = ⎣0     2     −2⎦
                                             2 −4.5 1                    0     0     −9

                        Solution of Ly = b by forward substitution      comes next. The augmented coeffi-
                    cient form of the equations is
                                                        ⎡                    ⎤
                                                         1     0         0 7
                                                        ⎢                    ⎥
                                                  L b = ⎣1     1         0 13⎦
                                                          2 −4.5         1 5

                    The solution is

                                         y1 = 7

                                         y2 = 13 − y1 = 13 − 7 = 6

                                         y3 = 5 − 2y1 + 4.5y2 = 5 − 2(7) + 4.5(6) = 18

                    Finally, the equations Ux = y, or
                                                           ⎡           ⎤
                                                            1 4   1 7
                                                           ⎢           ⎥
                                                     U y = ⎣0 2 −2 6⎦
                                                             0 0 −9 18

                    are solved by back substitution. This yields

                                                   18
                                            x3 =      = −2
                                                   −9
                                                 6 + 2x3    6 + 2(−2)
                                            x2 =         =             =1
                                                    2           2
                                            x1 = 7 − 4x2 − x3 = 7 − 4(1) − (−2) = 5

                    EXAMPLE 2.6
                    Compute Choleski’s decomposition of the matrix
                                                     ⎡              ⎤
                                                         4 −2      2
                                                     ⎢              ⎥
                                                A = ⎣−2      2 −4⎦
                                                         2 −4 11
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                     December 16, 2009   15:4




               49     2.3 LU Decomposition Methods

                     Solution First, we note that A is symmetric. Therefore, Choleski’s decomposition is
                     applicable, provided that the matrix is also positive definite. An a priori test for posi-
                     tive definiteness is not needed, since the decomposition algorithm contains its own
                     test: if the square root of a negative number is encountered, the matrix is not positive
                     definite and the decomposition fails.
                          Substituting the given matrix for A in Eq. (2.16) we obtain
                                 ⎡               ⎤ ⎡                                                         ⎤
                                    4 −2       2       L 211     L 11 L 21             L 11 L 31
                                 ⎢               ⎥ ⎢                                                         ⎥
                                 ⎣−2     2 −4⎦ = ⎣ L 11 L 21 L 221 + L 222             L 21 L 31 + L 22 L 32 ⎦
                                    2 −4 11            L 11 L 31 L 21 L 31 + L 22 L 32 L 31 + L 32 + L 33
                                                                                         2       2       2


                     Equating the elements in the lower (or upper) triangular portions yields
                                            √
                                      L 11 = 4 = 2

                                        L 21 = −2/L 11 = −2/2 = −1

                                        L 31 = 2/L 11 = 2/2 = 1

                                        L 22 =    2 − L 221 =   2 − 12 = 1
                                                 −4 − L 21 L 31   −4 − (−1)(1)
                                        L 32 =                  =              = −3
                                                     L 22              1

                                        L 33 =    11 − L 231 − L 232 =   11 − (1)2 − (−3)2 = 1

                     Therefore,
                                                                ⎡    ⎤
                                                               2  0 0
                                                             ⎢       ⎥
                                                         L = ⎣−1  1 0⎦
                                                               1 −3 1

                     The result can easily be verified by performing the multiplication LLT .

                     EXAMPLE 2.7
                     Write a program that solves AX = B with Doolittle’s decomposition method and com-
                     putes |A|. Utilize the functions LUdecomp and LUsolve. Test the program with
                                                  ⎡            ⎤          ⎡      ⎤
                                                     3 −1    4              6 −4
                                                  ⎢            ⎥          ⎢      ⎥
                                              A = ⎣−2    0   5⎦      B = ⎣3    2⎦
                                                     7   2 −2               7 −5

                     Solution

                     #!/usr/bin/python
                     ## example2_7
                     from numpy import array,prod,diagonal
                     from LUdecomp import *


                     a = array([[ 3.0, -1.0,            4.0], \
                                   [-2.0,        0.0,   5.0], \
                                   [ 7.0,        2.0, -2.0]])
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                  December 16, 2009   15:4




           50       Systems of Linear Algebraic Equations

                    b = array([[ 6.0,          3.0,      7.0], \
                                  [-4.0,       2.0, -5.0]])
                    a = LUdecomp(a)                                  # Decompose [a]
                    det = prod(diagonal(a))
                    print "\nDeterminant =",det
                    for i in range(len(b)):                          # Back-substitute one
                       x = LUsolve(a,b[i])                           # constant vector at a time
                       print "x",i+1,"=",x
                    raw_input("\nPress return to exit")


                        Running the program produced the following display:

                    Determinant = -77.0
                    x 1 = [ 1.     1.       1.]
                    x 2 = [ -1.00000000e+00               1.00000000e+00       2.30695693e-17]


                    EXAMPLE 2.8
                    Solve the equations Ax = b by Choleski’s decomposition, where
                                        ⎡                                 ⎤        ⎡      ⎤
                                      1.44          −0.36     5.52    0.00           0.04
                                    ⎢−0.36                   −7.78    0.00⎥       ⎢ −2.15 ⎥
                                    ⎢               10.33                 ⎥       ⎢       ⎥
                                  A=⎢                                     ⎥     b=⎢       ⎥
                                    ⎣ 5.52          −7.78    28.40    9.00⎦       ⎣     0⎦
                                      0.00           0.00     9.00   61.00           0.88

                    Also check the solution.

                    Solution

                    #!/usr/bin/python
                    ## example2_8
                    from numpy import array,dot
                    from choleski import *


                    a = array([[ 1.44, -0.36,               5.52,    0.0], \
                                  [-0.36, 10.33, -7.78,              0.0], \
                                  [ 5.52, -7.78, 28.40,              9.0], \
                                  [ 0.0,          0.0,      9.0,    61.0]])
                    b = array([0.04, -2.15, 0.0, 0.88])
                    aOrig = a.copy()
                    L = choleski(a)
                    x = choleskiSol(L,b)
                    print "x =",x
                    print ’\nCheck: A*x =\n’,dot(aOrig,x)
                    raw_input("\nPress return to exit")
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                 December 16, 2009    15:4




               51     2.3 LU Decomposition Methods

                         The output is:

                     x = [ 3.09212567 -0.73871706 -0.8475723                0.13947788]


                     Check: A*x =
                     [4.00000000e-02 -2.15000000e+00 -5.10702591e-15                 8.80000000e-01]


                     PROBLEM SET 2.1
                      1. By evaluating the determinant, classify the following matrices as singular, ill con-
                         ditioned, or well conditioned.
                                            ⎡        ⎤                    ⎡                     ⎤
                                              1 2 3                           2.11 −0.80 1.72
                                            ⎢        ⎥                    ⎢                     ⎥
                                 (a) A = ⎣2 3 4⎦                (b) A = ⎣−1.84       3.03 1.29⎦
                                              3 4 5                         −1.57    5.25 4.30
                                            ⎡             ⎤                ⎡             ⎤
                                               2 −1     0                    4     3 −1
                                            ⎢             ⎥                ⎢             ⎥
                                 (c) A = ⎣−1        2 −1⎦        (d) A = ⎣7       −2    3⎦
                                               0 −1     2                    5 −18 13

                      2. Given the LU decomposition A = LU, determine A and |A| .
                                                 ⎡          ⎤       ⎡           ⎤
                                                  1    0   0          1 2     4
                                                 ⎢          ⎥       ⎢           ⎥
                                         (a) L = ⎣1    1   0⎦ U = ⎣0 3 21⎦
                                                  1 5/3 1             0 0     0
                                                 ⎡           ⎤       ⎡             ⎤
                                                    2    0 0           2 −1       1
                                                 ⎢           ⎥       ⎢             ⎥
                                         (b) L = ⎣−1     1 0⎦ U = ⎣0        1 −3⎦
                                                    1 −3 1             0    0     1

                      3. Utilize the results of LU decomposition
                                                     ⎡               ⎤⎡                   ⎤
                                                        1    0      0   2     −3     −1
                                                     ⎢               ⎥⎢                   ⎥
                                          A = LU = ⎣3/2      1      0⎦ ⎣0    13/2   −7/2 ⎦
                                                       1/2 11/13    1   0     0     32/13

                         to solve Ax = b, where bT = 1 −1 2 .
                      4. Use Gauss elimination to solve the equations Ax = b, where
                                                   ⎡            ⎤          ⎡    ⎤
                                                     2 −3 −1                  3
                                                   ⎢            ⎥          ⎢    ⎥
                                               A = ⎣3     2 −5⎦        b = ⎣ −9 ⎦
                                                     2    4 −1               −5

                      5. Solve the equations AX = B by Gauss elimination, where
                                                      ⎡              ⎤          ⎡      ⎤
                                                  2 0       −1     0            1    0
                                                ⎢ 0 1              0⎥         ⎢0     0⎥
                                                ⎢            2       ⎥        ⎢        ⎥
                                              A=⎢                    ⎥      B=⎢        ⎥
                                                ⎣−1 2        0     1⎦         ⎣0     1⎦
                                                  0 0        1    −2            0    0
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                         December 16, 2009   15:4




           52       Systems of Linear Algebraic Equations

                     6. Solve the equations Ax = b by Gauss elimination, where
                                                ⎡            ⎤                      ⎡   ⎤
                                                0 0  2  1  2                          1
                                              ⎢0 1   0  2 −1⎥                      ⎢ 1⎥
                                              ⎢              ⎥                     ⎢    ⎥
                                              ⎢              ⎥                     ⎢    ⎥
                                          A = ⎢1 2   0 −2  0⎥                  b = ⎢ −4 ⎥
                                              ⎢              ⎥                     ⎢    ⎥
                                              ⎣0 0   0 −1  1⎦                      ⎣ −2 ⎦
                                                0 1 −1  1 −1                            −1

                        Hint: reorder the equations before solving.
                     7. Find L and U so that
                                                             ⎡          ⎤
                                                                 4 −1  0
                                                             ⎢          ⎥
                                                  A = LU = ⎣−1      4 −1⎦
                                                                 0 −1  4

                        using (a) Doolittle’s decomposition; (b) Choleski’s decomposition.
                     8. Use Doolittle’ decomposition method to solve Ax = b, where
                                                    ⎡         ⎤                 ⎡   ⎤
                                                    −3  6  −4                    −3
                                                  ⎢           ⎥               ⎢     ⎥
                                              A = ⎣ 9 −8   24⎦            b = ⎣ 65 ⎦
                                                   −12 24 −26                   −42

                     9. Solve the equations AX = b by Doolittle’s decomposition method, where
                                            ⎡                  ⎤                    ⎡      ⎤
                                             2.34  −4.10  1.78                        0.02
                                           ⎢                   ⎥                   ⎢       ⎥
                                       A = ⎣−1.98   3.47 −2.22⎦                b = ⎣ −0.73 ⎦
                                             2.36 −15.17  6.18                       −6.63

                    10. Solve the equations AX = B by Doolittle’s decomposition method, where
                                                    ⎡              ⎤         ⎡            ⎤
                                                  4         −3   6            1         0
                                                ⎢                  ⎥         ⎢            ⎥
                                              A=⎣ 8         −3  10⎦      B = ⎣0         1⎦
                                                 −4         12 −10            0         0

                    11. Solve the equations Ax = b by Choleski’s decomposition method, where
                                                        ⎡       ⎤          ⎡     ⎤
                                                          1 1 1               1
                                                        ⎢       ⎥          ⎢     ⎥
                                                    A = ⎣1 2 2⎦        b = ⎣ 3/2 ⎦
                                                          1 2 3               3

                    12. Solve the equations
                                                ⎡              ⎤⎡ ⎤ ⎡           ⎤
                                                    4   −2  −3    x1        1.1
                                                ⎢              ⎥⎢ ⎥ ⎢           ⎥
                                                ⎣ 12     4 −10⎦ ⎣ x2 ⎦ = ⎣    0⎦
                                                  −16   28  18    x3       −2.3

                        by Doolittle’s decomposition method.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                      December 16, 2009   15:4




               53     2.3 LU Decomposition Methods

                     13. Determine L that results from Choleski’s decomposition of the diagonal matrix
                                                         ⎡                    ⎤
                                                          α1 0       0  ···
                                                         ⎢0
                                                         ⎢      α2 0    · · ·⎥⎥
                                                         ⎢
                                                     A = ⎢0                   ⎥
                                                               0    α 3 · · ·⎥
                                                         ⎣                    ⎦
                                                           ..  ..   ..  ..
                                                            .   .    .      .

                     14.     Modify the function gaussElimin so that it will work with m constant vectors.
                           Test the program by solving AX = B, where
                                                  ⎡               ⎤         ⎡        ⎤
                                                      2 −1      0             1 0 0
                                                  ⎢               ⎥         ⎢        ⎥
                                              A = ⎣−1     2 −1⎦       B = ⎣0 1 0⎦
                                                      0 −1      2             0 0 1

                     15.     A well-known example of an ill-conditioned matrix is the Hilbert matrix
                                                        ⎡                      ⎤
                                                            1 1/2 1/3 · · ·
                                                        ⎢1/2 1/3 1/4 · · ·⎥
                                                        ⎢                      ⎥
                                                   A=⎢                         ⎥
                                                        ⎢1/3 1/4 1/5 · · ·⎥
                                                        ⎣                      ⎦
                                                            ..   ..   ..  ..
                                                             .    .    .     .

                           Write a program that specializes in solving the equations Ax = b by Doolittle’s
                           decomposition method, where A is the Hilbert matrix of arbitrary size n × n, and
                                                                       n
                                                               bi =          A ij
                                                                      j =1

                           The program should have no input apart from n. By running the program, de-
                           termine the largest n for which the solution is within 6 significant figures of the
                           exact solution
                                                                                      T
                                                          x= 1     1       1    ···

                     16. Derive the forward and back substitution algorithms for the solution phase of
                         Choleski’s method. Compare them with the function choleskiSol.
                     17.    Determine the coefficients of the polynomial y = a 0 + a 1 x + a 2 x 2 + a 3 x 3 that
                         passes through the points (0, 10), (1, 35), (3, 31), and (4, 2).
                     18.    Determine the fourth-degree polynomial y(x) that passes through the points
                         (0, −1), (1, 1), (3, 3), (5, 2), and (6, −2).
                     19.    Find the fourth-degree polynomial y(x) that passes through the points (0, 1),
                         (0.75, −0.25), and (1, 1) and has zero curvature at (0, 1) and (1, 1).
                     20.   Solve the equations Ax = b, where
                                              ⎡                               ⎤          ⎡       ⎤
                                                   3.50 2.77 −0.76       1.80               7.31
                                              ⎢−1.80 2.68          3.44 −0.09⎥           ⎢ 4.23 ⎥
                                              ⎢                               ⎥          ⎢       ⎥
                                        A=⎢                                   ⎥    b=⎢           ⎥
                                              ⎣ 0.27 5.07          6.90  1.61⎦           ⎣ 13.85 ⎦
                                                   1.71 5.45       2.68  1.71              11.55

                           By computing |A| and Ax, comment on the accuracy of the solution.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                 December 16, 2009     15:4




           54       Systems of Linear Algebraic Equations

                    21. Compute the condition number of the matrix
                                                        ⎡            ⎤
                                                          1 −1 −1
                                                        ⎢            ⎥
                                                  A = ⎣0      1 −2⎦
                                                          0   0    1

                        based on (a) the euclidean norm and (b) the infinity norm. You may use the func-
                        tion inv(A)in numpy.linalg to determine the inverse of A.
                    22.     Write a function that returns the condition number of a matrix based on the
                        euclidean norm. Test the function by computing the condition number of the
                        ill-conditioned matrix
                                                           ⎡               ⎤
                                                             1   4    9 16
                                                           ⎢ 4   9 16 25⎥
                                                           ⎢               ⎥
                                                      A=⎢                  ⎥
                                                           ⎣ 9 16 25 36⎦
                                                            16 25 36 49

                        Use the function inv(A)in numpy.linalg to determine the inverse of A.



          2.4       Symmetric and Banded Coefficient Matrices
                    Introduction
                    Engineering problems often lead to coefficient matrices that are sparsely populated,
                    meaning that most elements of the matrix are zero. If all the nonzero terms are clus-
                    tered about the leading diagonal, then the matrix is said to be banded. An example of
                    a banded matrix is
                                                       ⎡                  ⎤
                                                         X X 0 0 0
                                                       ⎢X X X 0 0 ⎥
                                                       ⎢                  ⎥
                                                       ⎢                  ⎥
                                                  A = ⎢0 X X X 0⎥
                                                       ⎢                  ⎥
                                                       ⎣ 0 0 X X X⎦
                                                         0 0 0 X X

                    where X’s denote the nonzero elements that form the populated band (some of these
                    elements may be zero). All the elements lying outside the band are zero. The matrix
                    shown above has a bandwidth of 3, because there are at most three nonzero elements
                    in each row (or column). Such a matrix is called tridiagonal.
                        If a banded matrix is decomposed in the form A = LU, both L and U will retain
                    the banded structure of A. For example, if we decomposed the matrix just shown, we
                    would get
                                        ⎡                     ⎤       ⎡                 ⎤
                                          X 0 0         0   0          X    X 0   0   0
                                        ⎢X X 0          0   0⎥        ⎢0    X X   0   0⎥
                                        ⎢                     ⎥       ⎢                 ⎥
                                        ⎢                     ⎥       ⎢                 ⎥
                                    L = ⎢0 X X          0   0⎥    U = ⎢0    0 X   X   0⎥
                                        ⎢                     ⎥       ⎢                 ⎥
                                        ⎣0 0 X          X   0⎦        ⎣0    0 0   X   X⎦
                                          0 0 0         X   X           0   0 0   0   X
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                  December 16, 2009     15:4




               55     2.4 Symmetric and Banded Coefficient Matrices

                     The banded structure of a coefficient matrix can be exploited to save storage and
                     computation time. If the coefficient matrix is also symmetric, further economies are
                     possible. In this section we show how the methods of solution discussed previously
                     can be adapted for banded and symmetric coefficient matrices.


                     Tridiagonal Coefficient Matrix
                     Consider the solution of Ax = b by Doolittle’s decomposition, where A is the n × n
                     tridiagonal matrix
                                                 ⎡                            ⎤
                                                   d1 e1 0        0 ···    0
                                                 ⎢                            ⎥
                                                 ⎢ c 1 d2 e2 0 · · ·       0⎥
                                                 ⎢                            ⎥
                                                 ⎢ 0 c 2 d3 e3 · · ·       0⎥
                                                 ⎢                            ⎥
                                             A=⎢0 0 c            d4 · · ·  0⎥
                                                 ⎢            3               ⎥
                                                 ⎢.     ..  ..    ..       .. ⎥
                                                 ⎢.                   ..      ⎥
                                                 ⎣.      .   .     .     .  .⎦
                                                   0 0 . . . 0 cn−1 dn
                     As the notation implies, we are storing the nonzero elements of A in the vectors
                                                              ⎡      ⎤
                                            ⎡      ⎤             d1           ⎡      ⎤
                                               c1             ⎢ d ⎥              e1
                                            ⎢ c ⎥             ⎢ 2 ⎥           ⎢ e ⎥
                                            ⎢ 2 ⎥             ⎢ . ⎥           ⎢ 2 ⎥
                                            ⎢
                                       c=⎢ . ⎥     ⎥          ⎢
                                                          d=⎢ . ⎥ .  ⎥     e=⎢       ⎥
                                                .                             ⎢ .. ⎥
                                            ⎣ . ⎦             ⎢      ⎥        ⎣ . ⎦
                                                              ⎣ dn−1 ⎦
                                              cn−1                              en−1
                                                                 dn
                     The resulting saving of storage can be significant. For example, a 100 × 100 tridiag-
                     onal matrix, containing 10,000 elements, can be stored in only 99 + 100 + 99 = 298
                     locations, which represents a compression ratio of about 33:1.
                         Let us now apply LU decomposition to the coefficient matrix. We reduce row k
                     by getting rid of ck−1 with the elementary operation

                                 row k ← row k − (ck−1 /dk−1 ) × row (k − 1), k = 2, 3, . . . , n

                     The corresponding change in dk is

                                                      dk ← dk − (ck−1 /dk−1 )ek−1                        (2.21)

                     whereas ek is not affected. In order to finish up with Doolittle’s decomposition of the
                     form [L\U], we store the multiplier λ = ck−1 /dk−1 in the location previously occupied
                     by ck−1 :

                                                          ck−1 ← ck−1 /dk−1                              (2.22)

                     Thus, the decomposition algorithm is

                     for k in range(1,n):
                          lam = c[k-1]/d[k-1]
                          d[k] = d[k] - lam*e[k-1]
                          c[k-1] = lam
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                 December 16, 2009      15:4




           56       Systems of Linear Algebraic Equations

                       Next we look at the solution phase, that is, the solution of Ly = b, followed by
                    Ux = y. The equations Ly = b can be portrayed by the augmented coefficient matrix
                                                  ⎡                              ⎤
                                                    1 0     0 0 · · · 0 b1
                                                  ⎢                              ⎥
                                                  ⎢c1 1     0 0 · · · 0 b2 ⎥
                                                  ⎢                              ⎥
                                                  ⎢ 0 c2 1 0 · · · 0 b 3 ⎥
                                                  ⎢                              ⎥
                                          L b = ⎢0 0 c           1 . . . 0 b4 ⎥
                                                  ⎢            3                 ⎥
                                                  ⎢.    ..   ..  ..        .. .. ⎥
                                                  ⎢.                             ⎥
                                                  ⎣.     .    .   . ···     . .⎦
                                                    0 0 · · · 0 cn−1 1 bn

                    Note that the original contents of c were destroyed and replaced by the multipliers
                    during the decomposition. The solution algorithm for y by forward substitution is

                    y[0] = b[0]
                    for k in range(1,n):
                         y[k] = b[k] - c[k-1]*y[k-1]

                        The augmented coefficient matrix representing Ux = y is
                                              ⎡                                     ⎤
                                               d1 e1 0 · · ·       0     0      y1
                                              ⎢                                     ⎥
                                              ⎢ 0 d2 e2 · · ·      0     0      y2 ⎥
                                              ⎢                                     ⎥
                                              ⎢ 0 0 d3 · · ·       0     0      y3 ⎥
                                              ⎢                                     ⎥
                                    U y =⎢ .        ..   ..         ..    ..     .. ⎥
                                              ⎢ ..                                . ⎥
                                              ⎢      .    .          .     .        ⎥
                                              ⎢                                     ⎥
                                              ⎣ 0 0 0 · · · dn−1 en−1 yn−1 ⎦
                                                0 0 0 ···          0     dn     yn

                    Note again that the contents of d were altered from the original values during the
                    decomposition phase (but e was unchanged). The solution for x is obtained by back
                    substitution using the algorithm

                    x[n-1] = y[n-1]/d[n-1]
                    for k in range(n-2,-1,-1):
                         x[k] = (y[k] - e[k]*x[k+1])/d[k]
                    end do



                      LUdecomp3

                    This module contains the functions LUdecomp3 and LUsolve3 for the decomposi-
                    tion and solution phases of a tridiagonal matrix. In LUsolve3, the vector y writes
                    over the constant vector b during forward substitution. Similarly, the solution vector
                    x overwrites y in the back substitution process. In other words, b contains the solu-
                    tion upon exit from LUsolve3.

                    ## module LUdecomp3
                    ’’’ c,d,e = LUdecomp3(c,d,e).
                         LU decomposition of tridiagonal matrix [c\d\e]. On output
                         {c},{d} and {e} are the diagonals of the decomposed matrix.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                           December 16, 2009     15:4




               57     2.4 Symmetric and Banded Coefficient Matrices

                           x = LUsolve(c,d,e,b).
                           Solves [c\d\e]{x} = {b}, where {c}, {d} and {e} are the
                           vectors returned from LUdecomp3.
                     ’’’


                     def LUdecomp3(c,d,e):
                           n = len(d)
                           for k in range(1,n):
                                 lam = c[k-1]/d[k-1]
                                 d[k] = d[k] - lam*e[k-1]
                                 c[k-1] = lam
                           return c,d,e


                     def LUsolve3(c,d,e,b):
                           n = len(d)
                           for k in range(1,n):
                                 b[k] = b[k] - c[k-1]*b[k-1]
                           b[n-1] = b[n-1]/d[n-1]
                           for k in range(n-2,-1,-1):
                                 b[k] = (b[k] - e[k]*b[k+1])/d[k]
                           return b




                     Symmetric Coefficient Matrices
                     More often than not, coefficient matrices that arise in engineering problems are sym-
                     metric as well as banded. Therefore, it is worthwhile to discover special properties of
                     such matrices and learn how to utilize them in the construction of efficient algo-
                     rithms.
                         If the matrix A is symmetric, then the LU decomposition can be presented in the
                     form


                                                             A = LU = LDLT                                         (2.23)


                     where D is a diagonal matrix. An example is Choleski’s decomposition A = LLT that
                     was discussed in the previous section (in this case, D = I). For Doolittle’s decomposi-
                     tion we have

                                             ⎡                               ⎤⎡                                ⎤
                                             D1        0     0     ···   0      1     L 21   L 31   ···   L n1
                                           ⎢0
                                           ⎢           D2    0     ···   0⎥  ⎥⎢
                                                                               ⎢0      1     L 32   ···   L n2 ⎥
                                                                                                               ⎥
                                           ⎢                                 ⎥⎢                                ⎥
                                 U = DLT = ⎢ 0         0     D3    ···   0 ⎥ ⎢0        0      1     ···   L n3 ⎥
                                           ⎢                                 ⎥⎢                                ⎥
                                           ⎢ ..         ..    ..          .. ⎥ ⎢ ..    ..     ..           .. ⎥
                                           ⎣ .           .     .   ···     . ⎦⎣.        .      .    ···     . ⎦
                                             0         0     0     ···   Dn     0      0      0     ···    1
P1: PHB

CUUS884-Kiusalaas    CUUS884-02      978 0 521 19132 6                                                      December 16, 2009      15:4




           58       Systems of Linear Algebraic Equations

                    which gives

                                                ⎡                                                  ⎤
                                               D1          D1 L 21      D1 L 31    ···     D1 L n1
                                             ⎢0             D2          D2 L 32    ···     D2 L n2 ⎥
                                             ⎢                                                     ⎥
                                             ⎢                                                     ⎥
                                           U=⎢ 0             0           D3        ···     D3 L 3n ⎥                      (2.24)
                                             ⎢                                                     ⎥
                                             ⎢ ..            ..           ..                  .. ⎥
                                             ⎣ .              .            .       ···         . ⎦
                                               0             0            0        ···       Dn


                    We now see that during decomposition of a symmetric matrix only U has to be stored,
                    because D and L can be easily recovered from U. Thus Gauss elimination, which re-
                    sults in an upper triangular matrix of the form shown in Eq. (2.24), is sufficient to
                    decompose a symmetric matrix.
                         There is an alternative storage scheme that can be employed during LU decom-
                    position. The idea is to arrive at the matrix

                                                       ⎡                                        ⎤
                                                      D1         L 21      L 31   ···      L n1
                                                    ⎢0           D2        L 32   ···      L n2 ⎥
                                                    ⎢                                           ⎥
                                                    ⎢                                           ⎥
                                               U∗ = ⎢ 0          0         D3     ···      L n3 ⎥                         (2.25)
                                                    ⎢                                           ⎥
                                                    ⎢..         ..        ..      ..      .. ⎥
                                                    ⎣.           .         .         .     . ⎦
                                                      0          0         0      ···      Dn


                    Here U can be recovered from Uij = Di L ji . It turns out that this scheme leads to a
                    computationally more efficient solution phase; therefore, we adopt it for symmetric,
                    banded matrices.




                    Symmetric, Pentadiagonal Coefficient Matrix
                    We encounter pentadiagonal (bandwidth = 5) coefficient matrices in the solution of
                    fourth-order, ordinary differential equations by finite differences. Often these matri-
                    ces are symmetric, in which case an n × n coefficient matrix has the form

                                      ⎡                                                                      ⎤
                                       d1       e1       f1      0        0         0        ···       0
                                      ⎢                                                                  ⎥
                                      ⎢ e1      d2       e2     f2        0         0        ···       0 ⎥
                                      ⎢                                                                  ⎥
                                      ⎢ f1      e2       d3     e3        f3        0        ···       0 ⎥
                                      ⎢                                                                  ⎥
                                      ⎢0                                                     ···         ⎥
                                      ⎢          f2      e3     d4        e4        f4                 0 ⎥
                                    A=⎢
                                      ⎢ ..        ..      ..     ..        ..        ..      ..        ..⎥
                                                                                                         ⎥                (2.26)
                                      ⎢.           .       .      .         .         .         .       .⎥
                                      ⎢                                                                  ⎥
                                      ⎢0        ···      0     fn−4      en−3     dn−2      en−2    fn−2 ⎥
                                      ⎢                                                                  ⎥
                                      ⎢                                                                  ⎥
                                      ⎣0        ···      0       0       fn−3     en−2      dn−1    en−1 ⎦
                                        0       ···      0       0         0      fn−2      en−1     dn
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                                          December 16, 2009      15:4




               59     2.4 Symmetric and Banded Coefficient Matrices

                     As in the case of tridiagonal matrices, we store the nonzero elements in the three
                     vectors

                                                ⎡ ⎤
                                              d1                         ⎡                ⎤
                                           ⎢      ⎥                            e1                     ⎡           ⎤
                                           ⎢ d2 ⎥                       ⎢      ⎥                            f1
                                           ⎢      ⎥                     ⎢      e2
                                                                               ⎥                      ⎢           ⎥
                                           ⎢ .. ⎥                       ⎢      ⎥                      ⎢     f2    ⎥
                                           ⎢ . ⎥                                ..
                                         d=⎢      ⎥                     ⎢
                                                                      e=⎢      ⎥
                                                                               ⎥ .                  f=⎢
                                                                                                      ⎢      ..   ⎥
                                                                                                                  ⎥
                                           ⎢d     ⎥                     ⎢      ⎥                      ⎣           ⎦
                                           ⎢ n−2 ⎥                      ⎣ en−2 ⎦
                                                                                                              .
                                           ⎢      ⎥
                                           ⎣ dn−1 ⎦                                                       fn−2
                                                                              en−1
                                              dn


                         Let us now look at the solution of the equations Ax = b by Doolittle’s decomposi-
                     tion. The first step is to transform A to upper triangular form by Gauss elimination. If
                     elimination has progressed to the stage where the kth row has become the pivot row,
                     we have the following situation:

                                       ⎡                                                                              ⎤
                                           ..   ..     ..       ..       ..          ..        ..      ..
                                    ⎢ .          .      .        .        .           .         .       .               ⎥
                                    ⎢                                                                                   ⎥
                                    ⎢· · ·      0     dk       ek       fk           0         0       0          · · ·⎥ ←
                                    ⎢                                                                                   ⎥
                                    ⎢· · ·      0     ek      dk+1     ek+1 fk+1               0       0          · · ·⎥⎥
                                  A=⎢
                                    ⎢· · ·
                                    ⎢           0     fk      ek+1     dk+2 ek+2              fk+2     0          · · ·⎥⎥
                                    ⎢                                                                                   ⎥
                                    ⎢· · ·      0     0       fk+1     ek+2 dk+3              ek+3    fk+3        · · ·⎥
                                    ⎣                                                                                   ⎦
                                                 ..     ..       ..      ..   ..                ..      ..        ..
                                                  .      .        .       .    .                 .       .            .


                     The elements ek and fk below the pivot row (the kth row) are eliminated by the oper-
                     ations


                                           row (k + 1) ← row (k + 1) − (ek /dk ) × row k

                                           row (k + 2) ← row (k + 2) − (fk /dk ) × row k


                     The only terms (other than those being eliminated) that are changed by the foregoing
                     operations are


                                                             dk+1 ← dk+1 − (ek /dk )ek

                                                             ek+1 ← ek+1 − (ek /dk )fk                                           (2.27a)

                                                             dk+2 ← dk+2 − (fk /dk )fk


                     Storage of the multipliers in the upper triangular portion of the matrix results in


                                                         ek ← ek /dk            fk ← fk /dk                                      (2.27b)
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                           December 16, 2009      15:4




           60       Systems of Linear Algebraic Equations

                    At the conclusion of the elimination phase, the matrix has the form (do not confuse
                    d, e, and f with the original contents of A)
                                               ⎡                                            ⎤
                                                d1          e1     f1    0     ···     0
                                               ⎢                                           ⎥
                                               ⎢0           d2    e2     f2    ···     0   ⎥
                                               ⎢                                           ⎥
                                               ⎢0           0     d3     e3    ···     0   ⎥
                                             ∗ ⎢                                           ⎥
                                            U =⎢ .           ..     ..    ..           ..  ⎥
                                               ⎢ ..           .      .     .    ···     .  ⎥
                                               ⎢                                           ⎥
                                               ⎢                                           ⎥
                                               ⎣0           0     ···    0     dn−1   en−1 ⎦
                                                 0          0     ···    0       0     dn

                         Now comes the solution phase. The equations Ly = b have the augmented coef-
                    ficient matrix
                                                 ⎡                               ⎤
                                                   1 0 0        0    · · · 0 b1
                                                 ⎢                               ⎥
                                                 ⎢e1 1 0        0    · · · 0 b2 ⎥
                                                 ⎢                               ⎥
                                                 ⎢ f1 e2 1      0    · · · 0 b3 ⎥
                                                 ⎢                               ⎥
                                         L b = ⎢0 f e                · · · 0 b4 ⎥
                                                 ⎢       2    3 1                ⎥
                                                 ⎢.     ..  ..  ..         .. .. ⎥
                                                 ⎢.                              ⎥
                                                 ⎣.      .   .   .   ···    . .⎦
                                                   0 0 0 fn−2 en−1 1 bn

                    Solution by forward substitution yields

                                       y1 = b1

                                       y 2 = b 2 − e1 y 1                                                     (2.28)
                                             ..
                                              .

                                       yk = bk − fk−2 yk−2 − ek−1 yk−1 , k = 3, 4, . . . , n

                    The equations to be solved by back substitution, namely, Ux = y, have the aug-
                    mented coefficient matrix
                                         ⎡                                           ⎤
                                            d1 d1 e1 d1 f1 0      ···    0       y1
                                         ⎢                                           ⎥
                                         ⎢0     d2 d2 e2 d2 f2 · · ·     0       y2 ⎥
                                         ⎢                                           ⎥
                                         ⎢0     0     d3 d3 e3 · · ·     0       y3 ⎥
                                         ⎢                                           ⎥
                                  U y =⎢ .       ..    ..  ..             ..      .. ⎥
                                         ⎢ ..     .     .   .     · · ·    .         ⎥
                                         ⎢                                         . ⎥
                                         ⎢                                           ⎥
                                         ⎣0     0    ···   0     dn−1 dn−1 en−1 yn−1 ⎦
                                            0   0    ···   0        0    dn      yn

                    the solution of which is obtained by back substitution:

                                    xn = yn /dn
                                                               .
                                  xn−1 = yn−1 /dn−1 − en−1 xn ..

                                    xk = yk /dk − ek xk+1 − fk xk+2 , k = n − 2, n − 3, . . . , 1
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                   December 16, 2009    15:4




               61     2.4 Symmetric and Banded Coefficient Matrices

                        LUdecomp5

                     The function LUdecomp5 decomposes a symmetric, pentadiagonal matrix A of the
                     form A = [f\e\d\e\f]. The original vectors d, e, and f are destroyed and replaced by
                     the vectors of the decomposed matrix. After decomposition, the solution of Ax = b
                     can be obtained by LUsolve5. During forward substitution, the original b is replaced
                     by y. Similarly, y is written over by x in the back substitution phase, so that b contains
                     the solution vector upon exit from LUsolve5.


                     ## module LUdecomp5
                     ’’’ d,e,f = LUdecomp5(d,e,f).
                          LU decomposition of symmetric pentadiagonal matrix
                          [f\e\d\e\f]. On output {d},{e} and {f} are the
                          diagonals of the decomposed matrix.


                          x = LUsolve5(d,e,f,b).
                          Solves [f\e\d\e\f]{x} = {b}, where {d}, {e} and {f}
                          are the vectors returned from LUdecomp5.
                          ’’’
                     def LUdecomp5(d,e,f):
                          n = len(d)
                          for k in range(n-2):
                                 lam = e[k]/d[k]
                                 d[k+1] = d[k+1] - lam*e[k]
                                 e[k+1] = e[k+1] - lam*f[k]
                                 e[k] = lam
                                 lam = f[k]/d[k]
                                 d[k+2] = d[k+2] - lam*f[k]
                                 f[k] = lam
                          lam = e[n-2]/d[n-2]
                          d[n-1] = d[n-1] - lam*e[n-2]
                          e[n-2] = lam
                          return d,e,f


                     def LUsolve5(d,e,f,b):
                          n = len(d)
                          b[1] = b[1] - e[0]*b[0]
                          for k in range(2,n):
                                 b[k] = b[k] - e[k-1]*b[k-1] - f[k-2]*b[k-2]
                          b[n-1] = b[n-1]/d[n-1]
                          b[n-2] = b[n-2]/d[n-2] - e[n-2]*b[n-1]
                          for k in range(n-3,-1,-1):
                                 b[k] = b[k]/d[k] - e[k]*b[k+1] - f[k]*b[k+2]
                          return b
P1: PHB

CUUS884-Kiusalaas    CUUS884-02      978 0 521 19132 6                                 December 16, 2009       15:4




           62       Systems of Linear Algebraic Equations

                    EXAMPLE 2.9
                    As a result of Gauss elimination, a symmetric matrix A was transformed to the upper
                    triangular form
                                                   ⎡                       ⎤
                                                    4     −2       1     0
                                                   ⎢0           −3/2     1⎥
                                                   ⎢       3               ⎥
                                                 U=⎢                       ⎥
                                                   ⎣0      0       3 −3/2⎦
                                                    0      0       0 35/12

                    Determine the original matrix A.

                    Solution First, we find L in the decomposition A = LU. Dividing each row of U by its
                    diagonal element yields
                                                     ⎡                  ⎤
                                                      1   −1/2 1/4   0
                                                     ⎢0        −1/2 1/3 ⎥
                                                     ⎢     1            ⎥
                                                LT = ⎢                  ⎥
                                                     ⎣0    0    1   −1/2⎦
                                                      0    0    0    1

                    Therefore, A = LU, or
                                       ⎡                        ⎤⎡                      ⎤
                                        1    0     0           0    4   −2      1     0
                                    ⎢−1/2                      0⎥ ⎢          −3/2     1⎥
                                    ⎢        1     0            ⎥ ⎢0     3              ⎥
                                  A=⎢                           ⎥⎢                      ⎥
                                    ⎣ 1/4 −1/2     1           0⎦ ⎣0     0      3 −3/2⎦
                                        0  1/3 −1/2            1   0     0      0 35/12
                                    ⎡             ⎤
                                      4 −2    1  0
                                    ⎢−2   4 −2   1⎥
                                    ⎢             ⎥
                                   =⎢             ⎥
                                    ⎣ 1 −2    4 −2⎦
                                      0   1 −2   4

                    EXAMPLE 2.10
                    Determine L and D that result from Doolittle’s decomposition A = LDLT of the sym-
                    metric matrix
                                                        ⎡             ⎤
                                                          3 −3       3
                                                        ⎢             ⎥
                                                    A = ⎣−3     5    1⎦
                                                          3     1 10

                    Solution We use Gauss elimination, storing the multipliers in the upper triangular
                    portion of A. At the completion of elimination, the matrix will have the form of U∗ in
                    Eq. (2.25).
                         The terms to be eliminated in the first pass are A 21 and A 31 using the elementary
                    operations

                                                 row 2 ← row 2 − (−1) × row 1

                                                 row 3 ← row 3 − (1) × row 1
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                            December 16, 2009   15:4




               63     2.4 Symmetric and Banded Coefficient Matrices

                     Storing the multipliers (−1 and 1) in the locations occupied by A 12 and A 13 , we get
                                                             ⎡          ⎤
                                                              3 −1 1
                                                             ⎢          ⎥
                                                       A = ⎣0      2 4⎦
                                                              0    4 7

                           The second pass is the operation

                                                           row 3 ← row 3 − 2 × row 2

                     which yields, after overwriting A 23 with the multiplier 2,
                                                                   ⎡             ⎤
                                                                     3 −1       1
                                                                   ⎢             ⎥
                                               A = 0\D\LT = ⎣0           2      2⎦
                                                                     0   0 −1
                     Hence,
                                                       ⎡                ⎤        ⎡          ⎤
                                                     1            0   0           3   0    0
                                                   ⎢                    ⎥        ⎢          ⎥
                                               L = ⎣−1            1   0⎦     D = ⎣0   2    0⎦
                                                     1            2   1           0   0   −1
                     EXAMPLE 2.11
                     Utilize the functions LUdecmp3 and LUsolve3 to solve Ax = b, where
                                             ⎡                      ⎤       ⎡     ⎤
                                                2 −1     0   0    0             5
                                             ⎢−1     2 −1    0    0⎥        ⎢ −5 ⎥
                                             ⎢                      ⎥       ⎢     ⎥
                                             ⎢                      ⎥       ⎢     ⎥
                                         A = ⎢ 0 −1      2 −1     0⎥ b = ⎢ 4 ⎥
                                             ⎢                      ⎥       ⎢     ⎥
                                             ⎣ 0     0 −1    2 −1⎦          ⎣ −5 ⎦
                                                       0      0       0     −1   2              5

                     Solution

                     #!/usr/bin/python
                     ## example2_11
                     from numpy import array,ones
                     from LUdecomp3 import *


                     d = ones((5))*2.0
                     c = ones((4))*(-1.0)
                     b = array([5.0, -5.0, 4.0, -5.0, 5.0])
                     e = c.copy()
                     c,d,e = LUdecomp3(c,d,e)
                     x = LUsolve3(c,d,e,b)
                     print ’’\nx =\n’’,x
                     raw_input(’’\nPress return to exit’’)

                           The output is:

                     x =
                     [ 2. -1.      1. -1.     2.]
P1: PHB

CUUS884-Kiusalaas    CUUS884-02      978 0 521 19132 6                                December 16, 2009         15:4




           64       Systems of Linear Algebraic Equations

          2.5       Pivoting
                    Introduction
                    Sometimes the order in which the equations are presented to the solution algorithm
                    has a profound effect on the results. For example, consider the equations

                                                               2x1 − x2 = 1

                                                         −x1 + 2x2 − x3 = 0

                                                              −x2 + x3 = 0

                    The corresponding augmented coefficient matrix is
                                                      ⎡                  ⎤
                                                          2 −1       0 1
                                                      ⎢                  ⎥
                                              A b = ⎣−1       2 −1 0⎦                                     (a)
                                                          0 −1       1 0

                    Equations (a) are in the “right order” in the sense that we would have no trouble
                    obtaining the correct solution x1 = x2 = x3 = 1 by Gauss elimination or LU decom-
                    position. Now suppose that we exchange the first and third equations, so that the
                    augmented coefficient matrix becomes
                                                         ⎡                 ⎤
                                                             0 −1      1 0
                                                         ⎢                 ⎥
                                                 A b = ⎣−1        2 −1 0⎦                          (b)
                                                             2 −1      0 1

                    Because we did not change the equations (only their order was altered), the solution
                    is still x1 = x2 = x3 = 1. However, Gauss elimination fails immediately as a result of
                    the presence of the zero pivot element (the element A 11 ).
                         The foregoing example demonstrates that it is sometimes essential to reorder the
                    equations during the elimination phase. The reordering, or row pivoting, is also re-
                    quired if the pivot element is not zero, but very small in comparison to other elements
                    in the pivot row, as demonstrated by the following set of equations:
                                                             ⎡                  ⎤
                                                                ε −1      1 0
                                                             ⎢                  ⎥
                                                   A b = ⎣−1          2 −1 0⎦                           (c)
                                                                2 −1      0 1

                    These equations are the same as Eqs. (b), except that the small number ε replaces the
                    zero element in Eq. (b). Therefore, if we let ε → 0, the solutions of Eqs. (b) and (c)
                    should become identical. After the first phase of Gauss elimination, the augmented
                    coefficient matrix becomes
                                                    ⎡                             ⎤
                                                      ε       −1          1     0
                                                    ⎢                             ⎥
                                           A b = ⎣0 2 − 1/ε −1 + 1/ε 0⎦                                (d)
                                                      0 −1 + 2/ε        −2/ε    1

                    Because the computer works with a fixed word length, all numbers are rounded off
                    to a finite number of significant figures. If ε is very small, then 1/ε is huge, and an
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                                December 16, 2009     15:4




               65     2.5 Pivoting

                     element such as 2 − 1/ε is rounded to −1/ε. Therefore, for sufficiently small ε, Eqs.
                     (d) are actually stored as
                                                        ⎡                   ⎤
                                                          ε   −1      1   0
                                                        ⎢                   ⎥
                                                A b = ⎣0 −1/ε 1/ε 0⎦
                                                          0 2/ε −2/ε 1
                     Because the second and third equations obviously contradict each other, the solution
                     process fails again. This problem would not arise if the first and second, or the first
                     and third, equations were interchanged in Eqs. (c) before the elimination.
                         The last example illustrates the extreme case where ε was so small that roundoff
                     errors resulted in total failure of the solution. If we were to make ε somewhat bigger
                     so that the solution would not “bomb” any more, the roundoff errors might still be
                     large enough to render the solution unreliable. Again, this difficulty could be avoided
                     by pivoting.


                     Diagonal Dominance
                     An n × n matrix A is said to be diagonally dominant if each diagonal element is larger
                     than the sum of the other elements in the same row (we are talking here about abso-
                     lute values). Thus, diagonal dominance requires that
                                                                   n
                                                      |A ii | >          A ij (i = 1, 2, ..., n)                       (2.30)
                                                                  j =1
                                                                  j =i

                     For example, the matrix
                                                                  ⎡         ⎤
                                                                    −2  4 −1
                                                                  ⎢         ⎥
                                                                  ⎣ 1 −1   3⎦
                                                                     4 −2  1
                     is not diagonally dominant, but if we rearrange the rows in the following manner:
                                                        ⎡             ⎤
                                                           4 −2      1
                                                        ⎢             ⎥
                                                        ⎣−2     4 −1⎦
                                                           1 −1      3
                     then we have diagonal dominance.
                         It can be shown that if the coefficient matrix of the equations Ax = b is diagonally
                     dominant, then the solution does not benefit from pivoting, that is, the equations are
                     already arranged in the optimal order. It follows that the strategy of pivoting should
                     be to reorder the equations so that the coefficient matrix is as close to diagonal dom-
                     inance as possible. This is the principle behind scaled row pivoting, discussed next.


                     Gauss Elimination with Scaled Row Pivoting
                     Consider the solution of Ax = b by Gauss elimination with row pivoting. Recall that
                     pivoting aims at improving diagonal dominance of the coefficient matrix, that is,
P1: PHB

CUUS884-Kiusalaas     CUUS884-02     978 0 521 19132 6                                           December 16, 2009      15:4




           66       Systems of Linear Algebraic Equations

                    making the pivot element as large as possible in comparison to other elements in the
                    pivot row. The comparison is made easier if we establish an array s with the elements

                                                 si = max A ij , i = 1, 2, ..., n                              (2.31)
                                                         j

                    Thus, si , called the scale factor of row i, contains the absolute value of the largest
                    element in the ith row of A. The vector s can be obtained with the algorithm

                    for i in range(n):
                         s[i] = max(abs(a[i,:]))

                         The relative size of an element A ij (that is, relative to the largest element in the
                    ith row) is defined as the ratio
                                                                             A ij
                                                                 rij =                                         (2.32)
                                                                             si
                       Suppose that the elimination phase has reached the stage where the kth row has
                    become the pivot row. The augmented coefficient matrix at this point is
                                        ⎡                                     ⎤
                                          A 11 A 12 A 13 A 14 · · · A 1n b1
                                        ⎢ 0 A       A 23 A 24 · · · A 2n b2 ⎥
                                        ⎢        22                           ⎥
                                        ⎢                                     ⎥
                                        ⎢ 0     0 A 33 A 34 · · · A 3n b3 ⎥
                                        ⎢                                     ⎥
                                        ⎢ ..    ..    ..    ..        ..  .. ⎥
                                        ⎢ .      .     .     . ···     .   .⎥
                                        ⎢                                     ⎥
                                        ⎢                                     ⎥
                                        ⎢ 0    ···   0 A kk · · · A kn bk ⎥ ←
                                        ⎢ .                                   ⎥
                                        ⎢ .          ..    ..        ..    .. ⎥
                                        ⎣ .    ···    .     .  ···    .     .⎦
                                             0    ···        0        A nk      ···    A nn bn

                    We don’t automatically accept A kk as the next pivot element, but look in the kth col-
                    umn below A kk for a “better” pivot. The best choice is the element A pk that has the
                    largest relative size, that is, we choose p such that

                                                    r pk = max r j k ,                j ≥k
                                                                  j

                    If we find such an element, then we interchange the rows k and p and proceed with
                    the elimination pass as usual. Note that the corresponding row interchange must also
                    be carried out in the scale factor array s. The algorithm that does all this is

                    for k in range(0,n-1):


                      # Find row containing element with largest relative size
                         p = argmax(abs(a[k:n,k])/s[k:n]) + k


                      # If this element is very small, matrix is singular
                         if abs(a[p,k]) < tol: error.err(’Matrix is singular’)


                      # Check whether rows k and p must be interchanged
                         if p != k:
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                             December 16, 2009   15:4




               67     2.5 Pivoting

                            # Interchange rows if needed
                                 swap.swapRows(b,k,p)
                                 swap.swapRows(s,k,p)
                                 swap.swapRows(a,k,p)
                        # Proceed with elimination

                         The Python statement argmax(v) returns the index of the largest element in the
                     vector v. The algorithms for exchanging rows (and columns) are included in the mod-
                     ule swap shown next.


                        swap

                     The function swapRows interchanges rows i and j of a matrix or vector v, whereas
                     swapCols interchanges columns i and j of a matrix.

                     ## module swap
                     ’’’ swapRows(v,i,j).
                           Swaps rows i and j of vector or matrix [v].


                           swapCols(v,i,j).
                           Swaps columns i and j of matrix [v].
                     ’’’
                     def swapRows(v,i,j):
                           if len(v.getshape()) == 1:
                                 v[i],v[j] = v[j],v[i]
                           else:
                                 temp = v[i].copy()
                                 v[i] = v[j]
                                 v[j] = temp


                     def swapCols(v,i,j):
                           temp = v[:,j].copy()
                           v[:,j] = v[:,i]
                           v[:,i] = temp



                        gaussPivot

                     The function gaussPivot performs Gauss elimination with row pivoting. Apart from
                     row swapping, the elimination and solution phases are identical to gaussElimin in
                     Section 2.2.

                     ## module gaussPivot
                     ’’’ x = gaussPivot(a,b,tol=1.0e-9).
                           Solves [a]{x} = {b} by Gauss elimination with
                           scaled row pivoting
P1: PHB

CUUS884-Kiusalaas    CUUS884-02      978 0 521 19132 6                                 December 16, 2009       15:4




           68       Systems of Linear Algebraic Equations

                    ’’’
                    from numpy import zeros,argmax.dot
                    import swap
                    import error


                    def gaussPivot(a,b,tol=1.0e-9):
                          n = len(b)


                      # Set up scale factors
                          s = zeros(n)
                          for i in range(n):
                              s[i] = max(abs(a[i,:]))


                          for k in range(0,n-1):


                            # Row interchange, if needed
                              p = argmax(abs(a[k:n,k])/s[k:n]) + k
                              if abs(a[p,k]) < tol:
                                   error.err(’Matrix is singular’)
                              if p != k:
                                   swap.swapRows(b,k,p)
                                   swap.swapRows(s,k,p)
                                   swap.swapRows(a,k,p)


                            # Elimination
                              for i in range(k+1,n):
                                   if a[i,k] != 0.0:
                                         lam = a[i,k]/a[k,k]
                                         a[i,k+1:n] = a [i,k+1:n] - lam*a[k,k+1:n]
                                         b[i] = b[i] - lam*b[k]
                          if abs(a[n-1,n-1]) < tol:
                              error.err(’Matrix is singular’)


                      # Back substitution
                          for k in range(n-1,-1,-1):
                              b[k] = (b[k] - dot(a[k,k+1:n],b[k+1:n]))/a[k,k]
                          return b



                      LUpivot

                    The Gauss elimination algorithm can be changed to Doolittle’s decomposition with
                    minor changes. The most important of these is keeping a record of the row inter-
                    changes during the decomposition phase. In LUdecomp this record is kept in the
                    array seq. Initially, seq contains [0, 1, 2, . . .]. Whenever two rows are interchanged,
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                              December 16, 2009   15:4




               69     2.5 Pivoting

                     the corresponding interchange is also carried out in seq. Thus seq shows the order
                     in which of the original rows have been rearranged. This information is passed on to
                     the solution phase (LUsolve), which rearranges the elements of the constant vector
                     in the same order before proceeding to forward and back substitutions.

                     ## module LUpivot
                     ’’’ a,seq = LUdecomp(a,tol=1.0e-9).
                        LU decomposition of matrix [a] using scaled row pivoting.
                        The returned matrix [a] = [L\U] contains [U] in the upper
                        triangle and the nondiagonal terms of [L] in the lower triangle.
                        Note that [L][U] is a row-wise permutation of the original [a];
                        the permutations are recorded in the vector {seq}.


                        x = LUsolve(a,b,seq).
                        Solves [L][U]{x} = {b}, where the matrix [a] = [L\U] and the
                        permutation vector {seq} are returned from LUdecomp.
                     ’’’
                     from numpy import argmax,abs,dot,zeros,float,array
                     import swap
                     import error


                     def LUdecomp(a,tol=1.0e-9):
                           n = len(a)
                           seq = array(range(n))


                        # Set up scale factors
                           s = zeros((n),dtype=float)
                           for i in range(n):
                                 s[i] = max(abs(a[i,:]))


                           for k in range(0,n-1):


                            # Row interchange, if needed
                                 p = int(argmax(abs(a[k:n,k])/s[k:n])) + k
                                 if abs(a[p,k]) <     tol:
                                     error.err(’Matrix is singular’)
                                 if p != k:
                                     swap.swapRows(s,k,p)
                                     swap.swapRows(a,k,p)
                                     swap.swapRows(seq,k,p)


                            # Elimination
                                 for i in range(k+1,n):
                                     if a[i,k] != 0.0:
P1: PHB

CUUS884-Kiusalaas    CUUS884-02      978 0 521 19132 6                                December 16, 2009       15:4




           70       Systems of Linear Algebraic Equations

                                         lam = a[i,k]/a[k,k]
                                         a[i,k+1:n] = a[i,k+1:n] - lam*a[k,k+1:n]
                                         a[i,k] = lam
                         return a,seq


                    def LUsolve(a,b,seq):
                         n = len(a)


                      # Rearrange constant vector; store it in [x]
                         x = b.copy()
                         for i in range(n):
                              x[i] = b[seq[i]]


                      # Solution
                         for k in range(1,n):
                              x[k] = x[k] - dot(a[k,0:k],x[0:k])
                         for k in range(n-1,-1,-1):
                             x[k] = (x[k] - dot(a[k,k+1:n],x[k+1:n]))/a[k,k]
                         return x




                    When to Pivot
                    Pivoting has a couple of drawbacks. One of these is the increased cost of computa-
                    tion; the other is the destruction of symmetry and banded structure of the coefficient
                    matrix. The latter is of particular concern in engineering computing, where the co-
                    efficient matrices are frequently banded and symmetric, a property that is utilized
                    in the solution, as seen in the previous article. Fortunately, these matrices are often
                    diagonally dominant as well, so that they would not benefit from pivoting anyway.
                         There are no infallible rules for determining when pivoting should be used. Ex-
                    perience indicates that pivoting is likely to be counterproductive if the coefficient
                    matrix is banded. Positive definite and, to a lesser degree, symmetric matrices also
                    seldom gain from pivoting. And we should not forget that pivoting is not the only
                    means of controlling roundoff errors – there is also double-precision arithmetic.
                         It should be strongly emphasized that the preceding rules of the thumb are only
                    meant for equations that stem from real engineering problems. It is not difficult to
                    concoct “textbook” examples that do not conform to these rules.

                    EXAMPLE 2.12
                    Employ Gauss elimination with scaled     row pivoting to solve the equations Ax = b,
                    where
                                              ⎡                ⎤          ⎡    ⎤
                                                 2 −2         6             16
                                              ⎢                ⎥          ⎢    ⎥
                                          A = ⎣−2      4      3⎦      b = ⎣ 0⎦
                                                −1     8      4             −1
P1: PHB

CUUS884-Kiusalaas   CUUS884-02    978 0 521 19132 6                                   December 16, 2009    15:4




               71     2.5 Pivoting

                     Solution The augmented coefficient matrix and the scale factor array are
                                                ⎡                 ⎤          ⎡ ⎤
                                                    2 −2 6 16                  6
                                                ⎢                 ⎥          ⎢ ⎥
                                        A b = ⎣−2        4 3 0⎦          s = ⎣4⎦
                                                  −1     8 4 −1                8

                     Note that s contains the absolute value of the biggest element in each row of A. At this
                     stage, all the elements in the first column of A are potential pivots. To determine the
                     best pivot element, we calculate the relative sizes of the elements in the first column:
                                                 ⎡      ⎤ ⎡                ⎤ ⎡       ⎤
                                                   r 11       |A 11 | /s 1       1/3
                                                 ⎢      ⎥ ⎢                ⎥ ⎢       ⎥
                                                 ⎣ r 21 ⎦ = ⎣ |A 21 | /s 2 ⎦ = ⎣ 1/2 ⎦
                                                   r 31       |A 31 | /s 3       1/8

                     Because r 21 is the biggest element, we conclude that A 21 makes the best pivot ele-
                     ment. Therefore, we exchange rows 1 and 2 of the augmented coefficient matrix and
                     the scale factor array, obtaining
                                                    ⎡              ⎤             ⎡ ⎤
                                                     −2    4 3 0 ←                4
                                                    ⎢              ⎥             ⎢ ⎥
                                           A b = ⎣ 2 −2 6 16⎦               s = ⎣6⎦
                                                     −1    8 4 −1                 8

                     Now the first pass of Gauss elimination is carried out (the arrow points to the pivot
                     row), yielding
                                                    ⎡                  ⎤         ⎡ ⎤
                                                      −2 4     3     0             4
                                                    ⎢                  ⎥         ⎢ ⎥
                                           A b =⎣ 0 2          9 16⎦         s = ⎣6⎦
                                                       0 6 5/2 −1                  8

                         The potential pivot elements for the next elimination pass are A 22 and A 32 . We
                     determine the “winner” from
                                               ⎡      ⎤ ⎡                ⎤ ⎡       ⎤
                                                  ∗             ∗               ∗
                                               ⎢      ⎥ ⎢                ⎥ ⎢       ⎥
                                               ⎣ r 22 ⎦ = ⎣ |A 22 | /s 2 ⎦ = ⎣ 1/3 ⎦
                                                 r 32       |A 32 | /s 3       3/4

                     Note that r 12 is irrelevant, since row 1 already acted as the pivot row. Therefore, it
                     is excluded from further consideration. As r 32 is bigger than r 22 , the third row is the
                     better pivot row. After interchanging rows 2 and 3, we have
                                                     ⎡                ⎤             ⎡ ⎤
                                                      −2 4      3    0                 4
                                                     ⎢                ⎥             ⎢ ⎥
                                           A b    =  ⎣ 0 6 5/2 −1⎦ ←            s = ⎣8⎦
                                                        0 2     9 16                   6

                     The second elimination pass now yields
                                                              ⎡               ⎤
                                                               −2 4  3    0
                                                              ⎢               ⎥
                                            A   b     = U c = ⎣ 0 6 5/2 −1 ⎦
                                                                0 0 49/6 49/3

                        This completes the elimination phase. It should be noted that U is the matrix that
                     would result from LU decomposition of the following row-wise permutation of A (the
P1: PHB

CUUS884-Kiusalaas     CUUS884-02     978 0 521 19132 6                                    December 16, 2009       15:4




           72       Systems of Linear Algebraic Equations

                    ordering of rows is the same as achieved by pivoting):
                                                       ⎡            ⎤
                                                         −2    4 3
                                                       ⎢            ⎥
                                                       ⎣−1     8 4⎦
                                                          2 −2 6

                    Because the solution of Ux = c by back substitution is not affected by pivoting, we
                    skip the details computation. The result is xT = 1 −1 2 .

                    Alternate Solution
                    It is not necessary to physically exchange equations during pivoting. We could ac-
                    complish Gauss elimination just as well by keeping the equations in place. The elim-
                    ination would then proceed as follows (for the sake of brevity, we skip repeating the
                    details of choosing the pivot equation):
                                                        ⎡                ⎤
                                                            2 −2 6 16
                                                        ⎢                ⎥
                                                A b = ⎣−2       4 3 0⎦ ←
                                                          −1    8 4 −1

                                                          ⎡       ⎤
                                                        0 2  9 16
                                                      ⎢           ⎥
                                                A b = ⎣−2 4  3   0⎦
                                                        0 6 5/2 −1 ←

                                                           ⎡               ⎤
                                                             0 0 49/6 49/3
                                                           ⎢               ⎥
                                                A   b    = ⎣−2 4  3    0 ⎦
                                                             0 6 5/2 −1

                    But now the back substitution phase is a little more involved, because the order in
                    which the equations must be solved has become scrambled. In hand computations
                    this is not a problem, because we can determine the order by inspection. Unfortu-
                    nately, “by inspection” does not work on a computer. To overcome this difficulty, we
                    have to maintain an integer array p that keeps track of the row permutations during
                    the elimination phase. The contents of p indicate the order in which the pivot rows
                    were chosen. In this example, we would have at the end of Gauss elimination
                                                              ⎡ ⎤
                                                                2
                                                              ⎢ ⎥
                                                         p = ⎣3⎦
                                                                1

                    showing that row 2 was the pivot row in the first elimination pass, followed by row 3 in
                    the second pass. The equations are solved by back substitution in the reverse order:
                    equation 1 is solved first for x3 , then equation 3 is solved for x2 , and finally equation
                    2 yields x1 .
                         By dispensing with swapping of equations, the scheme just outlined would prob-
                    ably result in a faster (and more complex) algorithm than gaussPivot, but the
                    number of equations would have to be quite large before the difference becomes
                    noticeable.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02   978 0 521 19132 6                                     December 16, 2009   15:4




               73     2.5 Pivoting

                     PROBLEM SET 2.2
                      1. Solve the equations Ax = b by utilizing Doolittle’s decomposition, where
                                                   ⎡             ⎤            ⎡    ⎤
                                                       3 −3 3                    9
                                                   ⎢             ⎥            ⎢    ⎥
                                               A = ⎣−3       5 1⎦       b = ⎣ −7 ⎦
                                                       3     1 5                12

                      2. Use Doolittle’s decomposition to solve Ax = b, where
                                                 ⎡               ⎤         ⎡      ⎤
                                                    4    8    20               24
                                                 ⎢               ⎥         ⎢      ⎥
                                             A = ⎣ 8 13       16⎦      b = ⎣ 18 ⎦
                                                   20 16 −91                 −119

                      3. Determine L and D that result from Doolittle’s decomposition of the symmetric
                         matrix
                                                    ⎡                        ⎤
                                                         2 −2     0     0   0
                                                    ⎢−2      5 −6       0   0⎥
                                                    ⎢                        ⎥
                                                    ⎢                        ⎥
                                                A = ⎢ 0 −6 16 12            0⎥
                                                    ⎢                        ⎥
                                                    ⎣ 0      0 12 39 −6⎦
                                                         0       0   0   −6   14

                      4. Solve the tridiagonal equations   Ax = b by Doolittle’s decomposition method,
                         where
                                               ⎡                     ⎤             ⎡   ⎤
                                                  6    2     0   0  0                2
                                               ⎢−1
                                               ⎢       7     2   0  0⎥
                                                                     ⎥
                                                                                  ⎢ −3 ⎥
                                                                                  ⎢    ⎥
                                               ⎢                     ⎥            ⎢    ⎥
                                           A = ⎢ 0 −2        8   2  0⎥        b = ⎢ 4⎥
                                               ⎢                     ⎥            ⎢    ⎥
                                               ⎣ 0     0     3   7 −2⎦            ⎣ −3 ⎦
                                                  0    0     0   3  5                    1

                      5. Use Gauss elimination with scaled row pivoting to solve
                                               ⎡              ⎤⎡ ⎤ ⎡            ⎤
                                                   4 −2      1    x1          2
                                               ⎢              ⎥⎢ ⎥ ⎢            ⎥
                                               ⎣−2      1 −1⎦ ⎣ x2 ⎦ = ⎣ −1 ⎦
                                                 −2     3    6    x3          0

                      6. Solve Ax = b by Gauss elimination with scaled row pivoting, where
                                             ⎡                      ⎤         ⎡        ⎤
                                              2.34   −4.10     1.78               0.02
                                             ⎢                      ⎥         ⎢        ⎥
                                         A = ⎣1.98     3.47 −2.22⎦        b = ⎣ −0.73 ⎦
                                              2.36 −15.17      6.81             −6.63

                      7. Solve the equations
                                                ⎡             ⎤⎡ ⎤ ⎡ ⎤
                                                   2 −1  0  0     x1     1
                                                ⎢ 0   0 −1    ⎥ ⎢    ⎥ ⎢
                                                            1⎥ ⎢ x 2 ⎥ ⎢ 0 ⎥
                                                ⎢                          ⎥
                                                ⎢             ⎥⎢ ⎥ = ⎢ ⎥
                                                ⎣ 0 −1   2 −1⎦ ⎣ x3 ⎦ ⎣ 0 ⎦
                                                  −1  2 −1  0     x4     0

                         by Gauss elimination with scaled row pivoting.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02       978 0 521 19132 6                                       December 16, 2009   15:4




           74       Systems of Linear Algebraic Equations

                     8.     Solve the equations
                                              ⎡              ⎤⎡ ⎤ ⎡        ⎤
                                                   0  2  5 −1    x1     −3
                                                 ⎢ 2        0⎥ ⎢ ⎥ ⎢       ⎥
                                                 ⎢    1  3   ⎥ ⎢ x2 ⎥ ⎢ 3 ⎥
                                                 ⎢           ⎥⎢ ⎥ = ⎢      ⎥
                                                 ⎣−2 −1  3  1⎦ ⎣ x3 ⎦ ⎣ −2 ⎦
                                                   3  3 −1  2    x4      5

                     9.     Solve the symmetric, tridiagonal equations

                                                           4x1 − x2 = 9

                                               −xi−1 + 4xi − xi+1 = 5, i = 2, . . . , n − 1

                                                      −xn−1 + 4xn = 5

                          with n = 10.
                    10.     Solve the equations Ax = b, where
                                        ⎡                                        ⎤           ⎡     ⎤
                                         1.3174 2.7250 2.7250             1.7181            8.4855
                                        ⎢0.4002 0.8278 1.2272             2.5322⎥         ⎢ 4.9874 ⎥
                                        ⎢                                        ⎥        ⎢        ⎥
                                    A=⎢                                          ⎥      b=⎢        ⎥
                                        ⎣0.8218 1.5608 0.3629             2.9210⎦         ⎣ 5.6665 ⎦
                                         1.9664 2.0011 0.6532             1.9945            6.6152

                    11.     Solve the equations
                                   ⎡                             ⎤⎡ ⎤ ⎡         ⎤
                                     10   −2 −1    2  3   1 −4  7    x1       0
                                   ⎢                             ⎥⎢ ⎥ ⎢         ⎥
                                   ⎢ 5     11   3 10 −3   3  3 −4⎥ ⎢ x2 ⎥ ⎢ 12 ⎥
                                   ⎢                             ⎥⎢ ⎥ ⎢         ⎥
                                   ⎢ 7                3 −12     3⎥ ⎢ ⎥ ⎢        ⎥
                                   ⎢       12   1  5         2   ⎥ ⎢ x3 ⎥ ⎢ −5 ⎥
                                   ⎢ 8      7 −2                   ⎢    ⎥ ⎢
                                                                4⎥ ⎢ x4 ⎥ ⎢ 3 ⎥
                                                                 ⎥
                                   ⎢               1  3   2  2                  ⎥
                                   ⎢                             ⎥⎢ ⎥ = ⎢       ⎥
                                   ⎢ 2 −15 −1      1  4  −1  8  3⎥ ⎢ x5 ⎥ ⎢ −25 ⎥
                                   ⎢                             ⎥⎢ ⎥ ⎢         ⎥
                                   ⎢ 4                   −1     1⎥ ⎢ ⎥ ⎢        ⎥
                                   ⎢        2   9  1 12      4   ⎥ ⎢ x6 ⎥ ⎢ −26 ⎥
                                   ⎢                             ⎥⎢ ⎥ ⎢         ⎥
                                   ⎣−1      4 −7 −1   1   1 −1 −3⎦ ⎣ x7 ⎦ ⎣ 9 ⎦
                                     −1     3   4  1  3  −4  7  6    x8      −7

                    12.      The system shown in Fig. (a) consists of n linear springs that support n masses.
                          The spring stiffnesses are denoted by ki , the weights of the masses are Wi , and
                          xi are the displacements of the masses (measured from the positions where the
                          springs are undeformed). The displacement formulation is obtained by writing
                          the equilibrium equation of each mass and substituting Fi = ki (xi+1 − xi ) for the
                          spring forces. The result is the symmetric, tridiagonal set of equations

                                                      (k 1 + k 2 )x1 − k 2 x2 = W1

                                     −ki xi−1 + (ki + ki+1 )xi − ki+1 xi+1 = Wi , i = 2, 3, . . . , n − 1

                                                          −kn xn−1 + kn xn = Wn

                          Write a program that solves these equations for given values of n, k, and W. Run
                          the program with n = 5 and

                                          k 1 = k 2 = k 3 = 10 N/mm          k 4 = k 5 = 5 N/mm
                                          W1 = W3 = W5 = 100 N               W2 = W4 = 50 N
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                           December 16, 2009   15:4




               75     2.5 Pivoting




                                             k1                               k1           k2

                                          W1                                         W1
                                                    x1                                            x1
                                               k2                             k3

                                          W2                                    W2         k5
                                                     x2                  x2
                                                k3                            k4
                                               kn
                                                                                    W3
                                           Wn                                                     x3
                                                     xn
                                           (a)                                     (b)
                     13.     The displacement formulation for the mass-spring system shown in Fig. (b)
                           results in the following equilibrium equations of the masses:
                                         ⎡                                    ⎤⎡ ⎤ ⎡         ⎤
                                           k1 + k2 + k3 + k5   −k 3     −k 5      x1      W1
                                         ⎢                                    ⎥⎢ ⎥ ⎢         ⎥
                                         ⎣       −k 3         k3 + k4   −k 4 ⎦ ⎣ x2 ⎦ = ⎣ W2 ⎦
                                                 −k 5          −k 4   k4 + k5     x3      W3

                           where ki are the spring stiffnesses, Wi represent the weights of the masses, and
                           xi are the displacements of the masses from the undeformed configuration of
                           the system. Write a program that solves these equations, given k and W. Use the
                           program to find the displacements if

                                                      k1 = k3 = k4 = k        k 2 = k 5 = 2k
                                                      W1 = W3 = 2W            W2 = W
                     14.

                                                                                     u2
                                                                2.4 m
                                                                                          u1

                                             1.8 m
                                                          u3                         u5
                                                                                          u4
                                                                                     45 kN

                           The displacement formulation for a plane truss is similar to that of a mass-spring
                           system. The differences are: (1) the stiffnesses of the members are ki = (E A /L)i ,
P1: PHB

CUUS884-Kiusalaas    CUUS884-02       978 0 521 19132 6                                          December 16, 2009   15:4




           76       Systems of Linear Algebraic Equations

                          where E is the modulus of elasticity, A represents the cross-sectional area, and L
                          is the length of the member; and (2) there are two components of displacement
                          at each joint. For the statically indeterminate truss shown, the displacement for-
                          mulation yields the symmetric equations Ku = p, where

                                           ⎡                                       ⎤
                                          27.58  7.004         −7.004      0      0
                                        ⎢ 7.004  29.57         −5.253      0 −24.32⎥
                                        ⎢                                          ⎥
                                        ⎢                                          ⎥
                                    K = ⎢−7.004 −5.253          29.57      0      0⎥ MN/m
                                        ⎢                                          ⎥
                                        ⎣     0      0              0  27.58 −7.004⎦
                                              0 −24.32              0 −7.004  29.57




                                                      p= 0     0    0    0    −45    T
                                                                                         kN


                          Determine the displacements ui of the joints.
                    15.


                                                 P6       P6
                                                                        P5
                                               P3                  P4
                                                           P3 P4                   P5
                                                    45o                 45o

                                                 P1       P1        P2        P2
                                                                   18 kN                 12 kN

                          In the force formulation of a truss, the unknowns are the member forces Pi . For
                          the statically determinate truss shown, the equilibrium equations of the joints
                          are
                                       ⎡         √                                    ⎤⎡ ⎤ ⎡ ⎤
                                         −1 1 −1/ 2   0      0                      0     P1       0
                                       ⎢         √                                    ⎥⎢ ⎥ ⎢ ⎥
                                       ⎢ 0  0  1/ 2   1      0                      0⎥ ⎢ P2 ⎥ ⎢ 18 ⎥
                                       ⎢                   √                          ⎥⎢ ⎥ ⎢ ⎥
                                       ⎢ 0 −1      0  0 −1/ 2                       0⎥  ⎢ ⎥ ⎢ ⎥
                                       ⎢                   √                          ⎥ ⎢ P3 ⎥ = ⎢ 0 ⎥
                                       ⎢ 0                                            ⎥
                                                                                    0⎥ ⎢     ⎥ ⎢ ⎥
                                       ⎢    0      0  0  1/ 2
                                                           √                            ⎢ P4 ⎥ ⎢ 12 ⎥
                                       ⎢                                              ⎥⎢ ⎥ ⎢ ⎥
                                       ⎣ 0  0      0  0  1/ 2                       1⎦ ⎣ P5 ⎦ ⎣ 0 ⎦
                                                           √
                                          0 0      0 −1 −1/ 2                       0     P6       0

                          where the units of Pi are kN. (a) Solve the equations as they are with a computer
                          program. (b) Rearrange the rows and columns so as to obtain a lower triangu-
                          lar coefficient matrix, and then solve the equations by back substitution using a
                          calculator.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02     978 0 521 19132 6                                                        December 16, 2009   15:4




               77     2.5 Pivoting

                     16.


                                                                   P4          P4
                                                    P2             P3          P3             P2

                                              P2         θ θ                           θ θ          P2
                                                                    P3         P3
                                                       P1          P1             P1          P1

                                                P5                  Load = 1                       P5

                           The force formulation of the symmetric truss shown results in the joint equilib-
                           rium equations
                                                   ⎡                            ⎤⎡ ⎤ ⎡ ⎤
                                                     c         1    0 0       0    P1       0
                                                   ⎢0          s    0 0       1⎥ ⎢ P ⎥ ⎢0⎥
                                                   ⎢                            ⎥⎢ 2⎥ ⎢ ⎥
                                                   ⎢                            ⎥⎢ ⎥ ⎢ ⎥
                                                   ⎢0          0   2s 0       0⎥ ⎢ P3 ⎥ = ⎢ 1 ⎥
                                                   ⎢                            ⎥⎢ ⎥ ⎢ ⎥
                                                   ⎣0         −c    c 1       0⎦ ⎣ P4 ⎦ ⎣ 0 ⎦
                                                     0         s    s 0       0          P5         0

                           where s = sin θ, c = cos θ , and Pi are the unknown forces. Write a program that
                           computes the forces, given the angle θ. Run the program with θ = 53◦ .
                     17.


                                                            20 Ω               5Ω
                                                                                              220 V
                                                                   i3
                                                                         15Ω
                                              5Ω




                                                         R                          i1
                                                         i2
                                                                                              0V
                                                            10Ω

                           The electrical network shown can be viewed as consisting of three loops. Apply-
                           ing Kirchoff’s law ( voltage drops = voltage sources) to each loop yields the
                           following equations for the loop currents i1 , i2 , and i3 :

                                                                        5i1 + 15(i1 − i3 ) = 220 V

                                                               R(i2 − i3 ) + 5i2 + 10i2 = 0

                                                   20i3 + R(i3 − i2 ) + 15(i3 − i1 ) = 0

                           Compute the three loop currents for R = 5, 10, and 20                        .
P1: PHB

CUUS884-Kiusalaas    CUUS884-02          978 0 521 19132 6                                                  December 16, 2009     15:4




           78       Systems of Linear Algebraic Equations

                    18.


                                            -120 V                           i1                +120 V
                                                             50 Ω                   30Ω




                                                  15 Ω




                                                                        10 Ω




                                                                                               5Ω
                                                                i2                      i3


                                                              25 Ω                 20 Ω




                                                                                               15 Ω
                                                  10 Ω
                                                                            i4


                                                                          30Ω


                          Determine the loop currents i1 to i4 in the electrical network shown.
                    19.    Consider the n simultaneous equations Ax = b, where

                                                             n−1
                               A ij = (i + j )2      bi =           A ij , i = 0, 1, . . . , n − 1,     j = 0, 1, . . . , n − 1
                                                             j =0


                                                                                    T
                          Clearly, the solution is x = 1 1 · · · 1 . Write a program that solves these
                          equations for any given n (pivoting is recommended). Run the program with n =
                          2, 3, and 4 and comment on the results.
                    20.


                                         8 m3/s          6m3/s                    3 m 3/s             2 m 3/s
                                  c1                c2                    c3                 c4                  c5
                                        4m3/s            2 m3/s                   5 m3/s              4m3/s
                        4m3/s                               6m3/s                                                 2 m 3/s
                     c = 20 mg/m3                                                                             c = 15 mg/m3


                          The diagram shows five mixing vessels connected by pipes. Water is pumped
                          through the pipes at the steady rates shown on the diagram. The incoming wa-
                          ter contains a chemical, the amount of which is specified by its concentration
                          c (mg/m3 ). Applying the principle of conservation of mass


                                       mass of chemical flowing in = mass of chemical flowing out
P1: PHB

CUUS884-Kiusalaas       CUUS884-02      978 0 521 19132 6                                December 16, 2009     15:4




                          ∗
               79             2.6 Matrix Inversion

                                to each vessel, we obtain the following simultaneous equations for the concen-
                                trations ci within the vessels:

                                                                  −8c1 + 4c2 = −80

                                                             8c1 − 10c2 + 2c3 = 0

                                                             6c2 − 11c3 + 5c4 = 0

                                                              3c3 − 7c4 + 4c5 = 0

                                                                   2c4 − 4c5 = −30

                                Note that the mass flow rate of the chemical is obtained by multiplying the vol-
                                ume flow rate of the water by the concentration. Verify the equations and deter-
                                mine the concentrations.
                         21.


                                     2m/s3                       4 m/s3                       3m/s3
                                                        c1                      c2
                                  c = 25 mg/m3
                                                                 2m/s3
                                               4 m/s3                                4 m/s3
                                                                 3m/s3
                                                                                              1 m/s3
                                                        c3                      c4
                                                                 1 m/s 3
                                                                                          c = 50 mg/m3

                                Four mixing tanks are connected by pipes. The fluid in the system is pumped
                                through the pipes at the rates shown in the figure. The fluid entering the system
                                contains a chemical of concentration c as indicated. Determine the concentra-
                                tion of the chemical in the four tanks, assuming a steady state.


              ∗
                  2.6    Matrix Inversion

                         Computing the inverse of a matrix and solving simultaneous equations are related
                         tasks. The most economical way to invert an n × n matrix A is to solve the equations

                                                                   AX = I                                    (2.33)

                         where I is the n × n identity matrix. The solution X, also of size n × n, will be the
                         inverse of A. The proof is simple: after we premultiply both sides of Eq. (2.33) by A−1 ,
                         we have A−1 AX = A−1 I, which reduces to X = A−1 .
                             Inversion of large matrices should be avoided whenever possible because of its
                         high cost. As seen from Eq. (2.33), inversion of A is equivalent to solving Axi = bi with
                         i = 1, 2, . . . , n, where bi is the ith column of I. Assuming that LU decomposition is
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                 December 16, 2009      15:4




           80       Systems of Linear Algebraic Equations

                    employed in the solution, the solution phase (forward and back substitution) must be
                    repeated n times, once for each bi . Because the cost of computation is proportional
                    to n3 for the decomposition phase and n2 for each vector of the solution phase, the
                    cost of inversion is considerably more expensive than the solution of Ax = b (single
                    constant vector b).
                         Matrix inversion has another serious drawback – a banded matrix loses its struc-
                    ture during inversion. In other words, if A is banded or otherwise sparse, then A−1 is
                    fully populated. However, the inverse of a triangular matrix remains triangular.

                    EXAMPLE 2.13
                    Write a function that inverts a matrix using LU decomposition with pivoting. Test the
                    function by inverting
                                                        ⎡                ⎤
                                                           0.6 −0.4 1.0
                                                        ⎢                ⎥
                                                    A = ⎣−0.3     0.2 0.5⎦
                                                           0.6 −1.0 0.5
                    Solution The function matInv listed here uses the decomposition and solution pro-
                    cedures in the module LUpivot.
                    #!/usr/bin/python
                    ## example2_13
                    from numpy import array,identity,dot
                    from LUpivot import *


                    def matInv(a):
                         n = len(a[0])
                         aInv = identity(n)
                         a,seq = LUdecomp(a)
                         for i in range(n):
                              aInv[:,i] = LUsolve(a,aInv[:,i],seq)
                         return aInv


                    a = array([[ 0.6, -0.4,             1.0],\
                                  [-0.3,      0.2,      0.5],\
                                  [ 0.6, -1.0,          0.5]])
                    aOrig = a.copy()          # Save original [a]
                    aInv = matInv(a)          # Invert [a] (original [a] is destroyed)
                    print "\naInv =\n",aInv
                    print "\nCheck: a*aInv =\n", dot(aOrig,aInv)
                    raw_input("\nPress return to exit")

                        The output is
                    aInv =
                    [[ 1.66666667 -2.22222222 -1.11111111]
                     [ 1.25             -0.83333333 -1.66666667]
                     [ 0.5               1.               0.        ]]
P1: PHB

CUUS884-Kiusalaas   CUUS884-02      978 0 521 19132 6                                 December 16, 2009   15:4




                      ∗
               81         2.6 Matrix Inversion

                     Check: a*aInv =
                     [[      1.00000000e+00       -4.44089210e-16       -1.11022302e-16]
                      [      0.00000000e+00         1.00000000e+00         5.55111512e-17]
                      [      0.00000000e+00       -3.33066907e-16          1.00000000e+00]]


                     EXAMPLE 2.14
                     Invert the matrix
                                                        ⎡             ⎤
                                                     2 −1  0  0  0  0
                                                   ⎢                  ⎥
                                                   ⎢−1  2 −1  0  0  0⎥
                                                   ⎢                  ⎥
                                                   ⎢ 0 −1  2 −1  0  0⎥
                                                   ⎢
                                                 A=⎢                  ⎥
                                                   ⎢ 0  0 −1  2 −1  0⎥⎥
                                                   ⎢                  ⎥
                                                   ⎣ 0  0  0 −1  2 −1⎦
                                                     0  0  0  0 −1  5

                     Solution Because the matrix is tridiagonal, we solve AX = I using the functions in the
                     module LUdecomp3 (LU decomposition of tridiagonal matrices).

                     #!/usr/bin/python
                     ## example2_14
                     from numpy import ones,identity
                     from LUdecomp3 import *


                     n = 6
                     d = ones((n))*2.0
                     e = ones((n-1))*(-1.0)
                     c = e.copy()
                     d[n-1] = 5.0
                     aInv = identity(n)
                     c,d,e = LUdecomp3(c,d,e)
                     for i in range(n):
                             aInv[:,i] = LUsolve3(c,d,e,aInv[:,i])
                     print ’’\nThe inverse matrix is:\n’’,aInv
                     raw_input(’’\nPress return to exit’’)


                            Running the program results in the following output:

                     The inverse matrix is:
                     [[ 0.84       0.68    0.52     0.36    0.2    0.04]
                      [ 0.68       1.36    1.04     0.72    0.4    0.08]
                      [ 0.52       1.04    1.56     1.08    0.6    0.12]
                      [ 0.36       0.72    1.08     1.44    0.8    0.16]
                      [ 0.2        0.4     0.6      0.8     1.     0.2 ]
                      [ 0.04       0.08    0.12     0.16    0.2    0.24]]]


                            Note that A is tridiagonal, whereas A−1 is fully populated.
P1: PHB

CUUS884-Kiusalaas     CUUS884-02      978 0 521 19132 6                                                      December 16, 2009   15:4




           82       Systems of Linear Algebraic Equations

          ∗
              2.7   Iterative Methods
                    Introduction
                    So far, we have discussed only direct methods of solution. The common character-
                    istic of these methods is that they compute the solution with a finite number of op-
                    erations. Moreover, if the computer were capable of infinite precision (no roundoff
                    errors), the solution would be exact.
                         Iterative, or indirect methods, start with an initial guess of the solution x and then
                    repeatedly improve the solution until the change in x becomes negligible. Because
                    the required number of iterations can be large, the indirect methods are, in general,
                    slower than their direct counterparts. However, iterative methods do have the follow-
                    ing advantages that make them attractive for certain problems:

                    1. It is feasible to store only the nonzero elements of the coefficient matrix. This
                       makes it possible to deal with very large matrices that are sparse, but not neces-
                       sarily banded. In many problems, there is no need to store the coefficient matrix
                       at all.
                    2. Iterative procedures are self-correcting, meaning that roundoff errors (or even
                       arithmetic mistakes) in one iterative cycle are corrected in subsequent cycles.

                         A serious drawback of iterative methods is that they do not always converge to
                    the solution. It can be shown that convergence is guaranteed only if the coefficient
                    matrix is diagonally dominant. The initial guess for x plays no role in determining
                    whether convergence takes place – if the procedure converges for one starting vector,
                    it would do so for any starting vector. The initial guess affects only the number of
                    iterations that are required for convergence.


                    Gauss–Seidel Method
                    The equations Ax = b are in scalar notation
                                                       n
                                                             A ij x j = bi , i = 1, 2, ..., n
                                                      j =1


                    Extracting the term containing xi from the summation sign yields
                                                               n
                                                A ii xi +           A ij x j = bi , i = 1, 2, ..., n
                                                             j =1
                                                             j =i


                    Solving for xi , we get
                                                           ⎛                       ⎞
                                                      1 ⎜                             ⎟
                                                                       n
                                              xi =        ⎜bi −              A ij x j ⎟
                                                     A ii ⎝                           ⎠ , i = 1, 2, ..., n
                                                                      j =1
                                                                      j =i
P1: PHB

CUUS884-Kiusalaas   CUUS884-02        978 0 521 19132 6                                                   December 16, 2009     15:4




                      ∗
               83         2.7 Iterative Methods

                            The last equation suggests the following iterative scheme:
                                                      ⎛               ⎞
                                                         1 ⎜                           ⎟
                                                                        n
                                                 xi ←        ⎜bi −            A ij x j ⎟
                                                        A ii ⎝                         ⎠ , i = 1, 2, ..., n                   (2.34)
                                                                       j =1
                                                                       j =i

                     We start by choosing the starting vector x. If a good guess for the solution is not avail-
                     able, x can be chosen randomly. Equation (2.34) is then used to recompute each ele-
                     ment of x, always using the latest available values of x j . This completes one iteration
                     cycle. The procedure is repeated until the changes in x between successive iteration
                     cycles become sufficiently small.
                          Convergence of the Gauss–Seidel method can be improved by a technique
                     known as relaxation. The idea is to take the new value of xi as a weighted average
                     of its previous value and the value predicted by Eq. (2.34). The corresponding itera-
                     tive formula is
                                              ⎛               ⎞
                                                 ω ⎜                          ⎟
                                                               n
                                          xi ←        ⎜bi −          A ij x j ⎟
                                                 A ii ⎝                       ⎠ + (1 − ω)xi , i = 1, 2, ..., n                (2.35)
                                                              j =1
                                                              j =i

                     where the weight ω is called the relaxation factor. It can be seen that if ω = 1, no
                     relaxation takes place, because Eqs. (2.34) and (2.35) produce the same result. If ω <
                     1, Eq. (2.35) represents interpolation between the old xi and the value given by Eq.
                     (2.34). This is called under-relaxation. In cases where ω > 1, we have extrapolation,
                     or over-relaxation.
                         There is no practical method of determining the optimal value of ω beforehand;
                     however, a good estimate can be computed during run time. Let x (k) = x(k−1) − x(k)
                     be the magnitude of the change in x during the kth iteration (carried out without
                     relaxation, that is, with ω = 1). If k is sufficiently large (say, k ≥ 5), it can be shown2
                     that an approximation of the optimal value of ω is

                                                                                    2
                                                    ωopt ≈                                                                    (2.36)
                                                                                                    1/p
                                                              1+        1−        x (k+p) / x (k)

                     where p is a positive integer.
                        The essential elements of a Gauss–Seidel algorithm with relaxation are:

                         1. Carry out k iterations with ω = 1 (k = 10 is reasonable). After the kth iteration,
                            record x (k) .
                         2. Perform an additional p iterations and record x (k+p) for the last iteration.
                         3. Perform all subsequent iterations with ω = ωopt , where ωopt is computed from
                            Eq. (2.36).


                     2    See, for example, Terrence J. Akai, Applied Numerical Methods for Engineers (John Wiley & Sons,
                          1994), p. 100.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02     978 0 521 19132 6                                December 16, 2009      15:4




           84       Systems of Linear Algebraic Equations

                      gaussSeidel

                    The function gaussSeidel is an implementation of the Gauss–Seidel method with
                    relaxation. It automatically computes ωopt from Eq. (2.36) using k = 10 and p = 1.
                    The user must provide the function iterEqs that computes the improved x from
                    the iterative formulas in Eq. (2.35) – see Example 2.17. The function gaussSeidel
                    returns the solution vector x, the number of iterations carried out, and the value of
                    ωopt used.

                    ## module gaussSeidel
                    ’’’ x,numIter,omega = gaussSeidel(iterEqs,x,tol = 1.0e-9)
                          Gauss-Seidel method for solving [A]{x} = {b}.
                          The matrix [A] should be sparse. User must supply the
                          function iterEqs(x,omega) that returns the improved {x},
                          given the current {x} (’omega’ is the relaxation factor).
                    ’’’
                    from numpy import dot
                    from math import sqrt


                    def gaussSeidel(iterEqs,x,tol = 1.0e-9):
                          omega = 1.0
                          k = 10
                          p = 1
                          for i in range(1,501):
                              xOld = x.copy()
                              x = iterEqs(x,omega)
                              dx = sqrt(dot(x-xOld,x-xOld))
                              if dx < tol: return x,i,omega
                            # Compute of relaxation factor after k+p iterations
                              if i == k: dx1 = dx
                              if i == k + p:
                                   dx2 = dx
                                   omega = 2.0/(1.0 + sqrt(1.0 - (dx2/dx1)**(1.0/p)))
                          print ’Gauss-Seidel failed to converge’



                    Conjugate Gradient Method
                    Consider the problem of finding the vector x that minimizes the scalar function
                                                                  1 T
                                                        f (x) =     x Ax − bT x                    (2.37)
                                                                  2
                    where the matrix A is symmetric and positive definite. Because f (x) is minimized
                    when its gradient ∇ f = Ax − b is zero, we see that minimization is equivalent to
                    solving

                                                                  Ax = b                           (2.38)
P1: PHB

CUUS884-Kiusalaas   CUUS884-02      978 0 521 19132 6                                  December 16, 2009     15:4




                      ∗
               85         2.7 Iterative Methods

                          Gradient methods accomplish the minimization by iteration, starting with an
                     initial vector x0 . Each iterative cycle k computes a refined solution

                                                          xk+1 = xk + α k sk                               (2.39)

                     The step length α k is chosen so that xk+1 minimizes f (xk+1 ) in the search direction sk .
                     That is, xk+1 must satisfy Eq. (2.38):

                                                          A(xk + α k sk ) = b                                 (a)

                     When we introduce the residual

                                                            rk = b − Axk                                   (2.40)

                     Eq. (a) becomes αAsk = rk . Premultiplying both sides by skT and solving for α k , we
                     obtain

                                                                    skT rk
                                                            αk =                                           (2.41)
                                                                   skT Ask

                          We are still left with the problem of determining the search direction sk . Intuition
                     tells us to choose sk = −∇ f = rk , because this is the direction of the largest negative
                     change in f (x). The resulting procedure is known as the method of steepest descent. It
                     is not a popular algorithm because its convergence can be slow. The more efficient
                     conjugate gradient method uses the search direction

                                                         sk+1 = rk+1 + β k sk                              (2.42)

                     The constant β k is chosen so that the two successive search directions are conjugate
                     to each other, meaning

                                                             T
                                                            sk+1 Ask = 0                                     (b)

                     The great attraction of conjugate gradients is that minimization in one conjugate di-
                     rection does not undo previous minimizations (minimizations do not interfere with
                     one another).
                          Substituting sk+1 from Eq. (2.42) into Eq. (b), we get

                                                         T
                                                        rk+1 + β k skT Ask = 0

                     which yields
                                                                    T
                                                                   rk+1 Ask
                                                          βk = −                                           (2.43)
                                                                    skT Ask

                            Here is the outline of the conjugate gradient algorithm:

                       • Choose x0 (any vector will do, but one close to solution results in fewer iterations)
                       • r0 ← b − Ax0
P1: PHB

CUUS884-Kiusalaas     CUUS884-02        978 0 521 19132 6                                   December 16, 2009        15:4




           86       Systems of Linear Algebraic Equations

                     • s0 ← r0 (lacking a previous search direction, choose the direction of steepest
                       descent)
                     • do with k = 0, 1, 2, . . .

                               skT rk
                       αk ←
                              skT Ask
                                  xk+1 ← xk + α k sk
                                 rk+1 ← b − Axk+1
                                 if |rk+1 | ≤ ε exit loop (ε is the error tolerance)
                                             T
                                            rk+1 Ask
                                 βk ← −
                                           skT Ask
                                 sk+1   ← rk+1 + β k sk

                         end do
                         It can be shown that the residual vectors r1 , r2 , r3 , . . . produced by the algorithm
                    are mutually orthogonal, that is, ri · r j = 0, i = j . Now suppose that we have carried
                    out enough iterations to have computed the whole set of n residual vectors. The resid-
                    ual resulting from the next iteration must be a null vector (rn+1 = 0), indicating that
                    the solution has been obtained. It thus appears that the conjugate gradient algorithm
                    is not an iterative method at all, because it reaches the exact solution after n compu-
                    tational cycles. In practice, however, convergence is usually achieved in fewer than n
                    iterations.
                         The conjugate gradient method is not competitive with direct methods in the
                    solution of small sets of equations. Its strength lies in the handling of large, sparse
                    systems (where most elements of A are zero). It is important to note that A enters the
                    algorithm only through its multiplication by a vector, that is, in the form Av, where v
                    is a vector (either xk+1 or sk ). If A is sparse, it is possible to write an efficient subrou-
                    tine for the multiplication and pass it, rather than A itself, to the conjugate gradient
                    algorithm.


                      conjGrad

                    The function conjGrad shown here implements the conjugate gradient algorithm.
                    The maximum allowable number of iterations is set to n (the number of unknowns).
                    Note that conjGrad calls the function Av, which returns the product Av. This func-
                    tion must be supplied by the user (see Example 2.18). We must also supply the start-
                    ing vector x0 and the constant (right-hand-side) vector b. The function returns the
                    solution vector x and the number of iterations.

                    ## module conjGrad
                    ’’’ x, numIter = conjGrad(Av,x,b,tol=1.0e-9)
                         Conjugate gradient method for solving [A]{x} = {b}.
                         The matrix [A] should be sparse. User must supply
                         the function Av(v) that returns the vector [A]{v}.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02      978 0 521 19132 6                                  December 16, 2009   15:4




                      ∗
               87         2.7 Iterative Methods

                     ’’’
                     from numpy import dot
                     from math import sqrt


                     def conjGrad(Av,x,b,tol=1.0e-9):
                             n = len(b)
                             r = b - Av(x)
                             s = r.copy()
                             for i in range(n):
                                  u = Av(s)
                                  alpha = dot(s,r)/dot(s,u)
                                  x = x + alpha*s
                                  r = b - Av(x)
                                  if(sqrt(dot(r,r))) < tol:
                                       break
                                  else:
                                       beta = -dot(r,u)/dot(s,u)
                                       s = r + beta*s
                             return x,i


                     EXAMPLE 2.15
                     Solve the equations
                                                   ⎡         ⎤⎡ ⎤ ⎡         ⎤
                                                     4 −1  1    x1       12
                                                   ⎢         ⎥⎢ ⎥ ⎢         ⎥
                                                   ⎣−1  4 −2⎦ ⎣ x2 ⎦ = ⎣ −1 ⎦
                                                     1 −2  4    x3        5

                     by the Gauss–Seidel method without relaxation.

                     Solution With the given data, the iteration formulas in Eq. (2.34) become

                                                               1
                                                           x1 =  (12 + x2 − x3 )
                                                               4
                                                               1
                                                           x2 = (−1 + x1 + 2x3 )
                                                               4
                                                               1
                                                           x3 = (5 − x1 + 2x2 )
                                                               4

                     Choosing the starting values x1 = x2 = x3 = 0, the first iteration gives us

                                                            1
                                                        x1 =  (12 + 0 − 0) = 3
                                                            4
                                                            1
                                                        x2 = [−1 + 3 + 2(0)] = 0.5
                                                            4
                                                            1
                                                        x3 = [5 − 3 + 2(0.5)] = 0.75
                                                            4
P1: PHB

CUUS884-Kiusalaas     CUUS884-02      978 0 521 19132 6                                 December 16, 2009    15:4




           88       Systems of Linear Algebraic Equations

                    The second iteration yields

                                              1
                                          x1 =  (12 + 0.5 − 0.75) = 2.9375
                                              4
                                              1
                                          x2 = [−1 + 2.9375 + 2(0.75)] = 0.859 38
                                              4
                                              1
                                          x3 = [5 − 2.9375 + 2(0.859 38)] = 0 .945 31
                                              4
                    and the third iteration results in
                                            1
                                        x1 =  (12 + 0.85938 − 0 .94531) = 2.978 52
                                            4
                                            1
                                        x2 = [−1 + 2.97852 + 2(0 .94531)] = 0.967 29
                                            4
                                            1
                                        x3 = [5 − 2.97852 + 2(0.96729)] = 0.989 02
                                            4
                        After five more iterations the results would agree with the exact solution x1 = 3,
                    x2 = x3 = 1 within five decimal places.

                    EXAMPLE 2.16
                    Solve the equations in Example 2.15 by the conjugate gradient method.

                    Solution The conjugate gradient method should converge after three iterations.
                    Choosing again for the starting vector
                                                                           T
                                                          x0 = 0   0   0

                    the computations outlined in the text proceed as follows:


                    First iteration
                                                    ⎡  ⎤ ⎡          ⎤⎡ ⎤ ⎡         ⎤
                                                    12       4 −1  1    0       12
                                                  ⎢    ⎥ ⎢          ⎥⎢ ⎥ ⎢         ⎥
                                   r0 = b − Ax0 = ⎣ −1 ⎦ − ⎣−1  4 −2⎦ ⎣ 0 ⎦ = ⎣ −1 ⎦
                                                     5       1 −2  4    0        5

                                                                   ⎡     ⎤
                                                                      12
                                                                    ⎢    ⎥
                                                          s0 = r0 = ⎣ −1 ⎦
                                                                       5

                                                   ⎡        ⎤⎡     ⎤ ⎡       ⎤
                                                     4 −1  1    12        54
                                                   ⎢        ⎥⎢     ⎥ ⎢       ⎥
                                             As0 = ⎣−1  4 −2⎦ ⎣ −1 ⎦ = ⎣ −26 ⎦
                                                     1 −2  4     5        34


                                              s0T r0        122 + (−1)2 + 52
                                      α0 =           =                            = 0.201 42
                                             s0T As0   12(54) + (−1)(−26) + 5(34)
P1: PHB

CUUS884-Kiusalaas   CUUS884-02        978 0 521 19132 6                                  December 16, 2009   15:4




                      ∗
               89         2.7 Iterative Methods
                                                         ⎡ ⎤              ⎡    ⎤ ⎡              ⎤
                                                           0                12        2.41 704
                                                         ⎢ ⎥              ⎢    ⎥ ⎢              ⎥
                                      x1 = x0 + α 0 s0 = ⎣ 0 ⎦ + 0.201 42 ⎣ −1 ⎦ = ⎣ −0. 201 42 ⎦
                                                           0                 5        1.007 10

                     Second iteration
                                                ⎡ ⎤ ⎡          ⎤⎡             ⎤ ⎡            ⎤
                                               12       4 −1  1     2.417 04        1.123 32
                                             ⎢    ⎥ ⎢          ⎥⎢             ⎥ ⎢            ⎥
                              r1 = b − Ax1 = ⎣ −1 ⎦ − ⎣−1  4 −2⎦ ⎣ −0. 201 42 ⎦ = ⎣ 4.236 92 ⎦
                                                5       1 −2  4     1.007 10       −1.848 28


                                      r1T As0    1.123 32(54) + 4.236 92(−26) − 1.848 28(34)
                             β0 = −           =−                                             = 0.133 107
                                      s0T As0            12(54) + (−1)(−26) + 5(34)

                                                        ⎡       ⎤            ⎡    ⎤ ⎡            ⎤
                                                       1.123 32                12       2.720 76
                                                    ⎢           ⎥            ⎢    ⎥ ⎢            ⎥
                                 s1 = r1 + β 0 s0 = ⎣ 4.236 92 ⎦ + 0.133 107 ⎣ −1 ⎦ = ⎣ 4.103 80 ⎦
                                                      −1.848 28                 5      −1.182 68

                                                ⎡                   ⎤⎡           ⎤ ⎡            ⎤
                                                4           −1    1     2.720 76       5.596 56
                                              ⎢                     ⎥⎢           ⎥ ⎢            ⎥
                                        As1 = ⎣−1            4   −2⎦ ⎣ 4.103 80 ⎦ = ⎣ 16.059 80 ⎦
                                                1           −2    4    −1.182 68     −10.217 60


                                      s1T r1
                              α1 =
                                     s1T As1
                                      2.720 76(1.123 32) + 4.103 80(4.236 92) + (−1.182 68)(−1.848 28)
                                 =
                                     2.720 76(5.596 56) + 4.103 80(16.059 80) + (−1.182 68)(−10.217 60)
                                 = 0.24276

                                                    ⎡         ⎤           ⎡            ⎤ ⎡           ⎤
                                                    2.417 04                 2. 720 76      3.077 53
                                                 ⎢            ⎥           ⎢            ⎥ ⎢           ⎥
                              x2 = x1 + α 1 s1 = ⎣ −0. 201 42 ⎦ + 0.24276 ⎣ 4. 103 80 ⎦ = ⎣ 0.794 82 ⎦
                                                    1.007 10                −1. 182 68      0.719 99

                     Third iteration
                                                 ⎡⎤ ⎡          ⎤⎡           ⎤ ⎡            ⎤
                                               12       4 −1  1    3.077 53      −0.235 29
                                             ⎢    ⎥ ⎢          ⎥⎢           ⎥ ⎢            ⎥
                              r2 = b − Ax2 = ⎣ −1 ⎦ − ⎣−1  4 −2⎦ ⎣ 0.794 82 ⎦ = ⎣ 0.338 23 ⎦
                                                5       1 −2  4    0.719 99       0.632 15


                                      r2T As1
                             β1 = −
                                      s1T As1
                                      (−0.235 29)(5.596 56) + 0.338 23(16.059 80) + 0.632 15(−10.217 60)
                                =−
                                      2.720 76(5.596 56) + 4.103 80(16.059 80) + (−1.182 68)(−10.217 60)
                                = 0.0251 452
P1: PHB

CUUS884-Kiusalaas       CUUS884-02          978 0 521 19132 6                                        December 16, 2009          15:4




           90       Systems of Linear Algebraic Equations
                                                 ⎡        ⎤             ⎡           ⎤ ⎡            ⎤
                                                −0.235 29                  2.720 76     −0.166 876
                                              ⎢           ⎥             ⎢           ⎥ ⎢            ⎥
                           s2 = r2 + β 1 s1 = ⎣ 0.338 23 ⎦ + 0.025 1452 ⎣ 4.103 80 ⎦ = ⎣ 0.441 421 ⎦
                                                 0.632 15                 −1.182 68      0.602 411

                                                  ⎡            ⎤⎡            ⎤
                                                    4     −1  1   −0.166 876    −0.506 514
                                                  ⎢            ⎥⎢            ⎥
                                            As2 = ⎣−1      4 −2⎦ ⎣ 0.441 421 ⎦ = 0.727 738
                                                    1     −2  4    0.602 411     1.359 930

                                   r2T s2
                          α2 =
                                  s2T As2
                                    (−0.235 29)(−0.166 876) + 0.338 23(0.441 421) + 0.632 15(0.602 411)
                              =
                                  (−0.166 876)(−0.506 514) + 0.441 421(0.727 738) + 0.602 411(1.359 930)
                              = 0.464 80
                                                      ⎡     ⎤            ⎡            ⎤ ⎡           ⎤
                                                   3.077 53                −0.166 876      2.999 97
                                                 ⎢          ⎥            ⎢            ⎥ ⎢           ⎥
                              x3 = x2 + α 2 s2 = ⎣ 0.794 82 ⎦ + 0.464 80 ⎣ 0.441 421 ⎦ = ⎣ 0.999 99 ⎦
                                                   0.719 99                 0.602 411      0.999 99

                    The solution x3 is correct to almost five decimal places. The small discrepancy is
                    caused by roundoff errors in the computations.

                    EXAMPLE 2.17
                    Write a computer program to solve the following n simultaneous equations by the
                    Gauss–Seidel method with relaxation (the program should work with any value of n3 ):
                              ⎡                                            ⎤⎡     ⎤ ⎡ ⎤
                                 2 −1     0     0 ...     0    0   0    1      x1       0
                              ⎢−1        −1         . . .                  ⎥⎢ x ⎥ ⎢0⎥
                              ⎢      2          0         0    0   0    0  ⎥⎢ 2 ⎥ ⎢ ⎥
                              ⎢                                            ⎥⎢     ⎥ ⎢ ⎥
                              ⎢ 0 −1      2 −1 . . .      0    0   0    0⎥ ⎢ x3 ⎥ ⎢ 0 ⎥
                              ⎢                                            ⎥⎢     ⎥ ⎢ ⎥
                              ⎢ ..    ..   ..    ..        ..   ..  ..   ..⎥ ⎢ .. ⎥ ⎢ .. ⎥
                              ⎢ .      .    .     .         .    .   .     ⎥ ⎢
                                                                          .⎥ ⎢ . ⎥  =⎢ ⎥
                              ⎢                                                   ⎥ ⎢.⎥
                              ⎢                                            ⎥⎢     ⎥ ⎢ ⎥
                              ⎢ 0    0    0     0 . . . −1     2 −1     0⎥ ⎢ xn−2 ⎥ ⎢ 0 ⎥
                              ⎢                                            ⎥⎢     ⎥ ⎢ ⎥
                              ⎣ 0    0    0     0 ...     0 −1     2 −1⎦ ⎣ xn−1 ⎦ ⎣ 0 ⎦
                                 1   0    0     0 ...     0    0 −1     2      xn       1

                    Run the program with n = 20. The exact solution can be shown to be xi = −n/4 + i/2,
                    i = 1, 2, ..., n.

                    Solution In this case the iterative formulas in Eq. (2.35) are

                                            x1 = ω(x2 − xn )/2 + (1 − ω)x1

                                            xi = ω(xi−1 + xi+1 )/2 + (1 − ω)xi , i = 2, 3, . . . , n − 1                 (a)

                                            xn = ω(1 − x1 + xn−1 )/2 + (1 − ω)xn

                    These formulas are evaluated in the function iterEqs.

                    3   Equations of this form are called cyclic tridiagonal. They occur in the finite difference formulation
                        of second-order differential equations with periodic boundary conditions.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02      978 0 521 19132 6                            December 16, 2009   15:4




                      ∗
               91         2.7 Iterative Methods

                     #!/usr/bin/python
                     ## example2_17
                     from numpy import zeros
                     from gaussSeidel import *


                     def iterEqs(x,omega):
                             n = len(x)
                             x[0] =omega*(x[1] - x[n-1])/2.0 + (1.0 - omega)*x[0]
                             for i in range(1,n-1):
                                  x[i] = omega*(x[i-1] + x[i+1])/2.0 + (1.0 - omega)*x[i]
                             x[n-1] = omega*(1.0 - x[0] + x[n-2])/2.0 \
                                      + (1.0 - omega)*x[n-1]
                             return x


                     n = eval(raw_input("Number of equations ==> "))
                     x = zeros(n)
                     x,numIter,omega = gaussSeidel(iterEqs,x)
                     print "\nNumber of iterations =",numIter
                     print "\nRelaxation factor =",omega
                     print "\nThe solution is:\n",x
                     raw_input("\nPress return to exit")



                            The output from the program is:

                     Number of equations ==> 20


                     Number of iterations = 259


                     Relaxation factor = 1.70545231071


                     The solution is:
                     [-4.50000000e+00 -4.00000000e+00 -3.50000000e+00 -3.00000000e+00
                      -2.50000000e+00 -2.00000000e+00 -1.50000000e+00 -9.99999997e-01
                      -4.99999998e-01          2.14046747e-09   5.00000002e-01     1.00000000e+00
                          1.50000000e+00       2.00000000e+00   2.50000000e+00     3.00000000e+00
                          3.50000000e+00       4.00000000e+00   4.50000000e+00     5.00000000e+00]

                         The convergence is very slow, because the coefficient matrix lacks diagonal
                     dominance – substituting the elements of A into Eq. (2.30) produces an equality
                     rather than the desired inequality. If we were to change each diagonal term of the
                     coefficient from 2 to 4, A would be diagonally dominant and the solution would con-
                     verge in only 17 iterations.

                     EXAMPLE 2.18
                     Solve Example 2.17 with the conjugate gradient method, also using n = 20.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02      978 0 521 19132 6                                     December 16, 2009   15:4




           92       Systems of Linear Algebraic Equations

                    Solution The program shown here utilizes the function conjGrad. The solution vec-
                    tor x is initialized to zero in the program, which also sets up the constant vector b.
                    The function Av(v) returns the product Av, where A is the coefficient matrix and v is
                    a vector. For the given A, the components of the vector Av are

                                        (Av)1 = 2v1 − v2 + vn

                                        (Av)i = −vi−1 + 2vi − vi+1 , i = 2, 3, . . . , n − 1

                                        (Av)n = −vn−1 + 2vn + v1

                    which are evaluated by the function Av(v).

                    #!/usr/bin/python
                    ## example2_18
                    from numpy import zeros,sqrt
                    from conjGrad import *


                    def Ax(v):
                         n = len(v)
                         Ax = zeros(n)
                         Ax[0] = 2.0*v[0] - v[1]+v[n-1]
                         Ax[1:n-1] = -v[0:n-2] + 2.0*v[1:n-1] -v [2:n]
                         Ax[n-1] = -v[n-2] + 2.0*v[n-1] + v[0]
                         return Ax


                    n = eval(raw_input("Number of equations ==> "))
                    b = zeros(n)
                    b[n-1] = 1.0
                    x = zeros(n)
                    x,numIter = conjGrad(Ax,x,b)
                    print "\nThe solution is:\n",x
                    print "\nNumber of iterations =",numIter
                    raw_input("\nPress return to exit")


                        Running the program results in

                    Number of equations ==> 20


                    The solution is:
                    [-4.5 -4. -3.5 -3. -2.5 -2. -1.5 -1. -0.5                  0.    0.5       1.   1.5
                             2.   2.5    3.    3.5       4.   4.5   5. ]


                    Number of iterations = 9


                       Note that convergence was reached in only 9 iterations, whereas 259 iterations
                    were required in the Gauss–Seidel method.
P1: PHB

CUUS884-Kiusalaas   CUUS884-02      978 0 521 19132 6                                     December 16, 2009   15:4




                      ∗
               93         2.7 Iterative Methods

                     PROBLEM SET 2.3
                      1. Let
                                                  ⎡          ⎤              ⎡                    ⎤
                                                 3      −1  2                 0 1              3
                                               ⎢             ⎥             ⎢                     ⎥
                                             A=⎣ 0       1  3⎦         B = ⎣ 3 −1              2⎦
                                                −2       2 −4                −2 2             −4
                            (note that B is obtained by interchanging the first two rows of A). Knowing that
                                                              ⎡                    ⎤
                                                                 0.5    0     0.25
                                                              ⎢                    ⎥
                                                       A−1 = ⎣ 0.3 0.4        0.45⎦
                                                               −0.1 0.2 −0.15
                         determine B−1 .
                      2. Invert the triangular matrices
                                                    ⎡            ⎤          ⎡             ⎤
                                                      2 4      3             2     0     0
                                                    ⎢            ⎥         ⎢              ⎥
                                                A = ⎣0 6       5⎦      B = ⎣3      4     0⎦
                                                      0 0      2             4     5     6
                      3. Invert the triangular matrix
                                                          ⎡                  ⎤
                                                           1      1/2 1/4 1/8
                                                          ⎢0        1 1/3 1/9⎥
                                                          ⎢                  ⎥
                                                        A=⎢                  ⎥
                                                          ⎣0        0   1 1/4⎦
                                                           0        0   0   1
                      4. Invert the following matrices:
                                                 ⎡       ⎤                    ⎡        ⎤
                                                  1 2   4                      4 −1  0
                                                 ⎢       ⎥                   ⎢         ⎥
                                         (a) A = ⎣1 3   9⎦           (b) B = ⎣−1  4 −1⎦
                                                  1 4 16                       0 −1  4
                      5. Invert the matrix
                                                              ⎡                ⎤
                                                                4 −2         1
                                                              ⎢                ⎥
                                                          A = ⎣−2  1        −1⎦
                                                                1 −2         4
                      6.      Invert the following matrices with any method:
                                             ⎡                   ⎤        ⎡                        ⎤
                                                5 −3 −1        0             4 −1              0  0
                                             ⎢−2               1⎥         ⎢−1                 −1  0⎥
                                             ⎢      1     1      ⎥        ⎢     4                  ⎥
                                        A=⎢                      ⎥    B=⎢                          ⎥
                                             ⎣ 3 −5       1    2⎦         ⎣ 0 −1               4 −1⎦
                                                0   8 −4 −3                  0  0             −1  4
                      7.      Invert the matrix by any method:
                                                          ⎡                          ⎤
                                                            1  3 −9  6             4
                                                          ⎢ 2 −1  6  7             1⎥
                                                          ⎢                          ⎥
                                                          ⎢                          ⎥
                                                      A=⎢ 3    2 −3 15             5⎥
                                                          ⎢                          ⎥
                                                          ⎣ 8 −1  1  4             2⎦
                                                           11  1 −2 18             7
                            and comment on the reliability of the result.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02       978 0 521 19132 6                                  December 16, 2009         15:4




           94       Systems of Linear Algebraic Equations

                     8.     The joint displacements u of the plane truss in Problem 14, Problem Set 2.2,
                          are related to the applied joint forces p by

                                                                Ku = p                                       (a)

                          where
                                       ⎡                                                ⎤
                                        27.580        7.004 −7.004        0.000   0.000
                                      ⎢ 7.004        29.570 −5.253        0.000 −24.320⎥
                                      ⎢                                                 ⎥
                                      ⎢                                                 ⎥
                                  K = ⎢−7.004        −5.253 29.570        0.000   0.000⎥ MN/m
                                      ⎢                                                 ⎥
                                      ⎣ 0.000         0.000  0.000       27.580  −7.004⎦
                                         0.000      −24.320  0.000       −7.004  29.570

                        is called the stiffness matrix of the truss. If Eq. (a) is inverted by multiplying each
                        side by K−1 , we obtain u = K−1 p, where K−1 is known as the flexibility matrix.
                        The physical meaning of the elements of the flexibility matrix is Kij−1 = displace-
                        ments ui (i = 1, 2, . . . 5) produced by the unit load p j = 1. Compute (a) the flex-
                        ibility matrix of the truss; (b) the displacements of the joints due to the load
                        p5 = −45 kN (the load shown in Problem 14, Problem Set 2.2).
                     9.    Invert the matrices
                                             ⎡                       ⎤             ⎡            ⎤
                                                  3 −7     45     21                 1 1 1 1
                                             ⎢12 11               17⎥              ⎢1 2 2 2⎥
                                             ⎢             10        ⎥             ⎢            ⎥
                                        A=⎢                          ⎥       B=⎢                ⎥
                                             ⎣ 6 25 −80 −24⎦                       ⎣2 3 4 4⎦
                                                17 55      −9      7                 4 5 6 7

                    10.     Write a program for inverting on n × n lower triangular matrix. The inversion
                          procedure should contain only forward substitution. Test the program by invert-
                          ing the matrix
                                                            ⎡                  ⎤
                                                             36  0      0    0
                                                            ⎢18 36           0⎥
                                                            ⎢           0      ⎥
                                                          A=⎢                  ⎥
                                                            ⎣ 9 12     36    0⎦
                                                              5  4      9   36

                    11. Use the Gauss–Seidel method to solve
                                              ⎡            ⎤⎡ ⎤ ⎡          ⎤
                                                −2 5      9    x1        1
                                              ⎢            ⎥⎢ ⎥ ⎢          ⎥
                                              ⎣ 7 1       1⎦ ⎣ x2 ⎦ = ⎣ 6 ⎦
                                                −3 7 −1        x3      −26

                    12. Solve the following equations with the Gauss–Seidel method:
                                                ⎡                 ⎤⎡ ⎤ ⎡ ⎤
                                                  12      −2  3  1    x1      0
                                                ⎢−2               ⎥ ⎢    ⎥ ⎢
                                                              6 −3⎥ ⎢ x2 ⎥ ⎢ 0 ⎥
                                                ⎢         15                    ⎥
                                                ⎢                 ⎥⎢ ⎥ = ⎢ ⎥
                                                ⎣ 1        6 20 −4⎦ ⎣ x3 ⎦ ⎣ 20 ⎦
                                                   0      −3  2  9    x4      0
P1: PHB

CUUS884-Kiusalaas   CUUS884-02      978 0 521 19132 6                                              December 16, 2009   15:4




                      ∗
               95         2.7 Iterative Methods

                     13. Use the Gauss–Seidel method with relaxation to solve Ax = b, where
                                                        ⎡        ⎤                         ⎡ ⎤
                                                      4 −1  0  0                          15
                                                    ⎢−1  4 −1  0⎥                       ⎢ 10 ⎥
                                                    ⎢            ⎥                      ⎢ ⎥
                                                  A=⎢            ⎥                    b=⎢ ⎥
                                                    ⎣ 0 −1  4 −1⎦                       ⎣ 10 ⎦
                                                      0  0 −1  3                          10

                         Take xi = bi /A ii as the starting vector and use ω = 1.1 for the relaxation factor.
                     14. Solve the equations
                                                         ⎡        ⎤⎡ ⎤ ⎡ ⎤
                                                           2 −1  0    x1       1
                                                         ⎢        ⎥⎢ ⎥ ⎢ ⎥
                                                         ⎣−1  2 −1⎦ ⎣ x2 ⎦ = ⎣ 1 ⎦
                                                           0 −1  1    x3       1

                         by the conjugate gradient method. Start with x = 0.
                     15. Use the conjugate gradient method to solve
                                                        ⎡                 ⎤⎡ ⎤ ⎡         ⎤
                                                           3      0    −1    x1        4
                                                        ⎢                 ⎥⎢ ⎥ ⎢         ⎥
                                                        ⎣ 0       4    −2⎦ ⎣ x2 ⎦ = ⎣ 10 ⎦
                                                          −1     −2     5    x3      −10

                         starting with x = 0.
                     16.    Solve the simultaneous equations Ax = b and Bx = b by the Gauss–Seidel
                         method with relaxation, where

                                                                                               T
                                                    b = 10            −8   10   10   −8   10


                                                             ⎡                      ⎤
                                                       3              −2  1  0  0  0
                                                     ⎢                              ⎥
                                                     ⎢−2               4 −2  1  0  0⎥
                                                     ⎢                              ⎥
                                                     ⎢ 1              −2  4 −2  1  0⎥
                                                   A=⎢
                                                     ⎢ 0
                                                                                    ⎥
                                                     ⎢                 1 −2  4 −2  1⎥
                                                                                    ⎥
                                                     ⎢                              ⎥
                                                     ⎣ 0               0  1 −2  4 −2⎦
                                                       0               0  0  1 −2  3


                                                             ⎡         ⎤
                                                       3 −2  1  0  0  1
                                                     ⎢                 ⎥
                                                     ⎢−2  4 −2  1  0  0⎥
                                                     ⎢                 ⎥
                                                     ⎢ 1 −2  4 −2  1  0⎥
                                                   B=⎢
                                                     ⎢ 0
                                                                       ⎥
                                                     ⎢    1 −2  4 −2  1⎥
                                                                       ⎥
                                                     ⎢                 ⎥
                                                     ⎣ 0  0  1 −2  4 −2⎦
                                                       1  0  0  1 −2  3

                            Note that A is not diagonally dominant, but that does not necessarily preclude
                            convergence.
P1: PHB

CUUS884-Kiusalaas    CUUS884-02       978 0 521 19132 6                                            December 16, 2009   15:4




           96       Systems of Linear Algebraic Equations

                    17.     Modify the program in Example 2.17 (Gauss–Seidel method) so that it will solve
                          the following equations:


                                ⎡                                                             ⎤⎡     ⎤ ⎡     ⎤
                                  4 −1    0   0               ···     0      0      0      1      x1      0
                                ⎢−1
                                ⎢    4   −1   0               ···     0      0      0      0⎥   ⎢    ⎥ ⎢
                                                                                              ⎥ ⎢ x2 ⎥ ⎢ 0 ⎥
                                                                                                             ⎥
                                ⎢                                                             ⎥⎢     ⎥ ⎢     ⎥
                                ⎢ 0 −1    4 −1                ···     0      0      0      0⎥ ⎢ x 3 ⎥ ⎢ 0 ⎥
                                ⎢                                                             ⎥⎢     ⎥ ⎢     ⎥
                                ⎢ ..  ..   ..  ..                      ..     ..     ..     ..⎥ ⎢ .. ⎥ ⎢ .. ⎥
                                ⎢ .    .    .   .             ···       .      .      .      .⎥ ⎢ . ⎥=⎢ . ⎥
                                ⎢                                                             ⎥⎢     ⎥ ⎢     ⎥
                                ⎢ 0                                                           ⎥⎢
                                ⎢    0    0   0               ···    −1      4     −1      0⎥ ⎢ xn−2 ⎥ ⎢
                                                                                                     ⎥ ⎢ 0 ⎥
                                                                                                             ⎥
                                ⎢                                                             ⎥⎢     ⎥ ⎢     ⎥
                                ⎣ 0  0    0   0               ···     0     −1      4     −1⎦ ⎣ xn−1 ⎦ ⎣ 0 ⎦
                                  1  0    0   0               ···     0      0     −1      4      xn     100


                        Run the program with n = 20 and compare the number of iterations with Exam-
                        ple 2.17.
                    18.   Modify the program in Example 2.18 to solve the equations in Problem 17 by
                        the conjugate gradient method. Run the program with n = 20.
                    19.



                                                                    T = 00

                                                          1          2        3

                                                          4          5        6           T = 1000
                                        T = 00
                                                          7          8        9


                                                                T = 2000


                          The edges of the square plate are kept at the temperatures shown. Assuming
                          steady-state heat conduction, the differential equation governing the tempera-
                          ture T in the interior is


                                                                    ∂2T    ∂2T
                                                                         +      =0
                                                                    ∂x 2   ∂y 2


                          If this equation is approximated by finite differences using the mesh shown, we
                          obtain the following algebraic equations for temperatures at the mesh points:
P1: PHB

CUUS884-Kiusalaas       CUUS884-02      978 0 521 19132 6                                    December 16, 2009   15:4




                          ∗
               97             2.8 Other Methods


                                       ⎡                                  ⎤⎡ ⎤ ⎡          ⎤
                                        −4      1     0  1  0  0  0  0  0     T1        0
                                       ⎢                                  ⎥⎢ ⎥ ⎢          ⎥
                                       ⎢ 1     −4     1  0  1  0  0  0  0⎥ ⎢ T2 ⎥ ⎢ 0 ⎥
                                       ⎢                                  ⎥⎢ ⎥ ⎢          ⎥
                                       ⎢ 0      1    −4  0  0  1  0  0  0⎥  ⎢ ⎥ ⎢         ⎥
                                       ⎢                                  ⎥ ⎢ T3 ⎥ ⎢ 100 ⎥
                                       ⎢ 1      0     0 −4  1  0  1  0    ⎥ ⎢    ⎥  ⎢
                                                                        0⎥ ⎢ T4 ⎥ ⎢ 0 ⎥
                                       ⎢                                                  ⎥
                                       ⎢                                  ⎥⎢ ⎥ ⎢          ⎥
                                       ⎢ 0      1     0  1 −4  1  0  1  0⎥ ⎢ T5 ⎥ = ⎢ 0 ⎥
                                       ⎢                                  ⎥⎢ ⎥ ⎢          ⎥
                                       ⎢ 0      0     1  0  1 −4  0  0  1⎥  ⎢ ⎥ ⎢         ⎥
                                       ⎢                                  ⎥ ⎢ T6 ⎥ ⎢ 100 ⎥
                                       ⎢ 0                     0 −4         ⎢    ⎥  ⎢
                                                                        0⎥ ⎢ T7 ⎥ ⎢ 200 ⎥
                                                                          ⎥
                                       ⎢        0     0  1  0        1                    ⎥
                                       ⎢                                  ⎥⎢ ⎥ ⎢          ⎥
                                       ⎣ 0      0     0  0  1  0  1 −4  1⎦ ⎣ T8 ⎦ ⎣ 200 ⎦
                                         0      0     0  0  0  1  0  1 −4     T9      300


                                Solve these equations with the conjugate gradient method.
                         20.

                                2 kN/m 3 kN/m                   3 kN/m       3 kN/m    3 kN/m            2 kN/m
                                     80 N                                          60 N

                                         1                  2            3              4            5

                                The equilibrium equations of the blocks in the spring–block system are

                                                                   3(x2 − x1 ) − 2x1 = −80

                                                            3(x3 − x2 ) − 3(x2 − x1 ) = 0

                                                            3(x4 − x3 ) − 3(x3 − x2 ) = 0

                                                            3(x5 − x4 ) − 3(x4 − x3 ) = 60

                                                                 −2x5 − 3(x5 − x4 ) = 0

                             where xi are the horizontal displacements of the blocks measured in mm. (a)
                             Write a program that solves these equations by the Gauss–Seidel method with-
                             out relaxation. Start with x = 0 and iterate until four-figure accuracy after the
                             decimal point is achieved. Also print the number of iterations required. (b) Solve
                             the equations using the function gaussSeidel using the same convergence cri-
                             terion as in Part (a). Compare the number of iterations in Parts (a) and (b).
                         21.    Solve the equations in Prob. 20 with the conjugate gradient method utilizing
                             the function conjGrad. Start with x = 0 and iterate until four-figure accuracy
                             after the decimal point is achieved.


              ∗
                  2.8    Other Methods

                         A matrix can be decomposed in numerous ways, some of which are generally useful,
                         whereas others find use in special applications. The most important of the latter are
                         the QR factorization and the singular value decomposition.
P1: PHB

CUUS884-Kiusalaas     CUUS884-02      978 0 521 19132 6                                    December 16, 2009        15:4




           98       Systems of Linear Algebraic Equations

                        The QR decomposition of a matrix A is

                                                              A = QR

                    where Q is an orthogonal matrix (recall that the matrix Q is orthogonal if Q−1 = QT )
                    and R is an upper triangular matrix. Unlike LU factorization, QR decomposition does
                    not require pivoting to sustain stability, but it does involve about twice as many op-
                    erations. Because of its relative inefficiency, the QR factorization is not used as a
                    general-purpose tool, but finds its niche in applications that put a premium on sta-
                    bility (e.g., solution of eigenvalue problems).
                         The singular value decomposition is useful in dealing with singular or ill-
                    conditioned matrices. Here the factorization is

                                                            A = U VT

                    where U and V are orthogonal matrices and
                                                     ⎡                            ⎤
                                                        λ1 0           0    ···
                                                     ⎢0 λ
                                                     ⎢        2        0    · · ·⎥⎥
                                                  =⎢                              ⎥
                                                     ⎢0 0              λ3   · · ·⎥
                                                     ⎣                            ⎦
                                                       ..  ..         ..    ..
                                                        .   .          .        .

                    is a diagonal matrix. The elements λi of can be shown to be positive or zero. If A
                    is symmetric and positive definite, then the λs are the eigenvalues of A. A nice char-
                    acteristic of the singular value decomposition is that it works even if A is singular or
                    ill conditioned. The conditioning of A can be diagnosed from magnitudes of λs: the
                    matrix is singular if one or more of the λs are zero, and it is ill conditioned if λmax /λmin
                    is very large.
P1: PHB

CUUS884-Kiusalaas   CUUS884-03     978 0 521 19132 6                                           December 16, 2009     15:4




              3      Interpolation and Curve Fitting




                                 Given the n + 1 data points (xi , yi ), i = 0, 1, . . . , n, estimate y(x).




              3.1    Introduction

                     Discrete data sets, or tables of the form

                                                         x0     x1   x2        ···        xn
                                                         y0     y1   y2        ···        yn

                     are commonly involved in technical calculations. The source of the data may be ex-
                     perimental observations or numerical computations. There is a distinction between
                     interpolation and curve fitting. In interpolation we construct a curve through the
                     data points. In doing so, we make the implicit assumption that the data points are
                     accurate and distinct. Curve fitting is applied to data that contains scatter (noise),
                     usually due to measurement errors. Here we want to find a smooth curve that ap-
                     proximates the data in some sense. Thus the curve does not necessarily hit the
                     data points. The difference between interpolation and curve fitting is illustrated in
                     Fig. 3.1.




              3.2    Polynomial Interpolation
                     Lagrange’s Method
                     The simplest form of an interpolant is a polynomial. It is always possible to construct
                     a unique polynomial of degree n that passes through n + 1 distinct data points. One
                     means of obtaining this polynomial is the formula of Lagrange,

                                                                         n
                                                              Pn (x) =         yi i (x)                            (3.1a)
                                                                         i=0


               99
P1: PHB

CUUS884-Kiusalaas       CUUS884-03     978 0 521 19132 6                                                             December 16, 2009         15:4




           100      Interpolation and Curve Fitting


                    y
                                Curve fitting
                          Interpolation                                                          Figure 3.1. Interpolation and curve fitting
                                                                                                 of data.


                                                          Data points

                                                                                         x

                    where the subscript n denotes the degree of the polynomial and

                                                 x − x0 x − x1       x − xi−1 x − xi+1       x − xn
                                     i (x)   =          ·        ···          ·          ···
                                                 xi − x0 xi − x1     xi − xi−1 xi − xi+1     xi − xn
                                                  n
                                                        x − xi
                                             =                   , i = 0, 1, . . . , n                                              (3.1b)
                                                        xi − x j
                                                 j =0
                                                 j =i


                    are called the cardinal functions.
                         For example, if n = 1, the interpolant is the straight line P1 (x) = y0 0 (x) +
                    y1 1 (x), where

                                                                     x − x1                             x − x0
                                                         0 (x)   =                         1 (x)    =
                                                                     x0 − x1                            x1 − x0

                    With n = 2, interpolation is parabolic: P2 (x) = y0 0 (x) + y1 1 (x) + y2 2 (x), where now

                                                                                 (x − x1 )(x − x2 )
                                                                 0 (x)      =
                                                                                (x0 − x1 )(x0 − x2 )
                                                                                 (x − x0 )(x − x2 )
                                                                 1 (x)      =
                                                                                (x1 − x0 )(x1 − x2 )
                                                                                 (x − x0 )(x − x1 )
                                                                 2 (x)      =
                                                                                (x2 − x0 )(x2 − x1 )

                          The cardinal functions are polynomials of degree n and have the property

                                                                                   0 if i = j
                                                                 i (x j )   =                         = δij                           (3.2)
                                                                                   1 if i = j

                    where δij is the Kronecker delta. This property is illustrated in Fig. 3.2 for three-point
                    interpolation (n = 2) with x0 = 0, x1 = 2, and x2 = 3.
                        To prove that the interpolating polynomial passes through the data points, we
                    substitute x = x j into Eq. (3.1a) and then utilize Eq. (3.2). The result is
                                                                            n                   n
                                                        Pn (x j ) =             yi i (x j ) =         yi δij = y j
                                                                       i=0                      i=0
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                              December 16, 2009    15:4




               101     3.2 Polynomial Interpolation


                      1
                                                                   l1
                                                                                      Figure 3.2. Example of quadratic cardi-

                                             l0               l2                      nal functions.


                      0                                                         x
                       0               1                 2                  3

                           It can be shown that the error in polynomial interpolation is

                                                                (x − x0 )(x − x1 ) · · · (x − xn ) (n+1)
                                           f (x) − Pn (x) =                                       f      (ξ )           (3.3)
                                                                           (n + 1)!

                      where ξ lies somewhere in the interval (x0 , xn ); its value is otherwise unknown. It is
                      instructive to note that the further a data point is from x, the more it contributes to
                      the error at x.


                      Newton’s Method
                      Although Lagrange’s method is conceptually simple, it does not lend itself to an
                      efficient algorithm. A better computational procedure is obtained with Newton’s
                      method, where the interpolating polynomial is written in the form

                       Pn (x) = a 0 + (x − x0 )a 1 + (x − x0 )(x − x1 )a 2 + · · · + (x − x0 )(x − x1 ) · · · (x − xn−1 )a n

                         This polynomial lends itself to an efficient evaluation procedure. Consider, for
                      example, four data points (n = 3). Here the interpolating polynomial is

                             P3 (x) = a 0 + (x − x0 )a 1 + (x − x0 )(x − x1 )a 2 + (x − x0 )(x − x1 )(x − x2 )a 3

                                    = a 0 + (x − x0 ) {a 1 + (x − x1 ) [a 2 + (x − x2 )a 3 ]}

                      which can be evaluated backward with the following recurrence relations:

                                                             P0 (x) = a 3

                                                             P1 (x) = a 2 + (x − x2 )P0 (x)

                                                             P2 (x) = a 1 + (x − x1 )P1 (x)

                                                             P3 (x) = a 0 + (x − x0 )P2 (x)

                      For arbitrary n, we have

                                  P0 (x) = a n      Pk (x) = a n−k + (x − xn−k )Pk−1 (x), k = 1, 2, . . . , n           (3.4)

                      Denoting the x-coordinate array of the data points by xData and the degree of the
P1: PHB

CUUS884-Kiusalaas     CUUS884-03       978 0 521 19132 6                                                December 16, 2009         15:4




           102      Interpolation and Curve Fitting

                    polynomial by n, we have the following algorithm for computing Pn (x):

                    p = a[n]
                    for k in range(1,n+1):
                         p = a[n-k] + (x - xData[n-k])*p

                        The coefficients of Pn are determined by forcing the polynomial to pass through
                    each data point: yi = Pn (xi ), i = 0, 1, . . . , n. This yields the simultaneous equations

                                y0 = a 0

                                y1 = a 0 + (x1 − x0 )a 1

                                y2 = a 0 + (x2 − x0 )a 1 + (x2 − x0 )(x2 − x1 )a 2                                          (a)
                                      ..
                                       .

                                yn = a 0 + (xn − x0 )a 1 + · · · + (xn − x0 )(xn − x1 ) · · · (xn − xn−1 )a n

                    Introducing the divided differences
                                                              yi − y0
                                                   ∇ yi =             , i = 1, 2, . . . , n
                                                              xi − x0
                                                             ∇ yi − ∇ y1
                                                  ∇ 2 yi =               , i = 2, 3, . . . , n
                                                               xi − x1
                                                             ∇ 2 yi − ∇ 2 y2
                                                  ∇ 3 yi =                   , i = 3, 4, . . . n                       (3.5)
                                                                 xi − x2
                                                             ..
                                                              .
                                                             ∇ n−1 yn − ∇ n−1 yn−1
                                                ∇ n yn =
                                                                   xn − xn−1
                    the solution of Eqs. (a) is

                                     a 0 = y0        a 1 = ∇ y1          a 2 = ∇ 2 y2      ···   a n = ∇ n yn          (3.6)

                    If the coefficients are computed by hand, it is convenient to work with the format in
                    Table 3.1 (shown for n = 4).
                         The diagonal terms (y0 , ∇ y1 , ∇ 2 y2 , ∇ 3 y3 , and ∇ 4 y4 ) in the table are the coeffi-
                    cients of the polynomial. If the data points are listed in a different order, the entries
                    in the table will change, but the resultant polynomial will be the same – recall that a
                    polynomial of degree n interpolating n + 1 distinct data points is unique.


                                                x0     y0
                                                x1     y1         ∇ y1
                                                x2     y2         ∇ y2   ∇ 2 y2
                                                x3     y3         ∇ y3   ∇ 2 y3   ∇ 3 y3
                                                x4     y4         ∇ y4   ∇ 2 y4   ∇ 3 y4     ∇ 4 y4

                                              Table 3.1
P1: PHB

CUUS884-Kiusalaas    CUUS884-03    978 0 521 19132 6                                 December 16, 2009    15:4




               103     3.2 Polynomial Interpolation

                          Machine computations can be carried out within a one-dimensional array a em-
                      ploying the following algorithm (we use the notation m = n + 1 = number of data
                      points):


                      a = yData.copy()
                      for k in range(1,m):
                            for i in range(k,m):
                                  a[i] = (a[i] - a[k-1])/(xData[i] - xData[k-1])


                           Initially, a contains the y-coordinates of the data, so that it is identical to the
                      second column in Table 3.1. Each pass through the outer loop generates the entries
                      in the next column, which overwrite the corresponding elements of a. Therefore, a
                      ends up containing the diagonal terms of Table 3.1, that is, the coefficients of the
                      polynomial.



                         newtonPoly

                      This module contains the two functions required for interpolation by Newton’s
                      method. Given the data point arrays xData and yData, the function coeffts re-
                      turns the coefficient array a. After the coefficients are found, the interpolant Pn (x)
                      can be evaluated at any value of x with the function evalPoly.


                      ## module newtonPoly
                      ’’’ p = evalPoly(a,xData,x).
                            Evaluates Newton’s polynomial p at x. The coefficient
                            vector {a} can be computed by the function ’coeffts’.


                            a = coeffts(xData,yData).
                            Computes the coefficients of Newton’s polynomial.
                      ’’’
                      def evalPoly(a,xData,x):
                            n = len(xData) - 1         # Degree of polynomial
                            p = a[n]
                            for k in range(1,n+1):
                                  p = a[n-k] + (x -xData[n-k])*p
                            return p


                      def coeffts(xData,yData):
                            m = len(xData)       # Number of data points
                            a = yData.copy()
                            for k in range(1,m):
                                  a[k:m] = (a[k:m] - a[k-1])/(xData[k:m] - xData[k-1])
                            return a
P1: PHB

CUUS884-Kiusalaas     CUUS884-03           978 0 521 19132 6                                               December 16, 2009         15:4




           104      Interpolation and Curve Fitting

                    Neville’s Method
                    Newton’s method of interpolation involves two steps: computation of the coeffi-
                    cients, followed by evaluation of the polynomial. This works well if the interpolation
                    is carried out repeatedly at different values of x using the same polynomial. If only
                    one point is to be interpolated, a method that computes the interpolant in a single
                    step, such as Neville’s algorithm, is a better choice.
                         Let Pk [xi , xi+1 , . . . , xi+k ] denote the polynomial of degree k that passes through
                    the k + 1 data points (xi , yi ), (xi+1 , yi+1 ), . . . , (xi+k , yi+k ). For a single data point, we
                    have

                                                                        P0 [xi ] = yi                                        (3.7)

                    The interpolant based on two data points is

                                                                   (x − xi+1 )P0 [xi ] + (xi − x)P0 [xi+1 ]
                                              P1 [xi , xi+1 ] =
                                                                                  xi − xi+1

                    It is easily verified that P1 [xi , xi+1 ] passes through the two data points; that is,
                    P1 [xi , xi+1 ] = yi when x = xi , and P1 [xi , xi+1 ] = yi+1 when x = xi+1 .
                         The three-point interpolant is

                                                               (x − xi+2 )P1 [xi , xi+1 ] + (xi − x)P1 [xi+1 , xi+2 ]
                                   P2 [xi , xi+1 , xi+2 ] =
                                                                                     xi − xi+2

                    To show that this interpolant does intersect the data points, we first substitute x = xi ,
                    obtaining

                                                        P2 [xi , xi+1 , xi+2 ] = P1 [xi , xi+1 ] = yi

                    Similarly, x = xi+2 yields

                                                     P2 [xi , xi+1 , xi+2 ] = P2 [xi+1 , xi+2 ] = yi+2

                    Finally, when x = xi+1 we have

                                                         P1 [xi , xi+1 ] = P1 [xi+1 , xi+2 ] = yi+1

                    so that
                                                                   (xi+1 − xi+2 )yi+1 + (xi − xi+1 )yi+1
                                      P2 [xi , xi+1 , xi+2 ] =                                           = yi+1
                                                                                 xi − xi+2

                       Having established the pattern, we can now deduce the general recursive for-
                    mula:

                                  Pk [xi , xi+1 , . . . , xi+k ]                                                             (3.8)
                                  (x − xi+k )Pk−1 [xi, xi+1 , . . . , xi+k−1 ] + (xi − x)Pk−1 [xi+1, xi+2 , . . . , xi+k ]
                              =
                                                                         xi − xi+k
P1: PHB

CUUS884-Kiusalaas    CUUS884-03     978 0 521 19132 6                                              December 16, 2009     15:4




               105     3.2 Polynomial Interpolation

                          Given the value of x, the computations can be carried out in the following tabular
                      format (shown for four data points):

                                           k=0             k=1                k=2                    k=3
                                   x0   P0 [x0 ] =   y0   P1 [x0 , x1 ]   P2 [x0 , x1 , x2 ]   P3 [x0 , x1 , x2 , x3 ]
                                   x1   P0 [x1 ] =   y1   P1 [x1 , x2 ]   P2 [x1, x2 , x3 ]
                                   x2   P0 [x2 ] =   y2   P1 [x2 , x3 ]
                                   x3   P0 [x3 ] =   y3

                                  Table 3.2


                          If we denote the number of data points by m, the algorithm that computes the
                      elements of the table is

                      y = yData.copy()
                      for k in range (1,m):
                            for i in range(m-k):
                               y[i] = ((x - xData[i+k])*y[i] + (xData[i] - x)*y[i+1])/ \
                                        (xData[i]-xData[i+k])


                            This algorithm works with the one-dimensional array y, which initially contains
                      the y values of the data (the second column in Table 3.2). Each pass through the outer
                      loop computes the elements of y in the next column, which overwrite the previous
                      entries. At the end of the procedure, y contains the diagonal terms of the table. The
                      value of the interpolant (evaluated at x) that passes through all the data points is the
                      first element of y.


                         neville

                      The following function implements Neville’s method; it returns Pn (x).

                      ## module neville
                      ’’’ p = neville(xData,yData,x).
                            Evaluates the polynomial interpolant p(x) that passes
                            trough the specified data points by Neville’s method.
                      ’’’
                      def neville(xData,yData,x):
                            m = len(xData)            # number of data points
                            y = yData.copy()
                            for k in range(1,m):
                                  y[0:m-k] = ((x - xData[k:m])*y[0:m-k] +                                    \
                                                     (xData[0:m-k] - x)*y[1:m-k+1])/                         \
                                                     (xData[0:m-k] - xData[k:m])
                            return y[0]
P1: PHB

CUUS884-Kiusalaas    CUUS884-03         978 0 521 19132 6                                  December 16, 2009    15:4




           106      Interpolation and Curve Fitting

                                 1.00

                                 0.80


                             y
                                 0.60

                                 0.40

                                 0.20

                                 0.00

                                 -0.20
                                      -6.0        -4.0      -2.0       0.0        2.0     4.0      6.0
                                                                         x
                            Figure 3.3. Polynomial interpolant displaying oscillations.




                    Limitations of Polynomial Interpolation
                    Polynomial interpolation should be carried out with the smallest feasible number
                    of data points. Linear interpolation, using the nearest two points, is often sufficient
                    if the data points are closely spaced. Three to six nearest-neighbor points produce
                    good results in most cases. An interpolant intersecting more than six points must be
                    viewed with suspicion. The reason is that the data points that are far from the point
                    of interest do not contribute to the accuracy of the interpolant. In fact, they can be
                    detrimental.
                         The danger of using too many points is illustrated in Fig. 3.3. There are 11 equally
                    spaced data points represented by the circles. The solid line is the interpolant, a poly-
                    nomial of degree 10, that intersects all the points. As seen in the figure, a polynomial
                    of such a high degree has a tendency to oscillate excessively between the data points.
                    A much smoother result would be obtained by using a cubic interpolant spanning
                    four nearest-neighbor points.
                         Polynomial extrapolation (interpolating outside the range of data points) is dan-
                    gerous. As an example, consider Fig. 3.4. There are six data points, shown as circles.
                    The fifth-degree interpolating polynomial is represented by the solid line. The inter-
                    polant looks fine within the range of data points, but drastically departs from the
                    obvious trend when x > 12. Extrapolating y at x = 14, for example, would be absurd
                    in this case.
                         If extrapolation cannot be avoided, the following three measures can be useful:


                     • Plot the data and visually verify that the extrapolated value makes sense.
                     • Use a low-order polynomial based on nearest-neighbor data points. Linear or
                       quadratic interpolant, for example, would yield a reasonable estimate of y(14)
                       for the data in Fig. 3.4.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03     978 0 521 19132 6                                           December 16, 2009   15:4




               107     3.2 Polynomial Interpolation


                                   400

                                   300



                              y
                                   200

                                   100


                                     0

                                  -100
                                         2.0     4.0       6.0         8.0        10.0   12.0      14.0     16.0
                                                                              x
                              Figure 3.4. Extrapolation may not follow the trend of data.


                        • Work with a plot of log x versus log y, which is usually much smoother than the
                          x–y curve and thus safer to extrapolate. Frequently this plot is almost a straight
                          line. This is illustrated in Fig. 3.5, which represents the logarithmic plot of the
                          data in Fig. 3.4.
                               y




                                    100




                                     10
                                           1                                                         10
                                                                              x
                              Figure 3.5. Logarithmic plot of the data in Fig. 3.4.


                      EXAMPLE 3.1
                      Given the data points

                                                              x    0      2        3
                                                              y    7     11       28

                      use Lagrange’s method to determine y at x = 1.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03       978 0 521 19132 6                                                          December 16, 2009   15:4




           108      Interpolation and Curve Fitting

                    Solution
                                                         (x − x1 )(x − x2 )    (1 − 2)(1 − 3)   1
                                               0   =                         =                =
                                                        (x0 − x1 )(x0 − x2 )   (0 − 2)(0 − 3)   3
                                                         (x − x0 )(x − x2 )    (1 − 0)(1 − 3)
                                               1   =                         =                =1
                                                        (x1 − x0 )(x1 − x2 )   (2 − 0)(2 − 3)
                                                         (x − x0 )(x − x1 )    (1 − 0)(1 − 2)    1
                                               2   =                         =                =−
                                                        (x2 − x0 )(x2 − x1 )   (3 − 0)(3 − 2)    3

                                                                                           7        28
                                                   y = y0    0   + y1   1   + y2   2   =     + 11 −    =4
                                                                                           3        3
                    EXAMPLE 3.2
                    The data points

                                                        x    −2         1      4       −1       3      −4
                                                        y    −1         2     59        4      24     −53

                    lie on a polynomial. Determine the degree of this polynomial by constructing the
                    divided difference table, similar to Table 3.1.

                    Solution
                                       i           xi       yi      ∇ yi       ∇ 2 yi       ∇ 3 yi   ∇ 4 yi   ∇ 5 yi
                                       0           −2        −1
                                       1            1         2          1
                                       2            4        59         10          3
                                       3           −1         4          5         −2           1
                                       4            3        24          5          2           1        0
                                       5           −4       −53         26         −5           1        0        0

                    Here are a few sample calculations used in arriving at the figures in the table:
                                                                   y2 − y0   59 − (−1)
                                                         ∇ y2 =            =           = 10
                                                                   x2 − x0   4 − (−2)
                                                                   ∇ y2 − ∇ y1   10 − 1
                                                        ∇ 2 y2 =               =        =3
                                                                     x2 − x1      4−1
                                                                   ∇ 2 y5 − ∇ 2 y2   −5 − 3
                                                        ∇ 3 y5 =                   =        =1
                                                                       x5 − x2       −4 − 4
                    From the table we see that the last nonzero coefficient (last nonzero diagonal term)
                    of Newton’s polynomial is ∇ 3 y3 , which is the coefficient of the cubic term. Hence, the
                    polynomial is a cubic.

                    EXAMPLE 3.3
                    Given the data points

                                           x           4.0                 3.9                3.8          3.7
                                           y        −0.06604            −0.02724            0.01282      0.05383

                    determine the root of y(x) = 0 by Neville’s method.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                                December 16, 2009   15:4




               109     3.2 Polynomial Interpolation

                      Solution This is an example of inverse interpolation, where the roles of x and y are
                      interchanged. Instead of computing y at a given x, we are finding x that corresponds
                      to a given y (in this case, y = 0). Employing the format of Table 3.2 (with x and y
                      interchanged, of course), we obtain

                                         i          yi          P0 [ ] = xi    P1 [ , ]    P2 [ , , ]    P3 [ , , , ]
                                        0    −0.06604              4.0        3.8298       3.8316          3.8317
                                        1    −0.02724              3.9        3.8320       3.8318
                                        2     0.01282              3.8        3.8313
                                        3     0.05383              3.7

                      The following are sample computations used in the table:
                                                    (y − y1 )P0 [y0 ] + (y0 − y)P0 [y1 ]
                                  P1 [y0 , y1 ] =
                                                                  y0 − y1
                                                    (0 + 0.02724)(4.0) + (−0.06604 − 0)(3.9)
                                              =                                              = 3.8298
                                                              −0.06604 + 0.02724
                                                    (y − y3 )P1 [y1 , y2 ] + (y1 − y)P1 [y2 , y3 ]
                            P2 [y1 , y2 , y3 ] =
                                                                       y1 − y3
                                                    (0 − 0.05383)(3.8320) + (−0.02724 − 0)(3.8313)
                                              =                                                    = 3.8318
                                                                 −0.02724 − 0.05383
                           All the Ps in the table are estimates of the root resulting from different orders of
                      interpolation involving different data points. For example, P1 [y0 , y1 ] is the root ob-
                      tained from linear interpolation based on the first two points, and P2 [y1 , y2 , y3 ] is
                      the result from quadratic interpolation using the last three points. The root obtained
                      from cubic interpolation over all four data points is x = P3 [y0 , y1 , y2 , y3 ] = 3.8317.

                      EXAMPLE 3.4
                                                                                      πx
                      The data points in the table lie on the plot of f (x) = 4.8 cos    . Interpolate this data
                                                                                      20
                      by Newton’s method at x = 0, 0.5, 1.0, . . . , 8.0 and compare the results with the “ex-
                      act” values yi = f (xi ).

                                   x       0.15            2.30       3.15         4.85          6.25            7.95
                                   y     4.79867         4.49013     4.2243      3.47313       2.66674         1.51909

                      Solution

                      #!/usr/bin/python
                      ## example3_4
                      from numpy import array,arange
                      from math import pi,cos
                      from newtonPoly import *


                      xData = array([0.15,2.3,3.15,4.85,6.25,7.95])
                      yData = array([4.79867,4.49013,4.2243,3.47313,2.66674,1.51909])
                      a = coeffts(xData,yData)
                      print ’’ x             yInterp           yExact’’
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                        December 16, 2009   15:4




           110      Interpolation and Curve Fitting

                    print ’’-----------------------’’
                    for x in arange(0.0,8.1,0.5):
                          y = evalPoly(a,xData,x)
                          yExact = 4.8*cos(pi*x/20.0)
                          print ’’%3.1f %9.5f %9.5f’’% (x,y,yExact)
                    raw_input(’’\nPress return to exit’’)


                          The results are:

                     x       yInterp         yExact
                    -----------------------
                    0.0      4.80003         4.80000
                    0.5      4.78518         4.78520
                    1.0      4.74088         4.74090
                    1.5      4.66736         4.66738
                    2.0      4.56507         4.56507
                    2.5      4.43462         4.43462
                    3.0      4.27683         4.27683
                    3.5      4.09267         4.09267
                    4.0      3.88327         3.88328
                    4.5      3.64994         3.64995
                    5.0      3.39411         3.39411
                    5.5      3.11735         3.11735
                    6.0      2.82137         2.82137
                    6.5      2.50799         2.50799
                    7.0      2.17915         2.17915
                    7.5      1.83687         1.83688
                    8.0      1.48329         1.48328




                    Rational Function Interpolation
                    Some data is better interpolated by rational functions rather than polynomials. A ra-
                    tional function R(x) is the quotient of two polynomials:

                                                Pm (x)   a 1 x m + a 2 x m−1 + · · · + a m x + a m+1
                                       R(x) =          =
                                                Qn (x)    b1 x n + b2 x n−1 + · · · + bn x + bn+1

                    Because R(x) is a ratio, it can be scaled so that one of the coefficients (usually bn+1 )
                    is unity. That leaves m + n + 1 undetermined coefficients that must be computed by
                    forcing R(x) through m + n + 1 data points.
                         A popular version of R(x) is the diagonal rational function, where the degree of
                    the numerator is equal to that of the denominator (m = n) if m + n is even, or less by
                    1 (m = n − 1) if m + n is odd. The advantage of using the diagonal form is that the in-
                    terpolation can be carried out with a Neville-type algorithm, similar to that outlined
                    in Table 3.2. The recursive formula that is the basis of the algorithm is due to Stoer
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                                     December 16, 2009          15:4




               111     3.2 Polynomial Interpolation

                                       k = −1           k=0               k=1               k=2                    k=3
                                 x1       0           R[x1 ] = y1        R[x1 , x2 ]     R[x1 , x2 , x3 ]     R[x1 , x2 , x3 , x4 ]
                                 x2       0           R[x2 ] = y2        R[x2 , x3 ]     R[x2 , x3 , x4 ]
                                 x3       0           R[x3 ] = y3        R[x3 , x4 ]
                                 x4       0           R[x4 ] = y4

                               Table 3.3


                      and Bulirsch.1 It is somewhat more complex than Eq. (3.8) used in Neville’s method:

                                 R[xi , xi+1 , . . . , xi+k ] = R[xi+1 , xi+2 , . . . , xi+k ]                                        (3.9a)
                                                                    R[xi+1 , xi+2 , . . . , xi+k ] − R[xi , xi+1 , . . . , xi+k−1 ]
                                                                +
                                                                                                   S
                      where
                                  x − xi               R[xi+1 , xi+2 , . . . , xi+k ] − R[xi , xi+1 , . . . , xi+k−1 ]
                            S=                 1−                                                                           −1        (3.9b)
                                 x − xi+k             R[xi+1 , xi+2 , . . . , xi+k ] − R[xi+1 , xi+2 , . . . , xi+k−1 ]
                      In Eqs. (3.9) R[xi , xi+1 , . . . , xi+k ] denotes the diagonal rational function that passes
                      through the data points (xi , yi ), (xi+1 , yi+1 ), . . . , (xi+k , yi+k ). It is also understood that
                      R[xi , xi+1 , . . . , xi−1 ] = 0 (corresponding to the case k = −1) and R[xi ] = yi (the case
                      k = 0).
                           The computations can be carried out in a tableau, similar to Table 3.2 used for
                      Neville’s method. An example of the tableau for four data points is shown in Table 3.3.
                      We start by filling the column k = −1 with zeroes and entering the values of yi in the
                      column k = 0. The remaining entries are computed by applying Eqs. (3.9).


                          rational

                      We managed to implement Neville’s algorithm with the tableau “compressed” to a
                      one-dimensional array. This will not work with the rational function interpolation,
                      where the formula for computing an R in the kth column involves entries in columns
                      k − 1 as well as k − 2. However, we can work with two one-dimensional arrays, one
                      array (called r in the program) containing the latest values of R while the other array
                      (rOld) saves the previous entries. Here is the algorithm for diagonal rational function
                      interpolation:


                      ## module rational
                      ’’’ p = rational(xData,yData,x)
                             Evaluates the diagonal rational function interpolant p(x)
                             that passes through he data points
                      ’’’
                      from numpy import zeros



                      1   J. Stoer, and R. Bulirsch, Introduction to Numerical Analysis (Springer, 1980).
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                 December 16, 2009   15:4




           112      Interpolation and Curve Fitting

                    def rational(xData,yData,x):
                         m = len(xData)
                         r = yData.copy()
                         rOld = zeros(m)
                         for k in range(m-1):
                               for i in range(m-k-1):
                                      if abs(x - xData[i+k+1]) < 1.0e-9:
                                             return yData[i+k+1]
                                      else:
                                             c1 = r[i+1] - r[i]
                                             c2 = r[i+1] - rOld[i+1]
                                             c3 = (x - xData[i])/(x - xData[i+k+1])
                                             r[i] = r[i+1] + c1/(c3*(1.0 - c1/c2) - 1.0)
                                             rOld[i+1] = r[i+1]
                         return r[0]



                    EXAMPLE 3.5
                    Given the data


                                                x    0       0.6      0.8      0.95
                                                y    0     1.3764   3.0777   12.7062


                    determine y(0.5) by the diagonal rational function interpolation.


                    Solution The plot of the data points indicates that y may have a pole at around x = 1.
                    Such a function is a very poor candidate for polynomial interpolation, but can be
                    readily represented by a rational function.


                               14.0

                               12.0

                               10.0
                           y




                                8.0

                                6.0

                                4.0

                                2.0

                                0.0
                                       0.0          0.2         0.4          0.6       0.8        1.0
                                                                       x
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                             December 16, 2009       15:4




               113     3.2 Polynomial Interpolation

                          We set up our work in the format of Table 3.3. After we complete the computa-
                      tions, the table looks like this:
                                                        k = −1         k=0           k=1         k=2       k=3
                                   i   =1        0         0              0             0        0.9544    1.0131
                                   i   =2       0.6        0           1.3764        1.0784      1.0327
                                   i   =3       0.8        0           3.0777        1.2235
                                   i   =4      0.95        0          12.7062

                      Let us now look at a few sample computations. We obtain, for example, R[x3 , x4 ] by
                      substituting i = 3, k = 1 into Eqs. (3.9). This yields
                                               x − x3              R[x4 ] − R[x3 ]
                                        S=              1−                                     −1
                                               x − x4           R[x4 ] − R[x4 , . . . , x3 ]
                                               0.5 − 0.8     12.7062 − 3.0777
                                           =              1−                                    − 1 = −0.83852
                                               0.5 − 0.95      12.7062 − 0

                                                                  R[x4 ] − R[x3 ]
                                            R[x3 , x4 ] = R[x4 ] +
                                                                         S
                                                                    12.7062 − 3.0777
                                                        = 12.7062 +                  = 1.2235
                                                                         −0.83852
                      The entry R[x2 , x3 , x4 ] is obtained with i = 2, k = 2. The result is
                                               x − x2           R[x3 , x4 ] − R[x2 , x3 ]
                                         S=                1−                                  −1
                                               x − x4            R[x3 , x4 ] − R[x3 ]
                                                0.5 − 0.6    1.2235 − 1.0784
                                           =              1−                                   − 1 = −0.76039
                                               0.5 − 0.95    1.2235 − 3.0777

                                                                         R[x3 , x4 ] − R[x2 , x3 ]
                                            R[x2 , x3 , x4 ] = R[x3 , x4 ] +
                                                                                     S
                                                                       1.2235 − 1.0784
                                                            = 1.2235 +                    = 1.0327
                                                                          −0.76039
                      The interpolant at x = 0.5 based on all four data points is R[x1 , x2 , x3 , x4 ] = 1.0131.

                      EXAMPLE 3.6
                      Interpolate the data shown at x increments of 0.05 and plot the results. Use both the
                      polynomial interpolation and the rational function interpolation.

                          x        0.1           0.2           0.5              0.6              0.8        1.2       1.5
                          y      −1.5342       −1.0811       −0.4445          −0.3085          −0.0868    0.2281    0.3824

                      Solution

                      #!/usr/bin/python
                      ## example 3_6
                      from numpy import array,arange
                      from rational import *
                      from neville import *
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                  December 16, 2009       15:4




           114      Interpolation and Curve Fitting

                    xData = array([0.1,0.2,0.5,0.6,0.8,1.2,1.5])
                    yData = array([-1.5342,-1.0811,-0.4445,-0.3085, \
                                        -0.0868,0.2281,0.3824])
                    x = arange(0.1,1.55,0.05)
                    n = len(x)
                    y = zeros((n,2))
                    print ’     x     Rational       Neville’
                    for i in range(n):
                         y[i,0] = rational(xData,yData,x[i])
                         y[i,1] = neville(xData,yData,x[i])
                         print ’%4.2f %9.5f %9.5f’% (x[i],y[i,0],y[i,1])


                        A plot of the printed output (the printout is not shown) follows.




                       In this case, the rational function interpolant (solid line) is smoother and thus
                    superior to the polynomial interpolant (dotted line).



          3.3       Interpolation with Cubic Spline

                    If there are more than a few data points, a cubic spline is hard to beat as a global
                    interpolant. It is considerably ”stiffer” than a polynomial in the sense that it has less
                    tendency to oscillate between data points.
                         The mechanical model of a cubic spline is shown in Fig. 3.6. It is a thin, elastic
                    beam that is attached with pins to the data points. Because the beam is unloaded
                    between the pins, each segment of the spline curve is a cubic polynomial – recall from
                    beam theory that d 4 y/dx 4 = q/(E I ), so that y(x) is a cubic since q = 0. At the pins,
P1: PHB

CUUS884-Kiusalaas    CUUS884-03       978 0 521 19132 6                                                  December 16, 2009   15:4




               115     3.3 Interpolation with Cubic Spline


                                                                                                Elastic strip

                                        y

                                                          Pins (data points)
                                                               x
                                        Figure 3.6. Mechanical model of natural cubic spline.



                      the slope and bending moment (and hence the second derivative) are continuous.
                      There is no bending moment at the two end pins; consequently, the second derivative
                      of the spline is zero at the end points. Because these end conditions occur naturally
                      in the beam model, the resulting curve is known as the natural cubic spline. The pins,
                      that is, the data points, are called the knots of the spline.
                            Figure 3.7 shows a cubic spline that spans n + 1 knots. We use the notation
                      fi,i+1 (x) for the cubic polynomial that spans the segment between knots i and i + 1.
                      Note that the spline is a piecewise cubic curve, put together from the n cubics
                      f0,1 (x), f1,2 (x), . . . , fn−1,n (x), all of which have different coefficients.
                            Denoting the second derivative of the spline at knot i by ki , continuity of second
                      derivatives requires that

                                                             fi−1,i (xi ) = fi,i+1 (xi ) = ki                                (a)

                      At this stage, each k is unknown, except for

                                                                     k 0 = kn = 0

                            The starting point for computing the coefficients of fi,i+1 (x) is the expression for
                      fi,i+1 (x), which we know to be linear. Using Lagrange’s two-point interpolation, we
                      can write

                                                          fi,i+1 (x) = ki i (x) + ki+1    i+1 (x)




                                                                                         f i, i + 1(x)
                                  y



                                                                   yi - 1      yi     yi + 1
                                         y0       y1
                                                                                                    yn - 1
                                                                                                                yn
                                            x0 x1                     x i - 1 xi xi + 1                  xn - 1 xn x
                                  Figure 3.7. Cubic spline.
P1: PHB

CUUS884-Kiusalaas     CUUS884-03        978 0 521 19132 6                                                   December 16, 2009         15:4




           116      Interpolation and Curve Fitting

                    where
                                                               x − xi+1                         x − xi
                                                  i (x)   =                      1+1 (x)   =
                                                               xi − xi+1                       xi+1 − xi
                    Therefore,
                                                                      ki (x − xi+1 ) − ki+1 (x − xi )
                                                   fi,i+1 (x) =                                                                 (b)
                                                                                xi − xi+1
                    Integrating twice with respect to x, we obtain
                                                ki (x − xi+1 )3 − ki+1 (x − xi )3
                                 fi,i+1 (x) =                                     + A (x − xi+1 ) − B(x − xi )                  (c)
                                                         6(xi − xi+1 )
                    where A and B are constants of integration. The terms arising from the integration
                    would usually be written as C x + D. By letting C = A − B and D = −A xi+1 + Bxi , we
                    end up with the last two terms of Eq. (c), which are more convenient to use in the
                    computations that follow.
                        Imposing the condition fi.i+1 (xi ) = yi , we get from Eq. (c)

                                                      ki (xi − xi+1 )3
                                                                       + A (xi − xi+1 ) = yi
                                                       6(xi − xi+1 )
                    Therefore,
                                                                      yi     ki
                                                          A=                − (xi − xi+1 )                                      (d)
                                                                  xi − xi+1  6
                    Similarly, fi,i+1 (xi+1 ) = yi+1 yields
                                                                   yi+1     ki+1
                                                          B=              −      (xi − xi+1 )                                   (e)
                                                                xi − xi+1    6
                    Substituting Eqs. (d) and (e) into Eq. (c) results in

                                                          ki     (x − xi+1 )3
                                          fi,i+1 (x) =                        − (x − xi+1 )(xi − xi+1 )
                                                          6       xi − xi+1
                                                               ki+1    (x − xi )3
                                                          −                       − (x − xi )(xi − xi+1 )                 (3.10)
                                                                6      xi − xi+1
                                                               yi (x − xi+1 ) − yi+1 (x − xi )
                                                          +
                                                                         xi − xi+1

                        The second derivatives ki of the spline at the interior knots are obtained from the
                    slope continuity conditions fi−1,i (xi ) = fi,i+1 (xi ), where i = 1, 2, . . . , n − 1. After a little
                    algebra, this results in the simultaneous equations

                                            ki−1 (xi−1 − xi ) + 2ki (xi−1 − xi+1 ) + ki+1 (xi − xi+1 )
                                                  yi−1 − yi   yi − yi+1
                                         =6                 −                     , i = 1, 2, · · · , n − 1               (3.11)
                                                  xi−1 − xi   xi − xi+1
                    Because Eqs. (3.11) have a tridiagonal coefficient matrix, they can be solved econom-
                    ically with the functions in module LUdecomp3 described in Section 2.4.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03    978 0 521 19132 6                                      December 16, 2009     15:4




               117     3.3 Interpolation with Cubic Spline

                          If the data points are evenly spaced at intervals h, then xi−1 − xi = xi − xi+1 = −h,
                      and Eqs. (3.11) simplify to

                                                        6
                                  ki−1 + 4ki + ki+1 =      (yi−1 − 2yi + yi+1 ), i = 1, 2, . . . , n − 1      (3.12)
                                                        h2


                         cubicSpline

                      The first stage of cubic spline interpolation is to set up Eqs. (3.11) and solve them for
                      the unknown ks (recall that k 0 = kn = 0). This task is carried out by the function cur-
                      vatures. The second stage is the computation of the interpolant at x from Eq. (3.10).
                      This step can be repeated any number of times with different values of x using the
                      function evalSpline. The function findSegment embedded in evalSpline finds
                      the segment of the spline that contains x using the method of bisection. It returns the
                      segment number, that is, the value of the subscript i in Eq. (3.10).


                      ## module cubicSpline
                      ’’’ k = curvatures(xData,yData).
                            Returns the curvatures of cubic spline at its knots.


                            y = evalSpline(xData,yData,k,x).
                            Evaluates cubic spline at x. The curvatures k can be
                            computed with the function ’curvatures’.
                      ’’’
                      from numpy import zeros,ones
                      from LUdecomp3 import *


                      def curvatures(xData,yData):
                            n = len(xData) - 1
                            c = zeros(n)
                            d = ones(n+1)
                            e = zeros(n)
                            k = zeros(n+1)
                            c[0:n-1] = xData[0:n-1] - xData[1:n]
                            d[1:n] = 2.0*(xData[0:n-1] - xData[2:n+1])
                            e[1:n] = xData[1:n] - xData[2:n+1]
                            k[1:n] =6.0*(yData[0:n-1] - yData[1:n]) \
                                             /(xData[0:n-1] - xData[1:n]) \
                                       -6.0*(yData[1:n] - yData[2:n+1])                   \
                                             /(xData[1:n] - xData[2:n+1])
                            LUdecomp3(c,d,e)
                            LUsolve3(c,d,e,k)
                            return k
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                    December 16, 2009   15:4




           118      Interpolation and Curve Fitting

                    def evalSpline(xData,yData,k,x):


                         def findSegment(xData,x):
                              iLeft = 0
                              iRight = len(xData)- 1
                              while 1:
                                   if (iRight-iLeft) <= 1: return iLeft
                                   i =(iLeft + iRight)/2
                                   if x < xData[i]: iRight = i
                                   else: iLeft = i


                         i = findSegment(xData,x)
                         h = xData[i] - xData[i+1]
                         y = ((x - xData[i+1])**3/h - (x - xData[i+1])*h)*k[i]/6.0 \
                           - ((x - xData[i])**3/h - (x - xData[i])*h)*k[i+1]/6.0                        \
                           + (yData[i]*(x - xData[i+1])                                                 \
                             - yData[i+1]*(x - xData[i]))/h
                         return y



                    EXAMPLE 3.7
                    Use the natural cubic spline to determine y at x = 1.5. The data points are


                                                         x   1   2   3   4   5
                                                         y   0   1   0   1   0


                    Solution The five knots are equally spaced at h = 1. Recalling that the second deriva-
                    tive of a natural spline is zero at the first and last knot, we have k 0 = k 4 = 0. The
                    second derivatives at the other knots are obtained from Eq. (3.12). Using i = 1, 2, 3
                    results in the simultaneous equations


                                               0 + 4k 1 + k 2 = 6 [0 − 2(1) + 0] = −12

                                              k 1 + 4k 2 + k 3 = 6 [1 − 2(0) + 1] = 12

                                               k 2 + 4k 3 + 0 = 6 [0 − 2(1) + 0] = −12


                    The solution is k 1 = k 3 = −30/7, k 2 = 36/7.
                        The point x = 1.5 lies in the segment between knots 0 and 1. The corresponding
                    interpolant is obtained from Eq. (3.10) by setting i = 0. With xi − xi+1 = −h = −1, we
                    obtain from Eq. (3.10)

                                           k0                            k1
                               f0,1 (x) = −     (x − x1 )3 − (x − x1 ) +    (x − x0 )3 − (x − x0 )
                                            6                            6
                                         − [y0 (x − x1 ) − y1 (x − x0 )]
P1: PHB

CUUS884-Kiusalaas    CUUS884-03         978 0 521 19132 6                                      December 16, 2009   15:4




               119     3.3 Interpolation with Cubic Spline

                      Therefore,

                                   y(1.5) = f0,1 (1.5)
                                                    1   30
                                             = 0+     −            (1.5 − 1)3 − (1.5 − 1) − [0 − 1(1.5 − 1)]
                                                    6    7
                                             = 0.7679

                      The plot of the interpolant, which in this case is made up of four cubic segments, is
                      shown in the figure.



                                  1.00

                                  0.80
                              y




                                  0.60

                                  0.40

                                  0.20

                                  0.00
                                     1.00        1.50       2.00    2.50       3.00   3.50   4.00   4.50   5.00
                                                                                x
                      EXAMPLE 3.8
                      Sometimes it is preferable to replace one or both of the end conditions of the cu-
                      bic spline with something other than the natural conditions. Use the end condition
                      f0,1 (0) = 0 (zero slope), rather than f0,1 (0) = 0 (zero curvature), to determine the cu-
                      bic spline interpolant at x = 2.6, given the data points

                                                               x     0     1     2    3
                                                               y     1     1    0.5   0

                      Solution We must first modify Eqs. (3.12) to account for the new end condition. Set-
                      ting i = 0 in Eq. (3.10) and differentiating, we get

                                        k0   (x − x1 )2                k1   (x − x0 )2                y0 − y1
                           f0,1 (x) =      3            − (x0 − x1 ) −    3            − (x0 − x1 ) +
                                        6     x0 − x1                  6     x0 − x1                  x0 − x1
                      Thus, the end condition f0,1 (x0 ) = 0 yields

                                                   k0             k1            y0 − y1
                                                      (x0 − x1 ) + (x0 − x1 ) +         =0
                                                   3              6             x0 − x1
                      or
                                                                                y0 − y1
                                                             2k 0 + k 1 = −6
                                                                               (x0 − x1 )2
P1: PHB

CUUS884-Kiusalaas    CUUS884-03         978 0 521 19132 6                                     December 16, 2009         15:4




           120      Interpolation and Curve Fitting

                    From the given data, we see that y0 = y1 = 1, so that the last equation becomes

                                                               2k 0 + k 1 = 0                                     (a)

                    The other equations in Eq. (3.12) are unchanged. Knowing that k 3 = 0, they are

                                                 k 0 + 4k 1 + k 2 = 6 [1 − 2(1) + 0.5] = −3                       (b)

                                                      k 1 + 4k 2 = 6 [1 − 2(0.5) + 0] = 0                         (c)

                    The solution of Eqs. (a)–(c) is k 0 = 0.4615, k 1 = −0.9231, k 2 = 0.2308.
                         The interpolant can now be evaluated from Eq. (3.10). Substituting i = 2 and xi −
                    xi+1 = −1, we obtain

                                              k2                           k3
                                 f2,3 (x) =       −(x − x3 )3 + (x − x3) −    −(x − x2 )3 + (x − x2 )
                                              6                            6
                                              −y2 (x − x3 ) + y3 (x − x2 )

                    Therefore,

                                                       0.2308
                               y(2.6) = f2,3 (2.6) =          −(−0.4)3 + (−0.4) − 0 − 0.5(−0.4) + 0
                                                          6
                                      = 0.1871

                    EXAMPLE 3.9
                    Utilize the module cubicSpline to write a program that interpolates between given
                    data points with the natural cubic spline. The program must be able to evaluate the
                    interpolant for more than one value of x. As a test, use the data points specified in Ex-
                    ample 3.4 and compute the interpolant at x = 1.5 and x = 4.5 (because of symmetry,
                    these values should be equal).

                    Solution

                    #!/usr/bin/python
                    ## example3_9
                    from numpy import array,float
                    from cubicSpline import *


                    xData = array([1,2,3,4,5],dtype=float)
                    yData = array([0,1,0,1,0],dtype=float)
                    k = curvatures(xData,yData)
                    while 1:
                         try: x = eval(raw_input(’’\nx ==> ’’))
                         except SyntaxError: break
                         print ’’y =’’,evalSpline(xData,yData,k,x)
                    raw_input(’’Done. Press return to exit’’)
P1: PHB

CUUS884-Kiusalaas    CUUS884-03       978 0 521 19132 6                                               December 16, 2009      15:4




               121     3.3 Interpolation with Cubic Spline

                          Running the program produces the following result:

                      x ==> 1.5
                      y = 0.767857142857


                      x ==> 4.5
                      y = 0.767857142857


                      x ==>
                      Done. Press return to exit


                      PROBLEM SET 3.1
                       1. Given the data points

                                                            x        −1.2       0.3          1.1
                                                            y        −5.76     −5.61        −3.69

                          determine y at x = 0 using (a) Neville’s method and (b) Lagrange’s method.
                       2. Find the zero of y(x) from the following data:

                                  x       0        0.5             1            1.5           2         2.5           3
                                  y    1.8421    2.4694         2.4921        1.9047       0.8509     −0.4112      −1.5727

                          Use Lagrange’s interpolation over (a) three and (b) four nearest-neighbor data
                          points. Hint: After finishing part (a), part (b) can be computed with a relatively
                          small effort.
                       3. The function y(x) represented by the data in Problem 2 has a maximum at
                          x = 0.7692. Compute this maximum by Neville’s interpolation over four nearest-
                          neighbor data points.
                       4. Use Neville’s method to compute y at x = π/4 from the data points

                                                    x         0         0.5          1       1.5       2
                                                    y       −1.00       1.75       4.00     5.75     7.00

                       5. Given the data

                                             x       0                 0.5           1         1.5           2
                                             y    −0.7854            0.6529       1.7390     2.2071       1.9425

                          find y at x = π/4 and at π /2. Use the method that you consider to be most con-
                          venient.
                       6. The points

                                                        x       −2     1      4     −1       3       −4
                                                        y       −1     2     59      4      24      −53

                          lie on a polynomial. Use the divided difference table of Newton’s method to de-
                          termine the degree of the polynomial.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                              December 16, 2009   15:4




           122      Interpolation and Curve Fitting

                     7. Use Newton’s method to find the polynomial that fits the following points:

                                                      x       −3     2      −1      3     1
                                                      y        0     5      −4     12     0

                     8. Use Neville’s method to determine the equation of the quadratic that passes
                        through the points

                                                          x        −1        1       3
                                                          y        17       −7     −15

                     9. The density of air ρ varies with elevation h in the following manner:

                                                    h (km)            0            3        6
                                                  ρ (kg/m3 )        1.225        0.905    0.652

                        Express ρ(h) as a quadratic function using Lagrange’s method.
                    10. Determine the natural cubic spline that passes through the data points

                                                               x        0    1    2
                                                               y        0    2    1

                        Note that the interpolant consists of two cubics, one valid in 0 ≤ x ≤ 1, the other
                        in 1 ≤ x ≤ 2. Verify that these cubics have the same first and second derivatives
                        at x = 1.
                    11. Given the data points

                                                      x        1     2       3        4    5
                                                      y       13    15       12       9   13

                        determine the natural cubic spline interpolant at x = 3.4.
                    12. Compute the zero of the function y(x) from the following data:

                                          x     0.2        0.4           0.6        0.8         1.0
                                          y    1.150      0.855         0.377      −0.266      −1.049

                        Use inverse interpolation with the natural cubic spline. Hint: reorder the data so
                        that the values of y are in ascending order.
                    13. Solve Example 3.6 with a cubic spline that has constant second derivatives within
                        its first and last segments (the end segments are parabolic). The end conditions
                        for this spline are k 0 = k 1 and kn−1 = kn .
                    14.    Write a computer program for interpolation by Neville’s method. The program
                        must be able to compute the interpolant at several user-specified values of x. Test
                        the program by determining y at x = 1.1, 1.2, and 1.3 from the following data:

                                              x      −2.0           −0.1          −1.5       0.5
                                              y     2.2796         1.0025        1.6467    1.0635
                                              x      −0.6            2.2           1.0       1.8
                                              y     1.0920         2.6291        1.2661    1.9896

                        (Answer: y = 1.3262, 1.3938, 1.4639)
P1: PHB

CUUS884-Kiusalaas    CUUS884-03       978 0 521 19132 6                                                 December 16, 2009     15:4




               123     3.3 Interpolation with Cubic Spline

                      15.      The specific heat c p of aluminum depends on temperature T as follows2 :

                                          T (◦ C)          −250           −200        −100        0        100      300
                                      c p (kJ/kg·K)       −0.0163         0.318       0.699     0.870     0.941     1.04

                          Plot the polynomial and the rational function interpolants from T = −250◦ to
                          500◦ . Comment on the results.
                      16.   Using the data

                                       x       0       0.0204       0.1055         0.241      0.582     0.712     0.981
                                       y     0.385      1.04         1.79          2.63        4.39      4.99      5.27

                            plot the rational function interpolant from x = 0 to x = 1.
                      17.     The table shows the drag coefficient c D of a sphere as a function of the Reynolds
                            number Re.3 Use the natural cubic spline to find c D at Re = 5, 50, 500, and 5000.
                            Hint: use log–log scale.

                                               Re      0.2        2          20      200       2000      20 000
                                               cD      103       13.9       2.72    0.800      0.401     0.433

                      18.   Solve Prob. 17 using a polynomial interpolant intersecting four nearest-
                          neighbor data points (do not use log scale).
                      19.   The kinematic viscosity µk of water varies with temperature T in the following
                          manner:

                                      T (◦ C)               0      21.1        37.8      54.4      71.1      87.8      100
                                  µk (10−3 m2 /s)         1.79     1.13       0.696     0.519     0.338     0.321     0.296

                          Interpolate µk at T = 10◦ , 30◦ , 60◦ , and 90◦ C.
                      20.   The table shows how the relative density ρ of air varies with altitude h. Deter-
                          mine the relative density of air at 10.5 km.

                                    h (km)     0       1.525      3.050         4.575          6.10      7.625     9.150
                                       ρ       1      0.8617      0.7385       0.6292         0.5328    0.4481     0.3741

                      21.     The vibrational amplitude of a driveshaft is measured at various speeds. The
                            results are

                                             Speed (rpm)                0     400       800       1200       1600
                                             Amplitude (mm)             0     0.072     0.233     0.712      3.400

                            Use rational function interpolation to plot amplitude versus speed from 0 to 2500
                            rpm. From the plot, estimate the speed of the shaft at resonance.


                      2   Source: Z. B. Black, and J. G. Hartley, Thermodynamics (Harper & Row, 1985).
                      3   Source: F. Kreith, Principles of Heat Transfer (Harper & Row, 1973).
P1: PHB

CUUS884-Kiusalaas     CUUS884-03       978 0 521 19132 6                                                 December 16, 2009      15:4




           124      Interpolation and Curve Fitting

          3.4       Least-Squares Fit
                    Overview
                    If the data are obtained from experiments, these typically contain a significant
                    amount of random noise due to measurement errors. The task of curve fitting is to
                    find a smooth curve that fits the data points “on the average.” This curve should have
                    a simple form (e.g., a low-order polynomial), so as to not reproduce the noise.
                         Let

                                                       f (x) = f (x; a 0 , a 1 , . . . , a m )

                    be the function that is to be fitted to the n + 1 data points (xi , yi ), i = 0, 1, . . . , n. The
                    notation implies that we have a function of x that contains m + 1 variable parameters
                    a 0 , a 1 , . . . , a m , where m < n. The form of f (x) is determined beforehand, usually from
                    the theory associated with the experiment from which the data are obtained. The
                    only means of adjusting the fit are the parameters. For example, if the data represent
                    the displacements yi of an overdamped mass–spring system at time ti , the theory
                    suggests the choice f (t ) = a 0te−a 1t . Thus, curve fitting consists of two steps: choosing
                    the form of f (x), followed by computation of the parameters that produce the best fit
                    to the data.
                           This brings us to the question: What is meant by “best” fit? If the noise is confined
                    to the y-coordinate, the most commonly used measure is the least-squares fit, which
                    minimizes the function
                                                                                n
                                                                                                     2
                                                 S(a 0 , a 1, . . . , a m ) =         yi − f (xi )                     (3.13)
                                                                                i=0

                    with respect to each a j . Therefore, the optimal values of the parameters are given by
                    the solution of the equations
                                                       ∂S
                                                            = 0, k = 0, 1, . . . , m                                   (3.14)
                                                       ∂a k
                    The terms ri = yi − f (xi ) in Eq. (3.13) are called residuals; they represent the discrep-
                    ancy between the data points and the fitting function at xi . The function S to be min-
                    imized is thus the sum of the squares of the residuals. Equations (3.14) are generally
                    nonlinear in a j and may thus be difficult to solve. Often the fitting function is chosen
                    as a linear combination of specified functions f j (x):

                                              f (x) = a 0 f0 (x) + a 1 f1 (x) + · · · + a m fm (x)

                    in which case Eqs. (3.14) are linear. If the fitting function is a polynomial, we have
                    f0 (x) = 1, f1 (x) = x, f2 (x) = x 2 , and so on.
                         The spread of the data about the fitting curve is quantified by the standard devi-
                    ation, defined as

                                                                            S
                                                                 σ =                                                   (3.15)
                                                                           n−m
P1: PHB

CUUS884-Kiusalaas    CUUS884-03     978 0 521 19132 6                                                              December 16, 2009           15:4




               125     3.4 Least-Squares Fit

                      Note that if n = m, we have interpolation, not curve fitting. In that case both the nu-
                      merator and the denominator in Eq. (3.15) are zero, so that σ is indeterminate.


                      Fitting a Straight Line
                      Fitting a straight line

                                                                     f (x) = a + bx                                                          (3.16)

                      to data is also known as linear regression. In this case, the function to be minimized
                      is
                                                              n                              n
                                                                                  2                                        2
                                               S(a, b) =           yi − f (xi )       =           yi − a − bxi
                                                             i=0                            i=0

                      Equations (3.14) now become
                                         n                                                           n                n
                                  ∂S
                                     =         −2(yi − a − bxi ) = 2 a (n + 1) + b                           xi −              yi = 0
                                  ∂a
                                         i=0                                                         i=0             i=0
                                         n                                             n              n                 n
                                  ∂S
                                     =         −2(yi − a − bxi )xi = 2 a                    xi + b           xi2 −              xi yi   =0
                                  ∂b
                                         i=0                                          i=0            i=0              i=0

                      Dividing both equations by 2 (n + 1) and rearranging terms, we get
                                                                                      n                                n
                                                                           1                              1
                                         a + xb
                                             ¯ = y¯            ¯ +
                                                               xa                           xi2 b =                            xi yi
                                                                          n+1                            n+1
                                                                                  i=0                                i=0

                      where
                                                                      n                                  n
                                                             1                                1
                                                     x¯ =                 xi          y¯ =                    yi                             (3.17)
                                                            n+1                              n+1
                                                                    i=0                               i=0

                      are the mean values of the x and y data. The solution for the parameters is
                                                    y¯      xi2 − x¯ xi yi                         xi yi − x¯ yi
                                               a=                                      b=                                                    (3.18)
                                                              xi2 − n x¯ 2                            xi2 − n x¯ 2
                      These expressions are susceptible to roundoff errors (the two terms in each numera-
                      tor as well as in each denominator can be roughly equal). It is better to compute the
                      parameters from
                                                                   yi (xi − x)
                                                                            ¯
                                                         b=                                a = y¯ − xb
                                                                                                    ¯                                        (3.19)
                                                                   xi (xi − x)
                                                                            ¯
                      which are equivalent to Eqs. (3.18), but much less affected by rounding off.


                      Fitting Linear Forms
                      Consider the least-squares fit of the linear form
                                                                                                              m
                                         f (x) = a 0 f0 (x) + a 1 f1 (x) + . . . + a m fm (x) =                      a j f j (x)             (3.20)
                                                                                                              j =0
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                                                      December 16, 2009      15:4




           126      Interpolation and Curve Fitting

                    where each f j (x) is a predetermined function of x, called a basis function. Substitu-
                    tion in Eq. (3.13) yields
                                                         ⎡                 ⎤2
                                                                     n                    m
                                                            S=            ⎣ yi −              a j f j (xi )⎦
                                                                   i=0                j =0

                    Thus, Eqs. (3.14) are
                                         ⎧              ⎡                                 ⎤          ⎫
                               ∂S        ⎨        n                m                                 ⎬
                                    = −2                ⎣ yi −           a j f j (xi )⎦ fk (xi )          = 0, k = 0, 1, . . . , m
                               ∂a k      ⎩                                                           ⎭
                                                  i=0             j =0

                    Dropping the constant (−2) and interchanging the order of summation, we get
                                    m       n                                         n
                                                   f j (xi )fk (xi ) a j =                 fk (xi )yi , k = 0, 1, . . . , m
                                    j =0    i=0                                    i=0

                    In matrix notation, these equations are

                                                                               Aa = b                                                    (3.21a)

                    where
                                                            n                                             n
                                              A kj =              f j (xi )fk (xi )            bk =            fk (xi )yi                (3.21b)
                                                            i=0                                        i=0

                    Equations (3.21a), known as the normal equations of the least-squares fit, can be
                    solved with the methods discussed in Chapter 2. Note that the coefficient matrix is
                    symmetric, that is, A kj = A j k .

                    Polynomial Fit
                    A commonly used linear form is a polynomial. If the degree of the polynomial is m,
                    we have f (x) = m         j
                                    j =0 a j x . Here the basis functions are

                                                         f j (x) = x j             ( j = 0, 1, . . . , m)                                 (3.22)

                    so that Eqs. (3.21b) become
                                                                    n                                 n
                                                                            j +k
                                                         A kj =           xi                  bk =         xik yi
                                                                   i=0                               i=0

                    or
                              ⎡                                                                    ⎤                    ⎡     ⎤
                               n                   xi             xi2          ...           xim                        yi
                           ⎢ x                     xi2            xi3          ...           xim+1 ⎥               ⎢ xy ⎥
                           ⎢     i                                                                 ⎥               ⎢     i i ⎥
                         A=⎢
                           ⎢..               ..             ..                 ..         ..       ⎥
                                                                                                   ⎥             b=⎢
                                                                                                                   ⎢ ..
                                                                                                                              ⎥
                                                                                                                              ⎥           (3.23)
                           ⎣.                 .              .                    .        .       ⎦               ⎣.         ⎦
                               xim−1               xim            xim+1        ...           xi2m                        m
                                                                                                                        xi yi
                                                   n
                    where     stands for           i=0 .
                                                The normal equations become progressively ill condi-
                    tioned with increasing m. Fortunately, this is of little practical consequence, because
                    only low-order polynomials are useful in curve fitting. Polynomials of high order are
                    not recommended, because they tend to reproduce the noise inherent in the data.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03    978 0 521 19132 6                                December 16, 2009    15:4




               127     3.4 Least-Squares Fit

                         polyFit

                      The function polyFit in this module sets up and solves the normal equations
                      for the coefficients of a polynomial of degree m. It returns the coefficients of the
                      polynomial. To facilitate computations, the terms n,      xi ,  xi2 , . . . , xi2m that
                      make up the coefficient matrix in Eq. (3.23) are first stored in the vector s and
                      then inserted into A. The normal equations are then solved by Gauss elimina-
                      tion with pivoting. Following the solution, the standard deviation σ can be com-
                      puted with the function stdDev. The polynomial evaluation in stdDev is carried
                      out by the embedded function evalPoly – see Section 4.7 for an explanation of the
                      algorithm.


                      ## module polyFit
                      ’’’ c = polyFit(xData,yData,m).
                            Returns coefficients of the polynomial
                            p(x) = c[0] + c[1]x + c[2]xˆ2 +...+ c[m]xˆm
                            that fits the specified data in the least
                            squares sense.


                            sigma = stdDev(c,xData,yData).
                            Computes the std. deviation between p(x)
                            and the data.
                      ’’’
                      from numpy import zeros
                      from math import sqrt
                      from gaussPivot import *


                      def polyFit(xData,yData,m):
                            a = zeros((m+1,m+1))
                            b = zeros(m+1)
                            s = zeros(2*m+1)
                            for i in range(len(xData)):
                                  temp = yData[i]
                                  for j in range(m+1):
                                      b[j] = b[j] + temp
                                      temp = temp*xData[i]
                                  temp = 1.0
                                  for j in range(2*m+1):
                                      s[j] = s[j] + temp
                                      temp = temp*xData[i]
                            for i in range(m+1):
                                  for j in range(m+1):
                                      a[i,j] = s[i+j]
                            return gaussPivot(a,b)
P1: PHB

CUUS884-Kiusalaas     CUUS884-03     978 0 521 19132 6                                                    December 16, 2009      15:4




           128      Interpolation and Curve Fitting

                    def stdDev(c,xData,yData):


                         def evalPoly(c,x):
                              m = len(c) - 1
                              p = c[m]
                              for j in range(m):
                                    p = p*x + c[m-j-1]
                              return p


                         n = len(xData) - 1
                         m = len(c) - 1
                         sigma = 0.0
                         for i in range(n+1):
                              p = evalPoly(c,xData[i])
                              sigma = sigma + (yData[i] - p)**2
                         sigma = sqrt(sigma/(n - m))
                         return sigma



                    Weighting of Data
                    There are occasions when our confidence in the accuracy of data varies from point to
                    point. For example, the instrument taking the measurements may be more sensitive
                    in a certain range of data. Sometimes the data represent the results of several exper-
                    iments, each carried out under different conditions. Under these circumstances, we
                    may want to assign a confidence factor, or weight, to each data point and minimize
                    the sum of the squares of the weighted residuals ri = Wi yi − f (xi ) , where Wi are the
                    weights. Hence, the function to be minimized is
                                                                             n
                                                                                                      2
                                             S(a 0 , a 1 , . . . , a m ) =         Wi2 yi − f (xi )                     (3.24)
                                                                             i=0

                    This procedure forces the fitting function f (x) closer to the data points that have
                    higher weights.

                    Weighted Linear Regression
                    If the fitting function is the straight line f (x) = a + bx, Eq. (3.24) becomes
                                                                    n
                                                   S(a, b) =             Wi2 (yi − a − bxi )2                           (3.25)
                                                                   i=0

                    The conditions for minimizing S are
                                                               n
                                               ∂S
                                                  = −2              Wi2 (yi − a − bxi ) = 0
                                               ∂a
                                                              i=0
                                                               n
                                               ∂S
                                                  = −2              Wi2 (yi − a − bxi )xi = 0
                                               ∂b
                                                              i=0
P1: PHB

CUUS884-Kiusalaas    CUUS884-03     978 0 521 19132 6                                                         December 16, 2009      15:4




               129     3.4 Least-Squares Fit

                      or
                                                           n                n               n
                                                     a          Wi2 + b         Wi2 xi =          Wi2 yi                          (3.26a)
                                                         i=0              i=0               i=0


                                                     n                    n                  n
                                                 a         Wi2 xi + b           Wi2 xi2 =         Wi2 xi yi                       (3.26b)
                                                     i=0                  i=0               i=0

                      Dividing Eq. (3.26a) by        Wi2 and introducing the weighted averages

                                                                   Wi2 xi                    Wi2 yi
                                                         xˆ =                     yˆ =                                             (3.27)
                                                                    Wi2                       Wi2
                      we obtain

                                                                      a = yˆ − b xˆ                                               (3.28a)

                      Substituting into Eq. (3.26b) and solving for b yields, after some algebra,
                                                                            Wi2 yi (xi − x)
                                                                                         ˆ
                                                                 b=                                                               (3.28b)
                                                                            Wi xi (xi − x)
                                                                              2
                                                                                         ˆ
                      Note that Eqs. (3.28) are quite similar to Eqs. (3.19) for unweighted data.

                      Fitting Exponential Functions
                      A special application of weighted linear regression arises in fitting various exponen-
                      tial functions to data. Consider as an example the fitting function

                                                                      f (x) = aebx

                      Normally, the least-squares fit would lead to equations that are nonlinear in a and b.
                      But if we fit ln y rather than y, the problem is transformed to linear regression: fit the
                      function

                                                           F (x) = ln f (x) = ln a + bx

                      to the data points (xi , ln yi ), i = 0, 1, . . . , n. This simplification comes at a price: the
                      least-squares fit to the logarithm of the data is not quite the same as the least-squares
                      fit to the original data. The residuals of the logarithmic fit are

                                                Ri = ln yi − F (xi ) = ln yi − ln a + bxi                                         (3.29a)

                      whereas the residuals used in fitting the original data are

                                                           ri = yi − f (xi ) = yi − aebxi                                         (3.29b)

                          This discrepancy can be largely eliminated by weighting the logarithmic fit. From
                      Eq. (3.29b) we obtain ln(ri − yi ) = ln(aebxi ) = ln a + bxi , so that Eq. (3.29a) can be
                      written as
                                                                                                      ri
                                                 Ri = ln yi − ln(ri − yi ) = ln 1 −
                                                                                                      yi
P1: PHB

CUUS884-Kiusalaas    CUUS884-03       978 0 521 19132 6                                                             December 16, 2009      15:4




           130      Interpolation and Curve Fitting

                    If the residuals ri are sufficiently small (ri << yi ), we can use the approximation
                    ln(1 − ri /yi ) ≈ ri /yi , so that

                                                                     Ri ≈ ri /yi

                    We can now see that by minimizing           Ri2 , we have inadvertently introduced the
                    weights 1/yi . This effect can be negated if we apply the weights Wi = yi when fitting
                    F (x) to (ln yi , xi ). That is, minimizing
                                                                           n
                                                                    S=           yi2 Ri2                                          (3.30)
                                                                           i=0

                    is a good approximation to minimizing ri2 .
                         Other examples that also benefit from the weights Wi = yi are given in Table 3.4.

                                   f (x)                    F (x)                      Data to be fitted by F (x)
                                  axe bx
                                              ln f (x)/x = ln a + bx                             xi , ln(yi /xi )
                                   ax b       ln f (x) = ln a + b ln(x)                           ln xi , ln yi

                                Table 3.4


                    EXAMPLE 3.10
                    Fit a straight line to the data shown and compute the standard deviation.

                                                      x     0.0      1.0         2.0       2.5    3.0
                                                      y     2.9      3.7         4.1       4.4    5.0

                    Solution The averages of the data are


                                                  1              0.0 + 1.0 + 2.0 + 2.5 + 3.0
                                           x¯ =           xi =                               = 1.7
                                                  5                           5

                                                  1              2.9 + 3.7 + 4.1 + 4.4 + 5.0
                                          y¯ =         yi =                                  = 4. 02
                                                  5                           5
                    The intercept a and slope b of the interpolant can now be determined from Eq. (3.19):
                                             yi (xi − x)
                                                      ¯
                                  b=
                                             xi (xi − x)
                                                      ¯
                                           2.9(−1.7) + 3.7(−0.7) + 4.1(0.3) + 4.4(0.8) + 5.0(1.3)
                                     =
                                           0.0(−1.7) + 1.0(−0.7) + 2.0(0.3) + 2.5(0.8) + 3.0(1.3)
                                           3. 73
                                     =           = 0. 6431
                                            5. 8

                                                 a = y¯ − xb
                                                          ¯ = 4.02 − 1.7(0.6431) = 2. 927

                    Therefore, the regression line is f (x) = 2.927 + 0.6431x, which is shown in the figure
                    together with the data points.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                             December 16, 2009     15:4




               131     3.4 Least-Squares Fit


                                   5.00

                                   4.50


                             y     4.00

                                   3.50


                                   3.00

                                   2.50
                                      0.00            0.50         1.00       1.50        2.00          2.50      3.00
                                                                                x
                          We start the evaluation of the standard deviation by computing the residuals:

                                              x            0.000     1.000      2.000      2.500        3.000
                                              y            2.900     3.700      4.100      4.400        5.000
                                            f (x)          2.927     3.570      4.213      4.535        4.856
                                          y − f (x)       −0.027     0.130     −0.113     −0.135        0.144

                      The sum of the squares of the residuals is
                                                          2
                             S=            yi − f (xi )

                                  = (−0.027)2 + (0.130)2 + (−0.113)2 + (−0.135)2 + (0.144)2 = 0.06936

                      so that the standard deviation in Eq. (3.15) becomes

                                                                S            0.06936
                                                       σ =         =                 = 0.1520
                                                               5−2              3

                      EXAMPLE 3.11
                      Determine the parameters a and b so that f (x) = aebx fits the following data in the
                      least-squares sense.

                                               x      1.2      2.8     4.3      5.4       6.8       7.9
                                               y      7.5     16.1    38.9     67.0     146.6     266.2

                      Use two different methods: (1) fit ln yi and (2) fit ln yi with weights Wi = yi . Compute
                      the standard deviation in each case.

                      Solution of Part (1) The problem is to fit the function ln(aebx ) = ln a + bx to the data

                                         x             1.2       2.8       4.3         5.4       6.8       7.9
                                      z = ln y        2.015     2.779     3.661       4.205     4.988     5.584
P1: PHB

CUUS884-Kiusalaas     CUUS884-03        978 0 521 19132 6                                                    December 16, 2009   15:4




           132      Interpolation and Curve Fitting

                    We are now dealing with linear regression, where the parameters to be found are
                    A = ln a and b. Following the steps in Example 3.8, we get (skipping some of the arith-
                    metic details)
                                                    1                                    1
                                             x¯ =          xi = 4. 733          z¯ =           zi = 3. 872
                                                    6                                    6

                                            zi (xi − x)
                                                     ¯    16.716
                                 b=                     =        = 0. 5366                    A = z¯ − xb
                                                                                                       ¯ = 1. 3323
                                            xi (xi − x)
                                                     ¯    31.153

                    Therefore, a = eA = 3. 790 and the fitting function becomes f (x) = 3.790e0.5366 . The
                    plots of f (x) and the data points are shown in the figure.

                                 300

                                 250

                                 200
                             y




                                 150

                                 100

                                   50

                                    0
                                        1           2           3          4              5         6         7        8
                                                                                 x

                        Here is the computation of standard deviation:

                                     x              1.20     2.80         4.30        5.40           6.80      7.90
                                     y              7.50    16.10        38.90       67.00         146.60    266.20
                                   f (x)            7.21    17.02        38.07       68.69         145.60    262.72
                                 y − f (x)          0.29    −0.92         0.83       −1.69           1.00      3.48

                                                                                     2
                                                           S=         yi − f (xi )       = 17.59


                                                                         S
                                                                σ =         = 2.10
                                                                        6−2

                        As pointed out before, this is an approximate solution of the stated problem, be-
                    cause we did not fit yi , but ln yi . Judging by the plot, the fit seems to be quite good.

                    Solution of Part (2) We again fit ln(aebx ) = ln a + bx to z = ln y, but this time the
                    weights Wi = yi are used. From Eqs. (3.27) the weighted averages of the data are
P1: PHB

CUUS884-Kiusalaas    CUUS884-03     978 0 521 19132 6                                               December 16, 2009   15:4




               133     3.4 Least-Squares Fit

                      (recall that we fit z = ln y)
                                                             yi2 xi   737.5 × 103
                                                  xˆ =              =             = 7.474
                                                              yi 2    98.67 × 103

                                                             yi2 zi   528.2 × 103
                                                  zˆ =              =             = 5.353
                                                              yi2     98.67 × 103
                      and Eqs. (3.28) yield for the parameters
                                                        yi2 zi (xi − x)
                                                                     ˆ    35.39 × 103
                                             b=                         =             = 0.5440
                                                        yi xi (xi − x)
                                                          2
                                                                     ˆ    65.05 × 103

                                            ln a = zˆ − b xˆ = 5.353 − 0.5440(7.474) = 1. 287

                      Therefore,

                                                          a = eln a = e1.287 = 3. 622

                      so that the fitting function is f (x) = 3.622e0.5440x . As expected, this result is somewhat
                      different from that obtained in Part (1).
                           The computations of the residuals and the standard deviation are as follows:

                                       x         1.20       2.80       4.30        5.40           6.80      7.90
                                       y         7.50      16.10      38.90       67.00         146.60    266.20
                                     f (x)       6.96      16.61      37.56       68.33         146.33    266.20
                                   y − f (x)     0.54      −0.51       1.34       −1.33          0.267      0.00

                                                                                  2
                                                        S=         yi − f (xi )       = 4.186


                                                                      S
                                                             σ =         = 1.023
                                                                     6−2
                      Observe that the residuals and standard deviation are smaller than those in Part (1),
                      indicating a better fit, as expected.
                           It can be shown that fitting yi directly (which involves the solution of a transcen-
                      dental equation) results in f (x) = 3.614e0.5442 . The corresponding standard deviation
                      is σ = 1.022, which is very close to the result in Part (2).

                      EXAMPLE 3.12
                      Write a program that fits a polynomial of arbitrary degree m to the data points shown
                      in the table. Use the program to determine m that best fits these data in the least-
                      squares sense.

                                        x      −0.04      0.93       1.95          2.90       3.83       5.00
                                        y      −8.66     −6.44      −4.36         −3.27      −0.88       0.87
                                        x       5.98       7.05       8.21            9.08   10.09
                                        y       3.31       4.63       6.19            7.40    8.85
P1: PHB

CUUS884-Kiusalaas    CUUS884-03     978 0 521 19132 6                                December 16, 2009   15:4




           134      Interpolation and Curve Fitting

                    Solution The program shown below prompts for m. Execution is terminated by en-
                    tering an invalid character (e.g., the “return” character).
                    #!/usr/bin/python
                    ## example3_12
                    from numpy import array
                    from polyFit import *


                    xData = array([-0.04,0.93,1.95,2.90,3.83,5.0,                    \
                                          5.98,7.05,8.21,9.08,10.09])
                    yData = array([-8.66,-6.44,-4.36,-3.27,-0.88,0.87, \
                                          3.31,4.63,6.19,7.4,8.85])
                    while 1:
                        try:
                               m = eval(raw_input(’’\nDegree of polynomial ==> ’’))
                               coeff = polyFit(xData,yData,m)
                               print ’’Coefficients are:\n’’,coeff
                               print ’’Std. deviation =’’,stdDev(coeff,xData,yData)
                        except SyntaxError: break
                    raw_input(’’Finished. Press return to exit’’)

                       The results are:
                    Degree of polynomial ==> 1
                    Coefficients are:
                    [-7.94533287     1.72860425]
                    Std. deviation = 0.511278836737


                    Degree of polynomial ==> 2
                    Coefficients are:
                    [-8.57005662     2.15121691 -0.04197119]
                    Std. deviation = 0.310992072855


                    Degree of polynomial ==> 3
                    Coefficients are:
                    [-8.46603423e+00        1.98104441e+00       2.88447008e-03 -2.98524686e-03]
                    Std. deviation = 0.319481791568


                    Degree of polynomial ==> 4
                    Coefficients are:
                    [ -8.45673473e+00         1.94596071e+00        2.06138060e-02
                            -5.82026909e-03             1.41151619e-04]
                    Std. deviation = 0.344858410479


                    Degree of polynomial ==>
                    Finished. Press return to exit
P1: PHB

CUUS884-Kiusalaas    CUUS884-03      978 0 521 19132 6                                          December 16, 2009   15:4




               135     3.4 Least-Squares Fit

                            Because the quadratic f (x) = −8.5700 + 2.1512x − 0.041971x 2 produces the
                      smallest standard deviation, it can be considered as the “best” fit to the data. But
                      be warned – the standard deviation is not a reliable measure of the goodness-of-fit.
                      It is always a good idea to plot the data points and f (x) before final determination is
                      made. The plot of our data indicates that the quadratic (solid line) is indeed a reason-
                      able choice for the fitting function.

                                  10.0


                                    5.0
                            y




                                    0.0


                                   -5.0


                                  -10.0
                                       -2.0      0.0       2.0     4.0       6.0          8.0       10.0     12.0
                                                                         x

                      PROBLEM SET 3.2

                      Instructions Plot the data points and the fitting function whenever appropriate.

                       1. Show that the straight line obtained by least-squares fit of unweighted data al-
                          ways passes through the point (x,¯ y).
                                                               ¯
                       2. Use linear regression to find the line that fits the data

                                                  x      −1.0    −0.5      0        0.5     1.0
                                                  y      −1.00   −0.55   0.00      0.45     1.00

                          and determine the standard deviation.
                       3. Three tensile tests were carried out on an aluminum bar. In each test the strain
                          was measured at the same values of stress. The results were
                                                Stress (MPa)      34.5   69.0      103.5        138.0
                                               Strain (Test 1)    0.46   0.95       1.48         1.93
                                               Strain (Test 2)    0.34   1.02       1.51         2.09
                                               Strain (Test 3)    0.73   1.10       1.62         2.12

                          where the units of strain are mm/m. Use linear regression to estimate the mod-
                          ulus of elasticity of the bar (modulus of elasticity = stress/strain).
                       4. Solve Problem 3 assuming that the third test was performed on an inferior ma-
                          chine, so that its results carry only half the weight of the other two tests.
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                            December 16, 2009   15:4




           136      Interpolation and Curve Fitting

                    5.     Fit a straight line to the following data and compute the standard deviation.

                                           x         0        0.5       1       1.5         2        2.5
                                           y       3.076     2.810    2.588    2.297      1.981     1.912
                                           x         3        3.5       4       4.5         5
                                           y       1.653     1.478    1.399    1.018      0.794

                    6.     The table displays the mass M and average fuel consumption φ of motor vehi-
                         cles manufactured by Ford and Honda in 2008. Fit a straight line φ = a + b M to
                         the data and compute the standard deviation.

                                                   Model                 M (kg)     φ (km/liter)
                                                   Focus                 1198              11.90
                                                   Crown Victoria        1715               6.80
                                                   Expedition            2530               5.53
                                                   Explorer              2014               6.38
                                                   F-150                 2136               5.53
                                                   Fusion                1492               8.50
                                                   Taurus                1652               7.65
                                                   Fit                   1168              13.60
                                                   Accord                1492               9.78
                                                   CR-V                  1602               8.93
                                                   Civic                 1192              11.90
                                                   Ridgeline             2045               6.38

                    7.     The relative density ρ of air was measured at various altitudes h. The results
                         were:
                                 h (km)        0   1.525       3.050      4.575       6.10        7.625       9.150
                                    ρ          1   0.8617     0.7385     0.6292      0.5328       0.4481     0.3741

                       Use a quadratic least-squares fit to determine the relative air density at h =
                       10.5 km. (This problem was solved by interpolation in Problem 20, Problem
                       Set 3.1.)
                    8.   The kinematic viscosity µk of water varies with temperature T as shown in the
                       table. Determine the cubic that best fits the data, and use it to compute µk at
                       T = 10◦ , 30◦ , 60◦ , and 90◦ C. (This problem was solved in Problem 19, Problem
                       Set 3.1, by interpolation.)

                                    T (◦ C)             0      21.1     37.8       54.4     71.1      87.8      100
                                µk (10−3 m2 /s)       1.79     1.13    0.696      0.519    0.338     0.321     0.296

                    9.     Fit a straight line and a quadratic to the data

                            x       1.0          2.5         3.5        4.0      1.1       1.8        2.2        3.7
                            y      6.008       15.722      27.130     33.772    5.257     9.549     11.098     28.828

                         Which is a better fit?
P1: PHB

CUUS884-Kiusalaas    CUUS884-03       978 0 521 19132 6                                          December 16, 2009     15:4




               137     3.4 Least-Squares Fit

                      10.     The table displays thermal efficiencies of some early steam engines.4 Deter-
                            mine the polynomial that provides the best fit to the data and use it to predict
                            the thermal efficiency in the year 2000.

                                                   Year      Efficiency (%)             Type
                                                  1718              0.5         Newcomen
                                                  1767              0.8         Smeaton
                                                  1774              1.4         Smeaton
                                                  1775              2.7         Watt
                                                  1792              4.5         Watt
                                                  1816              7.5         Woolf compound
                                                  1828              12.0        Improved Cornish
                                                  1834              17.0        Improved Cornish
                                                  1878              17.2        Corliss compound
                                                  1906              23.0        Triple expansion

                      11.     The table shows the variation of relative thermal conductivity k of sodium with
                            temperature T . Find the quadratic that fits the data in the least-squares sense.

                                                 T (◦ C)      79      190      357       524       690
                                                   k         1.00    0.932    0.839     0.759     0.693

                      12. Let f (x) = ax b be the least-squares fit of the data (xi , yi ), i = 0, 1, . . . , n, and let
                          F (x) = ln a + b ln x be the least-squares fit of (ln xi , ln yi ) – see Table 3.3. Prove
                          that Ri ≈ ri /yi , where the residuals are ri = yi − f (xi ) and Ri = ln yi − F (xi ). As-
                          sume that ri << yi .
                      13. Determine a and b for which f (x) = a sin(π x/2) + b cos(π x/2) fits the following
                          data in the least-squares sense.

                                         x     −0.5         −0.19       0.02         0.20      0.35        0.50
                                         y    −3.558        −2.874     −1.995       −1.040    −0.068      0.677

                      14. Determine a and b so that f (x) = ax b fits the following data in the least-squares
                          sense.

                                                       x      0.5     1.0    1.5      2.0      2.5
                                                       y     0.49    1.60    3.36    6.44     10.16

                      15. Fit the function f (x) = axebx to the data and compute the standard deviation.

                                                   x        0.5      1.0      1.5      2.0       2.5
                                                   y       0.541    0.398    0.232    0.106     0.052


                      4   Source: C. Singer, E. J. Holmyard, A. R. Hall, and T. H. Williams, A History of Technology (Oxford
                          University Press, 1958).
P1: PHB

CUUS884-Kiusalaas    CUUS884-03        978 0 521 19132 6                                              December 16, 2009   15:4




           138      Interpolation and Curve Fitting

                    16.     The intensity of radiation of a radioactive substance was measured at half-year
                          intervals. The results were:
                                      t (years)     0        0.5           1           1.5      2        2.5
                                          γ       1.000     0.994        0.990        0.985   0.979     0.977
                                      t (years)     3        3.5           4           4.5      5        5.5
                                          γ       0.972     0.969        0.967        0.960   0.956     0.952

                        where γ is the relative intensity of radiation. Knowing that radioactivity decays
                        exponentially with time, γ (t ) = ae−bt , estimate the radioactive half-life of the
                        substance.
                    17. Linear regression can be extended to data that depend on two or more variables
                        (called multiple linear regression). If the dependent variable is z and indepen-
                        dent variables are x and y, the data to be fitted has the form

                                                                x1        y1    z1
                                                                x2        y2    z2
                                                                x3        y3    z3
                                                               ..        ..     ..
                                                                .         .      .
                                                                xn        yn     zn

                          Instead of a straight line, the fitting function now represents a plane:

                                                           f (x, y) = a + bx + cy

                          Show that the normal equations for the coefficients are
                                            ⎡                       ⎤⎡ ⎤ ⎡             ⎤
                                                n      xi      yi       a         zi
                                            ⎢                       ⎥⎢ ⎥ ⎢             ⎥
                                            ⎣ xi       xi2    xi yi ⎦ ⎣ b ⎦ = ⎣ xi zi ⎦
                                                 yi   xi yi    yi2      c        yi zi
                    18. Use the multiple linear regression explained in Problem 17 to determine the
                        function

                                                           f (x, y) = a + bx + cy

                          that fits the data
                                                              x      y          z
                                                              0      0         1.42
                                                              0      1         1.85
                                                              1      0         0.78
                                                              2      0         0.18
                                                              2      1         0.60
                                                              2      2         1.05
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                                           December 16, 2009   15:4




              4       Roots of Equations




                                   Find the solutions of f (x) = 0, where the function f is given




              4.1     Introduction

                      A common problem encountered in engineering analysis is this: given a function
                      f (x), determine the values of x for which f (x) = 0. The solutions (values of x) are
                      known as the roots of the equation f (x) = 0, or the zeroes of the function f (x).
                           Before proceeding further, it might be helpful to review the concept of a function.
                      The equation

                                                                 y = f (x)

                      contains three elements: an input value x, an output value y, and the rule f for com-
                      puting y. The function is said to be given if the rule f is specified. In numerical com-
                      puting the rule is invariably a computer algorithm. It may be a function statement,
                      such as

                                                       f (x) = cosh(x) cos(x) − 1

                      or a complex procedure containing hundreds or thousands of lines of code. As long
                      as the algorithm produces an output y for each input x, it qualifies as a function.
                           The roots of equations may be real or complex. The complex roots are seldom
                      computed, because they rarely have physical significance. An exception is the poly-
                      nomial equation

                                                  a 0 + a 1 x + a 1 x 2 + . . . + an x n = 0

                      where the complex roots may be meaningful (as in the analysis of damped vibra-
                      tions, for example). For the time being, we concentrate on finding the real roots of
                      equations. Complex zeroes of polynomials are treated near the end of this chapter.
                          In general, an equation may have any number of (real) roots, or no roots at all.
                      For example,

                                                              sin x − x = 0

               139
P1: PHB

CUUS884-Kiusalaas     CUUS884-04      978 0 521 19132 6                                     December 16, 2009        15:4




           140      Roots of Equations

                    has a single root, namely, x = 0, whereas

                                                           tan x − x = 0

                    has an infinite number of roots (x = 0, ±4.493, ±7.725, . . .).
                          All methods of finding roots are iterative procedures that require a starting point,
                    that is, an estimate of the root. This estimate can be crucial; a bad starting value may
                    fail to converge, or it may converge to the “wrong” root (a root different from the one
                    sought). There is no universal recipe for estimating the value of a root. If the equa-
                    tion is associated with a physical problem, then the context of the problem (physical
                    insight) might suggest the approximate location of the root. Otherwise, a systematic
                    numerical search for the roots can be carried out. One such search method is de-
                    scribed in the next section. Plotting the function is another means of locating the
                    roots, but it is a visual procedure that cannot be programmed.
                          It is highly advisable to go a step further and bracket the root (determine its lower
                    and upper bounds) before passing the problem to a root-finding algorithm. Prior
                    bracketing is, in fact, mandatory in the methods described in this chapter.



          4.2       Incremental Search Method

                    The approximate locations of the roots are best determined by plotting the function.
                    Often a very rough plot, based on a few points, is sufficient to give us reasonable start-
                    ing values. Another useful tool for detecting and bracketing roots is the incremental
                    search method. It can also be adapted for computing roots, but the effort would not
                    be worthwhile, because other methods described in this chapter are more efficient
                    for that.
                         The basic idea behind the incremental search method is simple: If f (x1 ) and f (x2 )
                    have opposite signs, then there is at least one root in the interval (x1 , x2 ). If the inter-
                    val is small enough, it is likely to contain a single root. Thus, the zeroes of f (x) can be
                    detected by evaluating the function at intervals x and looking for change in sign.
                         There are a couple of potential problems with the incremental search method:

                     • It is possible to miss two closely spaced roots if the search increment x is larger
                       than the spacing of the roots.
                     • A double root (two roots that coincide) will not be detected.
                     • Certain singularities (poles) of f (x) can be mistaken for roots. For example,
                       f (x) = tan x changes sign at x = ± 12 nπ , n = 1, 3, 5, . . ., as shown in Fig. 4.1. How-
                       ever, these locations are not true zeroes, since the function does not cross the
                       x-axis.



                      rootsearch

                    This function searches for a zero of the user-supplied function f(x) in the interval
                    (a,b) in increments of dx. It returns the bounds (x1,x2)of the root if the search
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                                    December 16, 2009       15:4




               141     4.2 Incremental Search Method


                             10.0


                              5.0


                              0.0


                             -5.0


                            -10.0
                                    0              1          2           3           4          5             6
                                                                              x
                            Figure 4.1. Plot of tan x.



                      was successful; x1 = x2 = None indicates that no roots were detected. After the first
                      root (the root closest to a) has been detected, rootsearch can be called again with
                      a replaced by x2 in order to find the next root. This can be repeated as long as root-
                      search detects a root.



                      ## module rootsearch
                      ’’’ x1,x2 = rootsearch(f,a,b,dx).
                            Searches the interval (a,b) in increments dx for
                            the bounds (x1,x2) of the smallest root of f(x).
                            Returns x1 = x2 = None if no roots were detected.
                      ’’’
                      def rootsearch(f,a,b,dx):
                            x1 = a; f1 = f(a)
                            x2 = a + dx; f2 = f(x2)
                            while f1*f2 > 0.0:
                                  if x1       >=       b: return None,None
                                  x1 = x2; f1 = f2
                                  x2 = x1 + dx; f2 = f(x2)
                            else:
                                  return x1,x2



                      EXAMPLE 4.1
                      Use incremental search with           x = 0.2 to bracket the smallest positive zero of f (x) =
                      x 3 − 10x 2 + 5.
P1: PHB

CUUS884-Kiusalaas     CUUS884-04       978 0 521 19132 6                                        December 16, 2009          15:4




           142      Roots of Equations

                    Solution We evaluate f (x) at intervals x = 0.2, staring at x = 0, until the function
                    changes its sign (the value of the function is of no interest to us, only its sign is rele-
                    vant). This procedure yields the following results:

                                                               x       f (x)
                                                              0.0      5.000
                                                              0.2      4.608
                                                              0.4      3.464
                                                              0.6      1.616
                                                              0.8     −0.888

                    From the sign change of the function, we conclude that the smallest positive zero lies
                    between x = 0.6 and x = 0.8.


          4.3       Method of Bisection

                    After a root of f (x) = 0 has been bracketed in the interval (x1 , x2 ), several methods
                    can be used to close in on it. The method of bisection accomplishes this by succes-
                    sively halving the interval until it becomes sufficiently small. This technique is also
                    known as the interval halving method. Bisection is not the fastest method available
                    for computing roots, but it is the most reliable. Once a root has been bracketed, bi-
                    section will always close in on it.
                          The method of bisection uses the same principle as incremental search: If there
                    is a root in the interval (x1 , x2 ), then f (x1 ) · f (x2 ) < 0. In order to halve the interval, we
                    compute f (x3 ), where x3 = 12 (x1 + x2 ) is the midpoint of the interval. If f (x2 ) · f (x3 ) <
                    0, then the root must be in (x2 , x3 ), and we record this by replacing the original
                    bound x1 by x3 . Otherwise, the root lies in (x1 , x3 ), in which case x2 is replaced by
                    x3 . In either case, the new interval (x1 , x2 ) is half the size of the original interval.
                    The bisection is repeated until the interval has been reduced to a small value ε,
                    so that

                                                              |x2 − x1 | ≤ ε

                        It is easy to compute the number of bisections required to reach a prescribed ε.
                    The original interval x is reduced to x/2 after one bisection, to x/22 after two
                    bisections, and after n bisections it is x/2n . Setting x/2n = ε and solving for n,
                    we get
                                                                    ln (| x| /ε)
                                                            n=                                                    (4.1)
                                                                         ln 2


                       bisection

                    This function uses the method of bisection to compute the root of f(x) = 0 that is
                    known to lie in the interval (x1,x2). The number of bisections n required to reduce
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                                December 16, 2009    15:4




               143     4.3 Method of Bisection

                      the interval to tol is computed from Eq. (4.1). By setting switch = 1, we force the
                      routine to check whether the magnitude of f(x) decreases with each interval halv-
                      ing. If it does not, something may be wrong (probably the “root” is not a root at all,
                      but a pole) and root = None is returned. Because this feature is not always desir-
                      able, the default value is switch = 0. The function error.err, which we use to
                      terminate a program, is listed in Section 1.7.



                      ## module bisection
                      ’’’ root = bisection(f,x1,x2,switch=0,tol=1.0e-9).
                            Finds a root of f(x) = 0 by bisection.
                            The root must be bracketed in (x1,x2).
                            Setting switch = 1 returns root = None if
                            f(x) increases upon bisection.
                      ’’’
                      from math import log,ceil
                      import error


                      def bisection(f,x1,x2,switch=1,tol=1.0e-9):
                            f1 = f(x1)
                            if f1 == 0.0: return x1
                            f2 = f(x2)
                            if f2 == 0.0: return x2
                            if f1*f2 > 0.0: error.err(’Root is not bracketed’)
                            n = ceil(log(abs(x2 - x1)/tol)/log(2.0))
                            for i in range(n):
                                  x3 = 0.5*(x1 + x2); f3 = f(x3)
                                  if (switch == 1) and (abs(f3) > abs(f1)) \
                                                        and (abs(f3) > abs(f2)):
                                      return None
                                  if f3 == 0.0: return x3
                                  if f2*f3 < 0.0: x1 = x3; f1 = f3
                                  else:                x2 = x3; f2 = f3
                            return (x1 + x2)/2.0




                      EXAMPLE 4.2
                      Use bisection to find the root of f (x) = x 3 − 10x 2 + 5 = 0 that lies in the interval
                      (0.6, 0.8).



                      Solution The best way to implement the method is to use the following table. Note
                      that the interval to be bisected is determined by the sign of f (x), not its magnitude.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                                  December 16, 2009     15:4




           144      Roots of Equations


                                               x                    f (x)            Interval
                               0.6                                  1.616               −
                               0.8                                 −0.888           (0.6, 0.8)
                               (0.6 + 0.8)/2 = 0.7                  0.443           (0.7, 0.8)
                               (0.8 + 0.7)/2 = 0.75                −0. 203         (0.7, 0.75)
                               (0.7 + 0.75)/2 = 0.725               0.125         (0.725, 0.75)
                               (0.75 + 0.725)/2 = 0.7375           −0.038       (0.725, 0.7375)
                               (0.725 + 0.7375)/2 = 0.73125         0.044      (0.7375, 0.73125)
                               (0.7375 + 0.73125)/2 = 0.73438       0.003      (0.7375, 0.73438)
                               (0.7375 + 0.73438)/2 = 0.73594      −0.017     (0.73438, 0.73594)
                               (0.73438 + 0.73594)/2 = 0.73516     −0.007     (0.73438, 0.73516)
                               (0.73438 + 0.73516)/2 = 0.73477     −0.002     (0.73438, 0.73477)
                               (0.73438 + 0.73477)/2 = 0.73458      0.000               −
                    The final result x = 0.7346 is correct within four decimal places.

                    EXAMPLE 4.3
                    Find all the zeroes of f (x) = x − tan x in the interval (0, 20) by the method of bisec-
                    tion. Utilize the functions rootsearch and bisection.
                    Solution Note that tan x is singular and changes sign at x = π/2, 3π /2, . . .. To
                    prevent bisection from mistaking these point for roots, we set switch = 1. The
                    closeness of roots to the singularities is another potential problem that can be
                    alleviated by using small x in rootsearch. Choosing x = 0.01, we arrive at the
                    following program:

                    #!/usr/bin/python
                    ## example4_3
                    from math import tan
                    from rootsearch import *
                    from bisection import *


                    def f(x): return x - tan(x)


                    a,b,dx = (0.0, 20.0, 0.01)
                    print "The roots are:"
                    while 1:
                         x1,x2 = rootsearch(f,a,b,dx)
                         if x1 != None:
                              a = x2
                              root = bisection(f,x1,x2,1)
                              if root != None: print root
                         else:
                              print "\nDone"
                              break
                    raw_input("Press return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-04     978 0 521 19132 6                                          December 16, 2009    15:4




               145     4.4 Methods Based on Linear Interpolation

                            The output from the program is:

                      The roots are:
                      0.0
                      4.4934094581
                      7.72525183707
                      10.9041216597
                      14.0661939129
                      17.2207552722
                      Done




              4.4     Methods Based on Linear Interpolation
                      Secant and False Position Methods
                      The secant and the false position methods are closely related. Both methods require
                      two starting estimates of the root, say, x1 and x2 . The function f (x) is assumed to be
                      approximately linear near the root, so that the improved value x3 of the root can be
                      estimated by linear interpolation between x1 and x2 .
                      Referring to Fig. 4.2, we obtain from similar triangles (shaded in the figure)

                                                               f2     f1 − f2
                                                                    =
                                                            x3 − x2   x2 − x1

                      where we used the notation fi = f (xi ). This yields for the improved estimate of the
                      root
                                                                              x2 − x1
                                                           x3 = x2 − f2                                            (4.2)
                                                                              f2 − f1

                           The false position method (also known as regula falsi) requires x1 and x2 to
                      bracket the root. After the improved root is computed from Eq. (4.2), either x1 or x2
                      is replaced by x3 . If f3 has the same sign as f1 , we let x1 ← x3 ; otherwise, we choose
                      x2 ← x3 . In this manner, the root is always bracketed in (x1 , x2 ). The procedure is then
                      repeated until convergence is obtained.



                                     f(x)



                                              f1                                        Linear
                                                                                        approximation
                                                                    f2
                                               x1                        x2               x3   x
                                     Figure 4.2. Linear interpolation.
P1: PHB

CUUS884-Kiusalaas     CUUS884-04        978 0 521 19132 6                                           December 16, 2009         15:4




           146      Roots of Equations


                                                                           g(x)
                    f(x)
                                                            x2                                             x 4 x2
                                   x1          x3                      x                   x1        x3                  x
                                        h           h                                           h         h
                                             (a)                                                    (b)
                    Figure 4.3. Mapping used in Ridder’s method.


                         The secant method differs from the false-position method in two details: (1) It
                    does not require prior bracketing of the root; and (2) the oldest prior estimate of the
                    root is discarded, that is, after x3 is computed, we let x1 ← x2 , x2 ← x3 .
                         The convergence of the secant method can be shown to be superlinear, the error
                    behaving as E k+1 = c E k1.618... (the exponent 1.618... is the “golden ratio”). The precise
                    order of convergence for the false position method is impossible to calculate. Gener-
                    ally, it is somewhat better than linear, but not by much. However, because the false
                    position method always brackets the root, it is more reliable. We will not delve fur-
                    ther into these methods, because both of them are inferior to Ridder’s method as far
                    as the order of convergence is concerned.


                    Ridder’s Method
                    Ridder’s method is a clever modification of the false position method. Assuming that
                    the root is bracketed in (x1 , x2 ), we first compute f3 = f (x3 ), where x3 is the midpoint
                    of the bracket, as indicated in Fig. 4.3(a). Next, we the introduce the function

                                                             g(x) = f (x)e(x−x1 )Q                                      (a)

                    where the constant Q is determined by requiring the points (x1 , g1 ), (x2 , g2 ), and
                    (x3 , g3 ) to lie on a straight line, as shown in Fig. 4.3(b). As before, the notation we
                    use is gi = g(xi ). The improved value of the root is then obtained by linear interpola-
                    tion of g(x) rather than f (x).
                          Let us now look at the details. From Eq. (a) we obtain

                                                g1 = f1          g2 = f2 e2hQ        g3 = f3 ehQ                        (b)

                    where h = (x2 − x1 )/2. The requirement that the three points in Fig. 4.3b lie on a
                    straight line is g3 = (g1 + g2 )/2, or
                                                                       1
                                                            f3 ehQ =     (f1 + f2 e2hQ )
                                                                       2
                    which is a quadratic equation in ehQ . The solution is

                                                                    f3 ±     f32 − f1 f2
                                                            ehQ =                                                       (c)
                                                                             f2
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                                          December 16, 2009    15:4




               147     4.4 Methods Based on Linear Interpolation

                          Linear interpolation based on points (x1 , g1 ) and (x3 , g3 ) now yields for the im-
                      proved root
                                                             x3 − x1                x3 − x1
                                            x4 = x3 − g 3            = x3 − f3 ehQ hQ
                                                             g3 − g1              f3 e − f1
                      where in the last step we utilized Eqs. (b). As the final step, we substitute for ehQ from
                      Eq. (c) and obtain after some algebra
                                                                                    f3
                                                       x4 = x3 ± (x3 − x1 )                                       (4.3)
                                                                              f32   − f1 f2

                      It can be shown that the correct result is obtained by choosing the plus sign if f1 −
                      f2 > 0, and the minus sign if f1 − f2 < 0. After the computation of x4 , new brackets
                      are determined for the root and Eq. (4.3) is applied again. The procedure is repeated
                      until the difference between two successive values of x4 becomes negligible.
                            Ridder’s iterative formula in Eq. (4.3) has a very useful property: if x1 and
                      x2 straddle the root, then x4 is always within the interval (x1 , x2 ). In other words,
                      once the root is bracketed, it stays bracketed, making the method very reliable. The
                      downside is that each iteration requires two function evaluations. There are compet-
                      itive methods that get by with only one function evaluation per iteration (e.g., Brent’s
                      method), but they are more complex, with elaborate book-keeping.
                            Ridder’s method can be shown to converge quadratically, making it faster than
                      either the secant or the false position method. It is the method to use if the derivative
                      of f (x) is impossible or difficult to compute.

                         ridder

                      The following is the source code for Ridder’s method:

                      ## module ridder
                      ’’’ root = ridder(f,a,b,tol=1.0e-9).
                            Finds a root of f(x) = 0 with Ridder’s method.
                            The root must be bracketed in (a,b).
                      ’’’
                      import error
                      from math import sqrt


                      def ridder(f,a,b,tol=1.0e-9):
                            fa = f(a)
                            if fa == 0.0: return a
                            fb = f(b)
                            if fb == 0.0: return b
                            if fa*fb > 0.0: error.err(’Root is not bracketed’)
                            for i in range(30):
                              # Compute the improved root x from Ridder’s formula
                                  c = 0.5*(a + b); fc = f(c)
                                  s = sqrt(fc**2 - fa*fb)
P1: PHB

CUUS884-Kiusalaas     CUUS884-04       978 0 521 19132 6                                       December 16, 2009   15:4




           148      Roots of Equations

                               if s == 0.0: return None
                               dx = (c - a)*fc/s
                               if (fa - fb) < 0.0: dx = -dx
                               x = c + dx; fx = f(x)
                            # Test for convergence
                               if i > 0:
                                     if abs(x - xOld) < tol*max(abs(x),1.0): return x
                               xOld = x
                            # Re-bracket the root as tightly as possible
                               if fc*fx > 0.0:
                                     if fa*fx < 0.0: b = x; fb = fx
                                     else:                     a = x; fa = fx
                               else:
                                     a = c; b = x; fa = fc; fb = fx
                         return None
                         print ’Too many iterations’

                    EXAMPLE 4.4
                    Determine the root of f (x) = x 3 − 10x 2 + 5 = 0 that lies in (0.6, 0.8) with Ridder’s
                    method.

                    Solution The starting points are

                                         x1 = 0.6          f1 = 0.63 − 10(0.6)2 + 5 = 1.6160
                                         x2 = 0.8          f2 = 0.83 − 10(0.8)2 + 5 = −0.8880

                    First iteration
                    Bisection yields the point

                                          x3 = 0.7         f3 = 0.73 − 10(0.7)2 + 5 = 0.4430

                    The improved estimate of the root can now be computed with Ridder’s formula:

                                    s=     f32 − f1 f2 =     0.43302 − 1.6160(−0.8880) = 1.2738
                                                                                  f3
                                                           x4 = x3 ± (x3 − x1 )
                                                                                  s
                    Because f1 > f2 , we must use the plus sign. Therefore,
                                                                        0.4430
                                               x4 = 0.7 + (0.7 − 0.6)          = 0.7348
                                                                        1.2738

                                             f4 = 0.73483 − 10(0.7348)2 + 5 = −0.0026

                    As the root clearly lies in the interval (x3 , x4 ), we let

                                             x1 ← x3 = 0.7          f1 ← f3 = 0.4430

                                             x2 ← x4 = 0.7348           f2 ← f4 = −0.0026

                    which are the starting points for the next iteration.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                                     December 16, 2009   15:4




               149     4.4 Methods Based on Linear Interpolation

                      Second iteration


                                             x3 = 0.5(x1 + x2 ) = 0.5(0.7 + 0.7348) = 0.717 4

                                                f3 = 0.717 43 − 10(0.717 4)2 + 5 = 0.2226


                                        s=     f32 − f1 f2 =     0.22262 − 0.4430(−0.0026) = 0.2252


                                                                                      f3
                                                               x4 = x3 ± (x3 − x1 )
                                                                                      s
                      Because f1 > f2 , we again use the plus sign, so that
                                                                                 0.2226
                                             x4 = 0.717 4 + (0.717 4 − 0.7)             = 0.7346
                                                                                 0.2252

                                                 f4 = 0.73463 − 10(0.7346)2 + 5 = 0.0000

                      Thus the root is x = 0.7346, accurate to at least four decimal places.

                      EXAMPLE 4.5
                      Compute the zero of the function
                                                                  1                 1
                                               f (x) =                     −
                                                          (x − 0.3)2 + 0.01 (x − 0.8)2 + 0.04
                      Solution We obtain the approximate location of the root by plotting the function.

                            100


                              80


                              60


                              40


                              20


                                  0


                             -20


                             -40
                               -2                -1                 0             1           2             3
P1: PHB

CUUS884-Kiusalaas     CUUS884-04        978 0 521 19132 6                                          December 16, 2009         15:4




           150      Roots of Equations

                         It is evident that the root of f (x) = 0 lies between x = 0.5 and 0.7. We can extract
                    this root with the following program:

                    #!/usr/bin/python
                    ## example4_5
                    from math import cos
                    from ridder import *


                    def f(x):
                         a = (x - 0.3)**2 + 0.01
                         b = (x - 0.8)**2 + 0.04
                         return 1.0/a - 1.0/b


                    print "root =",ridder(f,0.5,0.7)
                    raw_input("Press return to exit")

                        The result is

                    root = 0.58




          4.5       Newton–Raphson Method

                    The Newton–Raphson algorithm is the best-known method of finding roots for a
                    good reason: it is simple and fast. The only drawback of the method is that it uses
                    the derivative f (x)of the function as well as the function f (x) itself. Therefore, the
                    Newton–Raphson method is usable only in problems where f (x) can be readily
                    computed.
                          The Newton–Raphson formula can be derived from the Taylor series expansion
                    of f (x) about x:

                                          f (xi+1 ) = f (xi ) + f (xi )(xi+1 − xi ) + O(xi+1 − xi )2                   (a)

                    where O(z) is to be read as “of the order of z” – see Appendix A1. If xi+1 is a root of
                    f (x) = 0, Eq. (a) becomes

                                             0 = f (xi ) + f (xi ) (xi+1 − xi ) + O(xi+1 − xi )2                       (b)

                    Assuming that xi is close to xi+1 , we can drop the last term in Eq. (b) and solve for
                    xi+1 . The result is the Newton–Raphson formula
                                                                            f (x)
                                                            xi+1 = xi −                                           (4.3)
                                                                           f (x)
                       Letting x denote the true value of the root, the error in xi is Ei = x − xi . It can be
                    shown that if xi+1 is computed from Eq. (4.3), the corresponding error is
                                                                       f (x) 2
                                                            Ei+1 = −         E
                                                                       2f (x) i
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                                     December 16, 2009   15:4




               151     4.5 Newton–Raphson Method


                                                                Tangent line
                                    f(x)

                                                                                    f(xi )

                                                 xi +1                            xi             x
                                    Figure 4.4. Graphical interpretation of Newon–Raphson formula.


                      indicating that the Newton–Raphson method converges quadratically (the error is
                      the square of the error in the previous step). As a consequence, the number of sig-
                      nificant figures is roughly doubled in every iteration, provided that xi is close to the
                      root.
                           A graphical depiction of the Newton–Raphson formula is shown in Fig. 4.4. The
                      formula approximates f (x) by the straight line that is tangent to the curve at xi . Thus,
                      xi+1 is at the intersection of the x-axis and the tangent line.
                           The algorithm for the Newton–Raphson method is simple: it repeatedly applies
                      Eq. (4.3), starting with an initial value x0 , until the convergence criterion

                                                            |xi+1 − xi | < ε

                      is reached, ε being the error tolerance. Only the latest value of x has to be stored. Here
                      is the algorithm:

                      1. Let x be a guess for the root of f (x) = 0.
                      2. Compute x = −f (x)/f (x).
                      3. Let x ← x + x and repeat steps 2–3 until | x| < ε.

                           Although the Newton–Raphson method converges fast near the root, its global
                      convergence characteristics are poor. The reason is that the tangent line is not always
                      an acceptable approximation of the function, as illustrated in the two examples in
                      Fig. 4.5. But the method can be made nearly fail-safe by combining it with bisection.


                      f(x)                                         f(x)


                                                                               x1                    x0 x2
                                                            x                                                 x
                                   x0

                                           (a)                                               (b)
                      Figure 4.5. Examples where the Newton–Raphson method diverges.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                               December 16, 2009       15:4




           152      Roots of Equations

                      newtonRaphson

                    The following safe version of the Newton–Raphson method assumes that the root to
                    be computed is initially bracketed in (a,b). The midpoint of the bracket is used
                    as the initial guess of the root. The brackets are updated after each iteration. If a
                    Newton–Raphson iteration does not stay within the brackets, it is disregarded and
                    replaced with bisection. Because newtonRaphson uses the function f(x) as well as
                    its derivative, function routines for both (denoted by f and df in the listing) must be
                    provided by the user.

                    ## module newtonRaphson
                    ’’’ root = newtonRaphson(f,df,a,b,tol=1.0e-9).
                          Finds a root of f(x) = 0 by combining the Newton--Raphson
                          method with bisection. The root must be bracketed in (a,b).
                          Calls user-supplied functions f(x) and its derivative df(x).
                    ’’’
                    def newtonRaphson(f,df,a,b,tol=1.0e-9):
                          import error
                          fa = f(a)
                          if fa == 0.0: return a
                          fb = f(b)
                          if fb == 0.0: return b
                          if fa*fb > 0.0: error.err(’Root is not bracketed’)
                          x = 0.5*(a + b)
                          for i in range(30):
                              fx = f(x)
                              if abs(fx) < tol: return x
                            # Tighten the brackets on the root
                              if fa*fx < 0.0:
                                   b = x
                              else:
                                   a = x
                            # Try a Newton-Raphson step
                              dfx = df(x)
                            # If division by zero, push x out of bounds
                              try: dx = -fx/dfx
                              except ZeroDivisionError: dx = b - a
                              x = x + dx
                            # If the result is outside the brackets, use bisection
                              if (b - x)*(x - a) < 0.0:
                                   dx = 0.5*(b-a)
                                   x = a + dx
                            # Check for convergence
                              if abs(dx) < tol*max(abs(b),1.0): return x
                          print ’Too many iterations in Newton-Raphson’
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                                       December 16, 2009   15:4




               153     4.5 Newton–Raphson Method

                      EXAMPLE 4.6
                      A root of f (x) = x 3 − 10x 2 + 5 = 0 lies close to x = 7. Compute this root with the
                      Newton–Raphson method.

                      Solution The derivative of the function is f (x) = 3x 2 − 20x, so that the Newton–
                      Raphson formula in Eq. (4.3) is
                                                  f (x)     x 3 − 10x 2 + 5   2x 3 − 10x 2 − 5
                                      x←x−              =x−                 =
                                                 f (x)         3x − 20x
                                                                 2              x (3x − 20)
                      It takes only two iterations to reach five-decimal-place accuracy:
                                                       2(0.7)3 − 10(0.7)2 − 5
                                               x←                             = 0.735 36
                                                          0.7 [3(0.7) − 20]

                                               2(0.735 36)3 − 10(0.735 36)2 − 5
                                         x←                                     = 0.734 60
                                                  0.735 36 [3(0.735 36) − 20]
                      EXAMPLE 4.6
                                                                                          √
                      Use the Newton–Raphson method to obtain successive approximations of 2 as the
                      ratio of two integers.

                      Solution The problem is equivalent to finding the root of f (x) = x 2 − 2 = 0. Here the
                      Newton–Raphson formula is
                                                            f (x)     x2 − 2   x2 + 2
                                               x←x−               =x−        =
                                                           f (x)        2x       2x
                      Starting with x = 1, successive iterations yield
                                                             (1)2 + 2   3
                                                        x←            =
                                                               2(1)     2
                                                             (3/2)2 + 2   17
                                                        x←              =
                                                               2(3/2)     12
                                                             (17/12)2 + 2   577
                                                        x←                =
                                                               2(17/12)     408
                                                             ..
                                                              .
                                                                                √
                      Note that x = 577/408 = 1.1414216 is already very close to 2 = 1.1414214.
                          The results are dependent on the starting value of x. For example, x = 2 would
                      produce a different sequence of ratios.

                      EXAMPLE 4.7
                      Find the smallest positive zero of


                                          f (x) = x 4 − 6.4x 3 + 6.45x 2 + 20.538x − 31.752

                      Solution Inspecting the plot of the function, we suspect that the smallest positive
                      zero is a double root at about x = 2. Bisection and Ridder’s method would not work
                      here, because they depend on the function changing its sign at the root. The same
P1: PHB

CUUS884-Kiusalaas    CUUS884-04        978 0 521 19132 6                              December 16, 2009     15:4




           154      Roots of Equations

                                     60

                                     40



                              f(x)
                                     20

                                      0


                                     -20

                                     -40
                                           0         1      2           3         4           5
                                                                    x

                    argument applies to the function newtonRaphson. But there is no reason why the
                    unrefined version of the Newton–Raphson method should not succeed. We used the
                    following program, which prints the number of iterations in addition to the root:

                    #!/usr/bin/python
                    ## example4_7


                    def f(x): return x**4 - 6.4*x**3 + 6.45*x**2 + 20.538*x - 31.752
                    def df(x): return 4.0*x**3 - 19.2*x**2 + 12.9*x + 20.538


                    def newtonRaphson(x,tol=1.0e-9):
                         for i in range(30):
                              dx = -f(x)/df(x)
                              x = x + dx
                              if abs(dx) < tol: return x,i
                         print ’Too many iterations\n’


                    root,numIter = newtonRaphson(2.0)
                    print ’Root =’,root
                    print ’Number of iterations =’,numIter
                    raw_input(’’Press return to exit’’)


                        The output is

                    Root = 2.09999998403
                    Number of iterations = 23


                        The true value of the root is x = 2.1. It can be shown that near a multiple root,
                    the convergence of the Newton–Raphson method is linear rather than quadratic,
                    which explains the large number of iterations. Convergence to a multiple root can
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                                            December 16, 2009     15:4




               155     4.6 Systems of Equations

                      be speeded up by replacing the Newton–Raphson formula in Eq. (4.3) with
                                                                                f (x)
                                                           xi+1 = xi − m
                                                                                f (x)
                      where m is the multiplicity of the root (m = 2 in this problem). After making the
                      change in the above program, we obtained the result in only five iterations.



              4.6     Systems of Equations
                      Introduction
                      Up to this point, we have confined our attention to solving the single equation f (x) =
                      0. Let us now consider the n-dimensional version of the same problem, namely

                                                                   f(x) = 0

                      or, using scalar notation,

                                                          f1 (x1 , x2 , . . . , xn ) = 0

                                                          f2 (x1 , x2 , . . . , xn ) = 0                             (4.4)
                                                                                        ..
                                                                                         .

                                                          fn (x1 , x2 , . . . , xn ) = 0

                      The solution of n simultaneous, nonlinear equations is a much more formidable task
                      than finding the root of a single equation. The trouble is the lack of a reliable method
                      for bracketing the solution vector x. Therefore, we cannot always provide the solution
                      algorithm with a good starting value of x, unless such a value is suggested by the
                      physics of the problem.
                           The simplest, and the most effective means of computing x is the Newton–
                      Raphson method. It works well with simultaneous equations, provided that it is sup-
                      plied with a good starting point. There are other methods that have better global
                      convergence characteristics, but all of them are variants of the Newton–Raphson
                      method.


                      Newton–Raphson Method
                      In order to derive the Newton–Raphson method for a system of equations, we start
                      with the Taylor series expansion of fi (x) about the point x:
                                                                         n
                                                                              ∂fi
                                             fi (x +   x) = fi (x) +                 x j + O( x 2 )                 (4.5a)
                                                                              ∂x j
                                                                       j =1

                      Dropping terms of order      x 2 , we can write Eq. (4.5a) as

                                                       f(x +    x) = f(x) + J(x) x                                  (4.5b)
P1: PHB

CUUS884-Kiusalaas        CUUS884-04    978 0 521 19132 6                                 December 16, 2009       15:4




           156      Roots of Equations

                    where J(x) is the Jacobian matrix (of size n × n) made up of the partial derivatives
                                                                       ∂fi
                                                               Jij =                                     (4.6)
                                                                       ∂x j
                    Note that Eq. (4.5b) is a linear approximation (vector x being the variable) of the
                    vector-valued function f in the vicinity of point x.
                         Let us now assume that x is the current approximation of the solution of
                    f(x) = 0, and let x + x be the improved solution. To find the correction x, we set
                    f(x + x) = 0 in Eq. (4.5b). The result is a set of linear equations for x :

                                                           J(x) x = −f(x)                                (4.7)

                       The following steps constitute the Newton–Raphson method for simultaneous,
                    nonlinear equations:

                    1.    Estimate the solution vector x.
                    2.    Evaluate f(x).
                    3.    Compute the Jacobian matrix J(x) from Eq. (4.6).
                    4.    Set up the simultaneous equations in Eq. (4.7) and solve for   x.
                    5.    Let x ← x + x and repeat steps 2–5.

                        The foregoing process is continued until | x| < ε, where ε is the error tolerance.
                    As in the one-dimensional case, the success of the Newton–Raphson procedure de-
                    pends entirely on the initial estimate of x. If a good starting point is used, conver-
                    gence to the solution is very rapid. Otherwise, the results are unpredictable.
                        Because analytical derivation of each ∂fi /∂x j can be difficult or impractical, it is
                    preferable to let the computer calculate the partial derivatives from the finite differ-
                    ence approximation
                                                      ∂fi    fi (x + e j h) − fi (x)
                                                           ≈                                             (4.8)
                                                      ∂x j              h
                    where h is a small increment of applied to x j and e j represents a unit vector in the
                    direction of x j . This formula can be obtained from Eq. (4.5a) after dropping the terms
                    of order x 2 and setting x = e j h. We get away with the approximation in Eq. (4.8)
                    because the Newton–Raphson method is rather insensitive to errors in J(x). By using
                    this approximation, we also avoid the tedium of typing the expressions for ∂fi /∂x j
                    into the computer code.


                         newtonRaphson2

                    This function is an implementation of the Newton–Raphson method. The nested
                    function jacobian computes the Jacobian matrix from the finite difference ap-
                    proximation in Eq. (4.8). The simultaneous equations in Eq. (4.7) are solved by
                    Gauss elimination with row pivoting using the function gaussPivot listed in Sec-
                    tion 2.5. The function subroutine f that returns the array f(x) must be supplied by
                    the user.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04    978 0 521 19132 6                               December 16, 2009   15:4




               157     4.6 Systems of Equations

                      ## module newtonRaphson2
                      ’’’ soln = newtonRaphson2(f,x,tol=1.0e-9).
                            Solves the simultaneous equations f(x) = 0 by
                            the Newton-Raphson method using {x} as the initial
                            guess. Note that {f} and {x} are vectors.
                      ’’’
                      from numpy import zeros,dot
                      from gaussPivot import *
                      from math import sqrt


                      def newtonRaphson2(f,x,tol=1.0e-9):


                            def jacobian(f,x):
                                  h = 1.0e-4
                                  n = len(x)
                                  jac = zeros((n,n))
                                  f0 = f(x)
                                  for i in range(n):
                                      temp = x[i]
                                      x[i] = temp + h
                                      f1 = f(x)
                                      x[i] = temp
                                      jac[:,i] = (f1 - f0)/h
                                  return jac,f0


                            for i in range(30):
                                  jac,f0 = jacobian(f,x)
                                  if sqrt(dot(f0,f0)/len(x)) < tol: return x
                                  dx = gaussPivot(jac,-f0)
                                  x = x + dx
                                  if sqrt(dot(dx,dx)) < tol*max(max(abs(x)),1.0): return x
                            print ’Too many iterations’



                           Note that the Jacobian matrix J(x) is recomputed in each iterative loop. Because
                      each calculation of J(x) involves n + 1 evaluations of f(x) (n is the number of equa-
                      tions), the expense of computation can be high depending on n and the complexity
                      of f(x). It is often possible to save computer time by neglecting the changes in the
                      Jacobian matrix between iterations, thus computing J(x) only once. This will work
                      provided that the initial x is sufficiently close to the solution.


                      EXAMPLE 4.8
                      Determine the points of intersection between the circle x 2 + y 2 = 3 and the hyper-
                      bola xy = 1.
P1: PHB

CUUS884-Kiusalaas     CUUS884-04     978 0 521 19132 6                                          December 16, 2009         15:4




           158      Roots of Equations

                    Solution The equations to be solved are

                                                     f1 (x, y) = x 2 + y 2 − 3 = 0                                  (a)

                                                     f2 (x, y) = xy − 1 = 0                                         (b)

                    The Jacobian matrix is

                                                           ∂f1 /∂x    ∂f1 /∂y   2x         2y
                                             J(x, y) =                        =
                                                           ∂f2 /∂x    ∂f2 /∂y    y         x

                    Thus, the linear equations J(x) x = −f(x) associated with the Newton–Raphson
                    method are
                                                2x    2y          x         −x 2 − y 2 + 3
                                                                      =                                             (c)
                                                 y    x           y           −xy + 1

                        By plotting the circle and the hyperbola, we see that there are four points of in-
                    tersection. It is sufficient, however, to find only one of these points, as the others can
                    be deduced from symmetry. From the plot, we also get rough estimate of the coordi-
                    nates of an intersection point: x = 0.5, y = 1.5, which we use as the starting values.


                                                                  y
                                                                      2

                                                                      1
                                                              3
                                                                                                x
                                        -2           -1                         1          2

                                                                      -1

                                                                      -2
                    The computations then proceed as follows.

                    First iteration
                    Substituting x = 0.5, y = 1.5 in Eq. (c), we get

                                                     1.0    3.0        x            0.50
                                                                            =
                                                     1.5    0.5        y            0.25

                    the solution of which is   x=        y = 0.125. Therefore, the improved coordinates of the
                    intersection point are

                                     x = 0.5 + 0.125 = 0.625               y = 1.5 + 0.125 = 1.625
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                         December 16, 2009   15:4




               159     4.6 Systems of Equations

                      Second iteration
                      Repeating the procedure using the latest values of x and y, we obtain

                                                 1.250      3.250      x           −0.031250
                                                                               =
                                                 1.625      0.625      y           −0.015625

                      which yields     x=     y = −0.00694. Thus,

                               x = 0.625 − 0.006 94 = 0.618 06             y = 1.625 − 0.006 94 = 1.618 06

                      Third iteration
                      Substitution of the latest x and y into Eq. (c) yields

                                              1.236 12      3.23612        x           −0.000 116
                                                                                   =
                                              1.618 06      0.61806        y           −0.000 058

                          The solution is      x=        y = −0.00003, so that

                                                    x = 0.618 06 − 0.000 03 = 0.618 03

                                                    y = 1.618 06 − 0.000 03 = 1.618 03

                          Subsequent iterations would not change the results within five significant fig-
                      ures. Therefore, the coordinates of the four intersection points are

                                            ±(0.618 03, 1.618 03) and ± (1.618 03, 0.618 03)

                      Alternate solution
                      If there are only a few equations, it may be possible to eliminate all but one of the
                      unknowns. Then we would be left with a single equation which can be solved by the
                      methods described in Sections 4.2–4.5. In this problem, we obtain from Eq. (b)
                                                                           1
                                                                      y=
                                                                           x
                      which, upon substitution into Eq. (a), yields x 2 + 1/x 2 − 3 = 0, or

                                                               x 4 − 3x 2 + 1 = 0

                      The solutions of this biquadratic equation; x = ±0.618 03 and ±1.618 03, agree with
                      the results obtained by the Newton–Raphson method.

                      EXAMPLE 4.9
                      Find a solution of

                                                           sin x + y 2 + ln z − 7 = 0

                                                               3x + 2y − z3 + 1 = 0

                                                                    x+y +z−5 = 0

                      using newtonRaphson2. Start with the point (1, 1, 1).
P1: PHB

CUUS884-Kiusalaas     CUUS884-04     978 0 521 19132 6                                    December 16, 2009       15:4




           160      Roots of Equations

                    Solution Letting x1 = x, x2 = y and x3 = z, we obtain the following program:

                    #!/usr/bin/python
                    ## example4_9
                    from numpy import zeros,array
                    from math import sin,log
                    from newtonRaphson2 import *


                    def f(x):
                         f = zeros(len(x))
                         f[0] = sin(x[0]) + x[1]**2 + log(x[2]) - 7.0
                         f[1] = 3.0*x[0] + 2.0**x[1] - x[2]**3 + 1.0
                         f[2] = x[0] + x[1] + x[2] - 5.0
                         return f


                    x = array([1.0, 1.0, 1.0])
                    print newtonRaphson2(f,x)
                    raw_input ("\nPress return to exit")

                        The output from this program is

                    [ 0.59905376       2.3959314         2.00501484]

                    PROBLEM SET 4.1
                     1. Use the Newton–Raphson method and a four-function calculator (+ − ×÷ oper-
                                                     √
                        ations only) to compute 3 75 with four-significant-figure accuracy.
                     2. Find the smallest positive (real) root of x 3 − 3.23x 2 − 5.54x + 9.84 = 0 by the
                        method of bisection.
                     3. The smallest positive, nonzero root of cosh x cos x − 1 = 0 lies in the interval
                        (4, 5). Compute this root by Ridder’s method.
                     4. Solve Problem 3 by the Newton–Raphson method.
                     5. A root of the equation tan x − tanh x = 0 lies in (7.0, 7.4). Find this root with
                        three-decimal-place accuracy by the method of bisection.
                     6. Determine the two roots of sin x + 3 cos x − 2 = 0 that lie in the interval (−2, 2).
                        Use the Newton–Raphson method.
                     7. Solve Prob. 6 using the secant formula, Eq. (4.2).
                     8. Draw a plot of f (x) = cosh x cos x − 1 in the range 0 ≤ x ≤ 10. (a) Verify from the
                        plot that the smallest positive, nonzero root of f (x) = 0 lies in the interval (4, 5).
                        (b) Show graphically that the Newton–Raphson formula would not converge to
                        this root if it is started with x = 4.
                     9. The equation x 3 − 1.2x 2 − 8.19x + 13.23 = 0 has a double root close to x = 2.
                        Determine this root with the Newton–Raphson method within four decimal
                        places.
                    10.   Write a program that computes all the roots of f (x) = 0 in a given interval with
                        Ridder’s method. Utilize the functions rootsearch and ridder. You may use
P1: PHB

CUUS884-Kiusalaas    CUUS884-04     978 0 521 19132 6                                    December 16, 2009   15:4




               161     4.6 Systems of Equations

                            the program in Example 4.3 as a model. Test the program by finding the roots of
                            x sin x + 3 cos x − x = 0 in (−6, 6).
                      11.      Repeat Prob. 10 with the Newton–Raphson method.
                      12.      Determine all real roots of x 4 + 0.9x 3 − 2.3x 2 + 3.6x − 25.2 = 0.
                      13.      Compute all positive real roots of x 4 + 2x 3 − 7x 2 + 3 = 0.
                      14.      Find all positive, nonzero roots of sin x − 0.1x = 0.
                      15.      The natural frequencies of a uniform cantilever beam are related to the roots
                            βi of the frequency equation f (β) = cosh β cos β + 1 = 0,
                            where
                                                            mL 3
                                               βi4 = (2π fi )2
                                                             EI
                                                fi = ith natural frequency (cps)

                                               m = mass of the beam

                                                L = length of the beam

                                                E = modulus of elasticity

                                                I = moment of inertia of the cross section

                            Determine the lowest two frequencies of a steel beam 0.9 m long, with a rectan-
                            gular cross section 25 mm wide and 2.5 mm high. The mass density of steel is
                            7850 kg/m3 and E = 200 GPa.
                      16.
                                                                 L               L
                                                                 2               2

                                                Length = s                O

                            A steel cable of length s is suspended as shown in the figure. The maximum ten-
                            sile stress in the cable, which occurs at the supports, is

                                                                 σ max = σ 0 cosh β

                            where
                                                        γL
                                                  β=
                                                        2σ 0
                                                σ 0 = tensile stress in the cable at O

                                                 γ = weight of the cable per unit volume

                                                  L = horizontal span of the cable

                            The length to span ratio of the cable is related to β by
                                                                   s  1
                                                                     = sinh β
                                                                   L  β
                            Find σ max if γ = 77 × 103 N/m3 (steel), L = 1000 m, and s = 1100 m.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                                  December 16, 2009   15:4




           162      Roots of Equations

                    17.


                                                                            c        e
                                      P                                                  P
                                                                    L

                          The aluminum W 310 × 202 (wide flange) column is subjected to an eccentric
                          axial load P as shown. The maximum compressive stress in the column is given
                          by the secant formula:

                                                                    ec     L    σ¯
                                                   σ max = σ¯ 1 +      sec
                                                                    r2     2r   E

                          where


                                      σ¯ = P/A = average stress

                                      A = 25, 800 mm2 = cross-sectional area of the column

                                       e = 85 mm = eccentricity of the load

                                       c = 170 mm = half-depth of the column

                                      r = 142 mm = radius of gyration of the cross section

                                      L = 7100 mm = length of the column

                                      E = 71 × 109 Pa = modulus of elasticity


                          Determine the maximum load P that the column can carry if the maximum
                          stress is not to exceed 120 × 106 Pa.
                    18.



                                                 ho Q                           h


                                                                                H
                          Bernoulli’s equation for fluid flow in an open channel with a small bump is

                                                      Q2            Q2
                                                         2
                                                           + h0 =          +h+ H
                                                       2
                                                    2gb h0        2gb 2 h2
P1: PHB

CUUS884-Kiusalaas    CUUS884-04     978 0 521 19132 6                                          December 16, 2009   15:4




               163     4.6 Systems of Equations

                            where
                                               Q = 1.2 m3 /s = volume rate of flow

                                                g = 9.81 m/s2 = gravitational acceleration

                                                b = 1.8 m = width of channel

                                               h0 = 0.6 m = upstream water level

                                               H = 0.075 m = height of bump

                                                h = water level above the bump

                            Determine h.
                      19.     The speed v of a Saturn V rocket in vertical flight near the surface of the earth
                            can be approximated by
                                                                    M0
                                                         v = u ln           − gt
                                                                  M0 − mt
                                                                        ˙
                            where

                                        u = 2510 m/s = velocity of exhaust relative to the rocket

                                       M0 = 2.8 × 106 kg = mass of rocket at liftoff

                                        ˙ = 13.3 × 103 kg/s = rate of fuel consumption
                                        m

                                        g = 9.81 m/s2 = gravitational acceleration

                                         t = time measured from liftoff

                            Determine the time when the rocket reaches the speed of sound (335 m/s).
                      20.

                                                        P
                                                        P2         T2

                                         Heating at                          Isothermal
                                         constant volume                     expansion

                                                        P1
                                                                 T1 Volume reducedT2
                                                                    by cooling
                                                                  V1             V2 V

                            The figure shows the thermodynamic cycle of an engine. The efficiency of this
                            engine for monatomic gas is
                                                                 ln(T2 /T1 ) − (1 − T1 /T2 )
                                                    η=
                                                             ln(T2 /T1 ) + (1 − T1 /T2 )/(γ − 1)
                            where T is the absolute temperature and γ = 5/3. Find T2 /T1 that results in 30%
                            efficiency (η = 0.3).
P1: PHB

CUUS884-Kiusalaas       CUUS884-04        978 0 521 19132 6                                      December 16, 2009   15:4




           164      Roots of Equations

                    21.      The Gibbs free energy of 1 mole of hydrogen at temperature T is

                                                              G = −RT ln (T/T0 )5/2 J

                        where R = 8.314 41 J/K is the gas constant and T0 = 4.444 18 K. Determine the
                        temperature at which G = −105 J.
                    22.   The chemical equilibrium equation in the production of methanol from CO
                        and H2 is1
                                                                 ξ (3 − 2ξ )2
                                                                              = 249.2
                                                                   (1 − ξ )3
                        where ξ is the equilibrium extent of the reaction. Determine ξ .
                    23.   Determine the coordinates of the two points where the circles (x − 2)2 + y 2 = 4
                        and x 2 + (y − 3)2 = 4 intersect. Start by estimating the locations of the points
                        from a sketch of the circles, and then use the Newton–Raphson method to com-
                        pute the coordinates.
                    24.   The equations
                                                               sin x + 3 cos x − 2 = 0
                                                               cos x − sin y + 0.2 = 0

                        have a solution in the vicinity of the point (1, 1). Use the Newton–Raphson
                        method to refine the solution.
                    25.   Use any method to find all real solutions of the simultaneous equations
                                                                      tan x − y = 1
                                                                 cos x − 3 sin y = 0

                          in the region 0 ≤ x ≤ 1.5.
                    26.     The equation of a circle is

                                                              (x − a)2 + (y − b)2 = R2

                          where R is the radius and (a, b) are the coordinates of the center. If the coordi-
                          nates of three points on the circle are

                                                          x (in.)    8.21    0.34         5.96
                                                          y (in.)    0.00    6.62        −1.12

                          determine R, a, and b.
                    27.

                                                                            R

                                                                O
                                                                         θ


                    1   From R. A. Alberty, Physical Chemistry, 7th ed. (Wiley, 1987).
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                        December 16, 2009   15:4




               165     4.6 Systems of Equations

                            The trajectory of a satellite orbiting the earth is

                                                                            C
                                                               R=
                                                                    1 + e sin(θ + α)

                            where (R, θ) are the polar coordinates of the satellite, and C, e, and α are con-
                            stants (e is known as the eccentricity of the orbit). If the satellite was observed at
                            the three positions

                                                             θ      −30◦         0◦     30◦
                                                          R (km)    6870        6728   6615

                            determine the smallest R of the trajectory and the corresponding value of θ.
                      28.


                                                  y
                                                                                45o
                                                      v
                                                                           61 m
                                                  Oθ                                          x
                                                                300 m

                            A projectile is launched at O with the velocity v at the angle θ to the horizontal.
                            The parametric equation of the trajectory is

                                                              x = (v cos θ )t
                                                                   1
                                                              y = − gt 2 + (v sin θ )t
                                                                   2

                            where t is the time measured from instant of launch, and g = 9.81 m/s2 repre-
                            sents the gravitational acceleration. If the projectile is to hit the target at the 45◦
                            angle shown in the figure, determine v, θ, and the time of flight.
                      29.


                                                                                   mm
                                                                           180
                                                                           θ2
                                                                                              mm




                                              y
                                                          m




                                                                                          200
                                                          m
                                                      0
                                                   15




                                                   θ1            200 mm                  θ3
                                                                                              x
P1: PHB

CUUS884-Kiusalaas     CUUS884-04        978 0 521 19132 6                                           December 16, 2009      15:4




           166      Roots of Equations

                          The three angles shown in the figure of the four-bar linkage are related by

                                                150 cos θ 1 + 180 cos θ 2 − 200 cos θ 3 = 200

                                                  150 sin θ 1 + 180 sin θ 2 − 200 sin θ 3 = 0

                          Determine θ 1 and θ 2 when θ 3 = 75◦ . Note that there are two solutions.
                    30.

                                                    A             12 m
                                                         θ1                               3m
                                                  4m
                                                                                      D
                                                        B   θ2   5m
                                                           6m     θ3
                                                      16 kN    C
                                                                         20 kN

                          The 15-m cable is suspended from A and D and carries concentrated loads at B
                          and C. The vertical equilibrium equations of joints B and C are

                                                            T (− tan θ 2 + tan θ 1 ) = 16

                                                              T (tan θ 3 + tan θ 2 ) = 20

                          where T is the horizontal component of the cable force (it is the same in all seg-
                          ments of the cable). In addition, there are two geometric constraints imposed by
                          the positions of the supports:

                                                    −4 sin θ 1 − 6 sin θ 2 + 5 sin θ 2 = −3

                                                     4 cos θ 1 + 6 cos θ 2 + 5 cos θ 3 = 12

                          Determine the angles θ 1 , θ 2 , and θ 3 .


          ∗
              4.7   Zeroes of Polynomials
                    Introduction
                    A polynomial of degree n has the form

                                                 Pn (x) = a 0 + a 1 x + a 2 x 2 + · · · + a n x n                  (4.9)

                    where the coefficients ai may be real or complex. We concentrate on polynomials
                    with real coefficients, but the algorithms presented in this section also work with
                    complex coefficients.
                        The polynomial equation Pn (x) = 0 has exactly n roots, which may be real or
                    complex. If the coefficients are real, the complex roots always occur in conjugate
                    pairs (xr + ixi , xr − ixi ), where xr and xi are the real and imaginary parts, respectively.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                            December 16, 2009   15:4




                       ∗
               167         4.7 Zeroes of Polynomials

                      For real coefficients, the number of real roots can be estimated from the rule of
                      Descartes:

                        • The number of positive, real roots equals the number of sign changes in the ex-
                          pression for Pn (x), or less by an even number.
                        • The number of negative, real roots is equal to the number of sign changes in
                          Pn (−x), or less by an even number.

                           As an example, consider P3 (x) = x 3 − 2x 2 − 8x + 27. Because the sign changes
                      twice, P3 (x) = 0 has either two or zero positive real roots. On the other hand,
                      P3 (−x) = −x 3 − 2x 2 + 8x + 27 contains a single sign change; hence P3 (x) possesses
                      one negative real zero.
                           The real zeroes of polynomials with real coefficients can always be computed by
                      one of the methods already described. But if complex roots are to be computed, it
                      is best to use a method that specializes in polynomials. Here we present a method
                      due to Laguerre, which is reliable and simple to implement. Before proceeding to La-
                      guerre’s method, we must first develop two numerical tools that are needed in any
                      method capable of determining the zeroes of a polynomial. The first of these is an
                      efficient algorithm for evaluating a polynomial and its derivatives. The second algo-
                      rithm we need is for the deflation of a polynomial, that is, for dividing the Pn (x) by
                      x − r , where r is a root of Pn (x) = 0.



                      Evaluation Polynomials
                      It is tempting to evaluate the polynomial in Eq. (4.9) from left to right by the following
                      algorithm (we assume that the coefficients are stored in the array a):

                      p = 0.0
                      for i in range(n+1):
                              p = p + a[i]*x**i


                          Because x k is evaluated as x × x × · · · × x (k − 1 multiplications), we deduce that
                      the number of multiplications in this algorithm is

                                                                                    1
                                                  1 + 2 + 3 + ··· +n − 1 =            n(n − 1)
                                                                                    2

                      If n is large, the number of multiplications can be reduced considerably if we evaluate
                      the polynomial from right to left. For an example, take

                                                 P4 (x) = a 0 + a 1 x + a 2 x 2 + a 3 x 3 + a 4 x 4

                      After rewriting the polynomial as

                                               P4 (x) = a 0 + x {a 1 + x [a 2 + x (a 3 + xa 4 )]}
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                          December 16, 2009      15:4




           168      Roots of Equations

                    the preferred computational sequence becomes obvious:

                                                         P0 (x) = a 4

                                                         P1 (x) = a 3 + x P0 (x)

                                                         P2 (x) = a 2 + x P1 (x)

                                                         P3 (x) = a 1 + x P2 (x)

                                                         P4 (x) = a 0 + x P3 (x)

                    For a polynomial of degree n, the procedure can be summarized as

                                           P0 (x) = a n

                                            Pi (x) = a n−i + x Pi−1 (x), i = 1, 2, . . . , n                  (4.10)

                    leading to the algorithm

                          p = a[n]
                          for i in range(1,n+1):
                              p = a[n-i] + p*x

                         The last algorithm involves only n multiplications, making it more efficient
                    for n > 3. But computational economy is not the prime reason why this algorithm
                    should be used. Because the result of each multiplication is rounded off, the proce-
                    dure with the least number of multiplications invariably accumulates the smallest
                    roundoff error.
                         Some root-finding algorithms, including Laguerre’s method, also require evalua-
                    tion of the first and second derivatives of Pn (x). From Eq. (4.10) we obtain by differ-
                    entiation

                                   P0 (x) = 0   Pi (x) = Pi−1 (x) + x Pi−1 (x),     i = 1, 2, . . . , n      (4.11a)

                                  P0 (x) = 0    Pi (x) = 2Pi−1 (x) + x Pi−1 (x), i = 1, 2, . . . , n         (4.11b)


                      evalPoly

                    Here is the function that evaluates a polynomial and its derivatives:

                    ## module evalPoly
                    ’’’ p,dp,ddp = evalPoly(a,x).
                          Evaluates the polynomial
                          p = a[0] + a[1]*x + a[2]*xˆ2 +...+ a[n]*xˆn
                          with its derivatives dp = p’ and ddp = p’’
                          at x.
                    ’’’
                    def evalPoly(a,x):
                          n = len(a) - 1
                          p = a[n]
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                             December 16, 2009     15:4




                       ∗
               169         4.7 Zeroes of Polynomials

                              dp = 0.0 + 0.0j
                              ddp = 0.0 + 0.0j
                              for i in range(1,n+1):
                                   ddp = ddp*x + 2.0*dp
                                   dp = dp*x + p
                                   p = p*x + a[n-i]
                              return p,dp,ddp



                      Deflation of Polynomials
                      After a root r of Pn (x) = 0 has been computed, it is desirable to factor the polynomial
                      as follows:

                                                           Pn (x) = (x − r )Pn−1 (x)                                   (4.12)

                      This procedure, known as deflation or synthetic division, involves nothing more than
                      computing the coefficients of Pn−1 (x). Because the remaining zeroes of Pn (x) are also
                      the zeroes of Pn−1 (x), the root-finding procedure can now be applied to Pn−1 (x) rather
                      than Pn (x). Deflation thus makes it progressively easier to find successive roots, be-
                      cause the degree of the polynomial is reduced every time a root is found. Moreover,
                      by eliminating the roots that have already been found, the chances of computing the
                      same root more than once are eliminated.
                           If we let

                                              Pn−1 (x) = b0 + b1 x + b2 x 2 + · · · + bn−1 x n−1

                      then Eq. (4.12) becomes

                                                 a 0 + a 1 x + a 2 x 2 + · · · + a n−1 x n−1 + a n x n

                                              = (x − r )(b0 + b1 x + b2 x 2 + · · · + bn−1 x n−1 )

                      Equating the coefficients of like powers of x, we obtain

                                       bn−1 = a n        bn−2 = a n−1 + r bn−1        ···   b0 = a 1 + r b1            (4.13)

                      which leads to Horner’s deflation algorithm:

                      b[n-1] = a[n]
                      for i in range(n-2,-1,-1):
                              b[i] = a[i+1] + r*b[i+1]



                      Laguerre’s Method
                      Laquerre’s formulas are not easily derived for a general polynomial Pn (x). However,
                      the derivation is greatly simplified if we consider the special case where the polyno-
                      mial has a zero at x = r and (n − 1) zeroes at x = q. Hence, the polynomial can be
P1: PHB

CUUS884-Kiusalaas        CUUS884-04    978 0 521 19132 6                                           December 16, 2009         15:4




           170      Roots of Equations

                    written as

                                                      Pn (x) = (x − r )(x − q)n−1                                      (a)

                    Our problem is now this: given the polynomial in Eq. (a) in the form

                                               Pn (x) = a 0 + a 1 x + a 2 x 2 + · · · + a n x n

                    determine r (note that q is also unknown). It turns out that the result, which is ex-
                    act for the special case considered here, works well as an iterative formula with any
                    polynomial.
                         Differentiating Eq. (a) with respect to x, we get

                                          Pn (x) = (x − q)n−1 + (n − 1)(x − r )(x − q)n−2
                                                                 1    n−1
                                                 = Pn (x)           +
                                                               x −r   x −q
                    Thus,
                                                        Pn (x)     1    n−1
                                                               =      +                                                (b)
                                                        Pn (x)   x −r   x −q
                    which upon differentiation yields
                                                                 2
                                            Pn (x)   Pn (x)                  1          n−1
                                                   −                 =−              −                                 (c)
                                            Pn (x)   Pn (x)               (x − r ) 2   (x − q)2
                    It is convenient to introduce the notation
                                                      Pn (x)                              Pn (x)
                                             G(x) =                  H(x) = G 2 (x) −                            (4.14)
                                                      Pn (x)                              Pn (x)
                    so that Eqs. (b) and (c) become
                                                                 1    n−1
                                                     G(x) =         +                                           (4.15a)
                                                               x −r   x −q
                                                                  1         n−1
                                                     H(x) =              +                                      (4.15b)
                                                               (x − r )2   (x − q)2
                    If we solve Eq. (4.15a) for x − q and substitute the result into Eq. (4.15b), we obtain a
                    quadratic equation for x − r. The solution of this equation is the Laguerre’s formula
                                                                           n
                                            x −r =                                                               (4.16)
                                                      G(x) ±      (n − 1) nH(x) − G 2 (x)

                           The procedure for finding a zero of a general polynomial by Laguerre’s formula
                    is:

                    1. Let x be a guess for the root of Pn (x) = 0 (any value will do).
                    2. Evaluate Pn (x), Pn (x), and Pn (x) using the procedure outlined in Eqs. (4.11).
                    3. Compute G(x) and H(x) from Eqs. (4.14).
                    4. Determine the improved root r from Eq. (4.16) choosing the sign that results in the
                       larger magnitude of the denominator (this can be shown to improve convergence).
                    5. Let x ← r and repeat steps 2–5 until |Pn (x)| < ε or |x − r | < ε, where ε is the error
                       tolerance.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                         December 16, 2009       15:4




                       ∗
               171         4.7 Zeroes of Polynomials

                          One nice property of Laguerre’s method is that it converges to a root, with very
                      few exceptions, from any starting value of x.

                           polyRoots

                      The function polyRoots in this module computes all the roots of Pn (x) = 0, where
                      the polynomial Pn (x) defined by its coefficient array a = [a 0 , a 1 , . . . , a n ]. After the first
                      root is computed by the nested function laguerre, the polynomial is deflated using
                      deflPoly and the next zero computed by applying laguerre to the deflated poly-
                      nomial. This process is repeated until all n roots have been found. If a computed root
                      has a very small imaginary part, it is more than likely that it represents roundoff error.
                      Therefore, polyRoots replaces a tiny imaginary part by zero.

                      from evalPoly import *
                      from numpy import zeros,complex
                      from cmath import sqrt
                      from random import random


                      def polyRoots(a,tol=1.0e-12):


                              def laguerre(a,tol):
                                   x = random()          # Starting value (random number)
                                   n = len(a) - 1
                                   for i in range(30):
                                        p,dp,ddp = evalPoly(a,x)
                                        if abs(p) < tol: return x
                                        g = dp/p
                                        h = g*g - ddp/p
                                        f = sqrt((n - 1)*(n*h - g*g))
                                        if abs(g + f) > abs(g - f): dx = n/(g + f)
                                        else: dx = n/(g - f)
                                        x = x - dx
                                        if abs(dx) < tol: return x
                                   print ’Too many iterations’


                              def deflPoly(a,root):            # Deflates a polynomial
                                   n = len(a)-1
                                   b = [(0.0 + 0.0j)]*n
                                   b[n-1] = a[n]
                                   for i in range(n-2,-1,-1):
                                        b[i] = a[i+1] + root*b[i+1]
                                   return b


                              n = len(a) - 1
                              roots = zeros((n),dtype=complex)
P1: PHB

CUUS884-Kiusalaas    CUUS884-04      978 0 521 19132 6                                        December 16, 2009   15:4




           172      Roots of Equations

                         for i in range(n):
                              x = laguerre(a,tol)
                              if abs(x.imag) < tol: x = x.real
                              roots[i] = x
                              a = deflPoly(a,x)
                         return roots
                         raw_input("\nPress return to exit")


                         Because the roots are computed with finite accuracy, each deflation introduces
                    small errors in the coefficients of the deflated polynomial. The accumulated roundoff
                    error increases with the degree of the polynomial and can become severe if the poly-
                    nomial is ill conditioned (small changes in the coefficients produce large changes in
                    the roots). Hence, the results should be viewed with caution when dealing with poly-
                    nomials of high degree.
                         The errors caused by deflation can be reduced by recomputing each root using
                    the original, undeflated polynomial. The roots obtained previously in conjunction
                    with deflation are employed as the starting values.


                    EXAMPLE 4.10
                    A zero of the polynomial P4 (x) = 3x 4 − 10x 3 − 48x 2 − 2x + 12 is x = 6. Deflate the
                    polynomial with Horner’s algorithm, that is, find P3 (x) so that (x − 6)P3 (x) = P4 (x).


                    Solution With r = 6 and n = 4, Eqs. (4.13) become


                                                b3 = a 4 = 3

                                                b2 = a 3 + 6b3 = −10 + 6(3) = 8

                                                b1 = a 2 + 6b2 = −48 + 6(8) = 0

                                                b0 = a 1 + 6b1 = −2 + 6(0) = −2


                    Therefore,


                                                         P3 (x) = 3x 3 + 8x 2 − 2


                    EXAMPLE 4.11
                    A root of the equation P3 (x) = x 3 − 4.0x 2 − 4.48x + 26.1 is approximately x = 3 − i.
                    Find a more accurate value of this root by one application of Laguerre’s iterative
                    formula.


                    Solution Use the given estimate of the root as the starting value. Thus,


                                          x = 3 −i          x 2 = 8 − 6i     x 3 = 18 − 26i
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                                 December 16, 2009   15:4




                       ∗
               173         4.7 Zeroes of Polynomials

                      Substituting these values in P3 (x) and its derivatives, we get

                                  P3 (x) = x 3 − 4.0x 2 − 4.48x + 26.1

                                        = (18 − 26i) − 4.0(8 − 6i) − 4.48(3 − i) + 26.1 = −1.34 + 2.48i

                                 P3 (x) = 3.0x 2 − 8.0x − 4.48

                                        = 3.0(8 − 6i) − 8.0(3 − i) − 4.48 = −4.48 − 10.0i

                                 P3 (x) = 6.0x − 8.0 = 6.0(3 − i) − 8.0 = 10.0 − 6.0i

                      Equations (4.14) then yield

                                            P3 (x)   −4.48 − 10.0i
                                  G(x) =           =               = −2.36557 + 3.08462i
                                            P3 (x)   −1.34 + 2.48i
                                                     P3 (x)                             10.0 − 6.0i
                                  H(x) = G 2 (x) −          = (−2.36557 + 3.08462i)2 −
                                                     P3 (x)                            −1.34 + 2.48i
                                        = 0.35995 − 12.48452i

                      The term under the square root sign of the denominator in Eq. (4.16) becomes


                                    F (x) =    (n − 1) n H(x) − G 2 (x)

                                          =    2 3(0.35995 − 12.48452i) − (−2.36557 + 3.08462i)2

                                          =    5.67822 − 45.71946i = 5.08670 − 4.49402i

                      Now we must find which sign in Eq. (4.16) produces the larger magnitude of the de-
                      nominator:

                                   |G(x) + F (x)| = |(−2.36557 + 3.08462i) + (5.08670 − 4.49402i)|

                                                   = |2.72113 − 1.40940i| = 3.06448

                                   |G(x) − F (x)| = |(−2.36557 + 3.08462i) − (5.08670 − 4.49402i)|

                                                   = |−7.45227 + 7.57864i| = 10.62884

                      Using the minus sign, Eq. (4.16) yields the following improved approximation for the
                      root:

                                                        n                            3
                                        r =x−                   = (3 − i) −
                                                   G(x) − F (x)             −7.45227 + 7.57864i
                                           = 3.19790 − 0.79875i

                      Thanks to the good starting value, this approximation is already quite close to the
                      exact value r = 3.20 − 0.80i.
P1: PHB

CUUS884-Kiusalaas    CUUS884-04        978 0 521 19132 6                                            December 16, 2009   15:4




           174      Roots of Equations

                    EXAMPLE 4.12
                    Use polyRoots to compute all the roots of x 4 − 5x 3 − 9x 2 + 155x − 250 = 0.

                    Solution The commands

                    >>> from polyRoots import *
                    >>> print polyRoots([-250.0,155.0,-9.0,-5.0,1.0])

                          resulted in the output

                    [ 2.+0.j      4.-3.j      4.+3.j -5.+0.j]

                    PROBLEM SET 4.2

                    Problems 1–5 A zero x = r of Pn (x) is given. Verify that r is indeed a zero, and then
                    deflate the polynomial, that is, find Pn−1 (x) so that Pn (x) = (x − r )Pn−1 (x).

                     1.   P3 (x) = 3x 3 + 7x 2 − 36x + 20, r = −5.
                     2.   P4 (x) = x 4 − 3x 2 + 3x − 1, r = 1.
                     3.   P5 (x) = x 5 − 30x 4 + 361x 3 − 2178x 2 + 6588x − 7992, r = 6.
                     4.   P4 (x) = x 4 − 5x 3 − 2x 2 − 20x − 24, r = 2i.
                     5.   P3 (x) = 3x 3 − 19x 2 + 45x − 13, r = 3 − 2i.


                    Problems 6–9 A zero x = r of Pn (x) is given. Determine all the other zeroes of Pn (x)
                    by using a calculator. You should need no tools other than deflation and the quadratic
                    formula.

                     6.   P3 (x) = x 3 + 1.8x 2 − 9.01x − 13.398,, r = −3.3.
                     7.   P3 (x) = x 3 − 6.64x 2 + 16.84x − 8.32, r = 0.64.
                     8.   P3 (x) = 2x 3 − 13x 2 + 32x − 13, r = 3 − 2i.
                     9.   P4 (x) = x 4 − 3x 2 + 10x 2 − 6x − 20, r = 1 + 3i.


                    Problems 10–15 Find all the zeroes of the given Pn (x).

                    10.    P4 (x) = x 4 + 2.1x 3 − 2.52x 2 + 2.1x − 3.52.
                    11.    P5 (x) = x 5 − 156x 4 − 5x 3 + 780x 2 + 4x − 624.
                    12.    P6 (x) = x 6 + 4x 5 − 8x 4 − 34x 3 + 57x 2 + 130x − 150.
                    13.    P7 (x) = 8x 7 + 28x 6 + 34x 5 − 13x 4 − 124x 3 + 19x 2 + 220x − 100.
                    14.    P8 (x) = x 8 − 7x 7 + 7x 6 + 25x 5 + 24x 4 − 98x 3 − 472x 2 + 440x + 800.
                    15.    P4 (x) = x 4 + (5 + i)x 3 − (8 − 5i)x 2 + (30 − 14i)x − 84.
                    16.
                          The two blocks of mass m each are connected by springs and a dashpot. The stiff-
                          ness of each spring is k, and c is the coefficient of damping of the dashpot. When
                          the system is displaced and released, the displacement of each block during the
                          ensuing motion has the form

                                                   xk (t ) = A k eωr t cos(ωi t + φ k ), k = 1, 2
P1: PHB

CUUS884-Kiusalaas    CUUS884-04       978 0 521 19132 6                                          December 16, 2009   15:4




                       ∗
               175         4.7 Zeroes of Polynomials




                                                                          k

                                                                      m
                                                                                  x1

                                                                k             c


                                                                      m
                                                                                  x2

                             where A k and φ k are constants, and ω = ωr ± iωi are the roots of
                                                                                                 2
                                                           c 3    k     c k    k
                                                  ω4 + 2     ω + 3 ω2 +     ω+                       =0
                                                           m      m     mm     m
                             Determine the two possible combinations of ωr and ωi if c/m = 12 s−1 and
                             k/m = 1500 s−2 .
                      17.




                                                                                                     w0
                                                                                                          x
                                                                     L
                                           y

                             The lateral deflection of the beam shown is
                                                             w0
                                                      y=          (x 5 − 3L 2 x 3 + 2L 3 x 2 )
                                                           120E I
                             where ω0 is the maximum load intensity and E I represents the bending rigidity.
                             Determine the value of x/L where the maximum displacement occurs.


                      Other Methods
                      The most prominent root-finding algorithm omitted from this chapter is Brent’s
                      method, which combines bisection and quadratic interpolation. It is potentially more
                      efficient than Ridder’s method, requiring only one function evaluation per iteration
                      (as compared to two evaluations in Ridder’s method), but this advantage is somewhat
                      negated by elaborate bookkeeping.
P1: PHB

CUUS884-Kiusalaas       CUUS884-04       978 0 521 19132 6                                          December 16, 2009   15:4




           176      Roots of Equations

                         There are many methods for finding zeroes of polynomials. Of these, the Jenkins-
                    Traub algorithm2 deserves special mention because of its robustness and widespread
                    use in packaged software.
                         The zeroes of a polynomial can also be obtained by calculating the eigenvalues
                    of the n × n “companion matrix”
                                          ⎡                                                 ⎤
                                            −a n−1 /a n −a 2 /a n · · · −a 1 /a n −a 0 /a n
                                          ⎢1             0        ··· 0            0        ⎥
                                          ⎢                                                 ⎥
                                          ⎢                                                 ⎥
                                     A=⎢  ⎢ 0            0               0         0        ⎥
                                                                                            ⎥
                                          ⎢..           ..              ..        ..        ⎥
                                          ⎣.             .               .         .        ⎦
                                           0            0         ··· 1            0
                    where ai are the coefficients of the polynomial. The characteristic equation (see Sec-
                    tion 9.1) of this matrix is
                                              a n−1 n−1 a n−2 n−2         a1    a0
                                       xn +        x   +      x   + ··· +    x+     =0
                                               an         an              an    an
                    which is equivalent to Pn (x) = 0. Thus the eigenvalues of A are the zeroes of Pn (x).
                    The eigenvalue method is robust, but considerably slower than Laguerre’s method.
                    But it is worthy of consideration if a good program for eigenvalue problems is
                    available.




                    2   M. Jenkins and J. Traub, SIAM Journal on Numerical Analysis, Vol. 7 (1970), p. 545.
P1: PHB

CUUS884-Kiusalaas    CUUS884-05     978 0 521 19132 6                                     December 16, 2009      15:4




              5       Numerical Differentiation




                                       Given the function f (x), compute d n f/dx n at given x




              5.1     Introduction

                      Numerical differentiation deals with the following problem: We are given the func-
                      tion y = f (x) and wish to obtain one of its derivatives at the point x = xk . The term
                      “given” means that we either have an algorithm for computing the function or pos-
                      sess a set of discrete data points (xi , yi ), i = 0, 1, . . . , n. In either case, we have ac-
                      cess to a finite number of (x, y) data pairs from which to compute the derivative. If
                      you suspect by now that numerical differentiation is related to interpolation, you are
                      right – one means of finding the derivative is to approximate the function locally by
                      a polynomial and then differentiate it. An equally effective tool is the Taylor series
                      expansion of f (x) about the point xk , which has the advantage of providing us with
                      information about the error involved in the approximation.
                           Numerical differentiation is not a particularly accurate process. It suffers from a
                      conflict between roundoff errors (due to limited machine precision) and errors inher-
                      ent in interpolation. For this reason, a derivative of a function can never be computed
                      with the same precision as the function itself.



              5.2     Finite Difference Approximations

                      The derivation of the finite difference approximations for the derivatives of f (x) is
                      based on forward and backward Taylor series expansions of f (x) about x, such as

                                                                 h2         h3         h4 (4)
                                  f (x + h) = f (x) + hf (x) +      f (x) +    f (x) +    f (x) + · · ·          (a)
                                                                 2!         3!         4!


                                                                 h2         h3         h4 (4)
                                  f (x − h) = f (x) − hf (x) +      f (x) −    f (x) +    f (x) − · · ·          (b)
                                                                 2!         3!         4!

               177
P1: PHB

CUUS884-Kiusalaas     CUUS884-05      978 0 521 19132 6                                       December 16, 2009         15:4




           178      Numerical Differentiation

                                                            (2h)2         (2h)3         (2h)4 (4)
                         f (x + 2h) = f (x) + 2hf (x) +           f (x) +       f (x) +      f (x) + · · ·        (c)
                                                              2!            3!            4!

                                                            (2h)2         (2h)3         (2h)4 (4)
                         f (x − 2h) = f (x) − 2hf (x) +           f (x) −       f (x) +      f (x) − · · ·        (d)
                                                              2!            3!            4!
                    We also record the sums and differences of the series:
                                                                                   h4 (4)
                                    f (x + h) + f (x − h) = 2f (x) + h2 f (x) +       f (x) + · · ·               (e)
                                                                                   12
                                                                         h3
                                    f (x + h) − f (x − h) = 2hf (x) +       f (x) + . . .                         (f)
                                                                         3

                                                                                    4h4 (4)
                                 f (x + 2h) + f (x − 2h) = 2f (x) + 4h2 f (x) +        f (x) + · · ·              (g)
                                                                                     3
                                                                         8h3
                                 f (x + 2h) − f (x − 2h) = 4hf (x) +         f (x) + · · ·                        (h)
                                                                          3
                    Note that the sums contain only even derivatives, whereas the differences retain just
                    the odd derivatives. Equations (a)–(h) can be viewed as simultaneous equations that
                    can be solved for various derivatives of f (x). The number of equations involved and
                    the number of terms kept in each equation depend on the order of the derivative and
                    the desired degree of accuracy.


                    First Central Difference Approximations
                    The solution of Eq. (f) for f (x) is
                                                    f (x + h) − f (x − h) h2
                                          f (x) =                        −   f (x) − · · ·
                                                             2h            6
                    or

                                                          f (x + h) − f (x − h)
                                               f (x) =                          + O(h2 )                     (5.1)
                                                                   2h
                    which is called the first central difference approximation for f (x). The term O(h2 )
                    reminds us that the truncation error behaves as h2 .
                       Similarly, Eq. (e) yields the first central difference approximation for f (x):
                                               f (x + h) − 2f (x) + f (x − h)   h2 (4)
                                     f (x) =                   2
                                                                              +    f (x) + . . .
                                                            h                   12
                    or
                                                    f (x + h) − 2f (x) + f (x − h)
                                          f (x) =                                  + O(h2 )                  (5.2)
                                                                 h2
                         Central difference approximations for other derivatives can be obtained from
                    Eqs. (a)–(h) in the same manner. For example, eliminating f (x) from Eqs. (f) and
                    (h) and solving for f (x) yields
                                         f (x + 2h) − 2f (x + h) + 2f (x − h) − f (x − 2h)
                               f (x) =                                                     + O(h2 )          (5.3)
                                                                2h3
P1: PHB

CUUS884-Kiusalaas    CUUS884-05         978 0 521 19132 6                                      December 16, 2009    15:4




               179     5.2 Finite Difference Approximations


                                                     f (x − 2h)    f (x − h)   f (x)   f (x + h)     f (x + 2h)
                                   2hf (x)                              −1       0            1
                                   h2 f (x)                              1      −2            1
                                   2h3 f (x)                 −1          2       0           −2              1
                                   h4 f (4) (x)               1         −4       6           −4              1

                                  Table 5.1 Coefficients of central finite difference approximations
                                  of O(h2 )


                      The approximation
                                         f (x + 2h) − 4f (x + h) + 6f (x) − 4f (x − h) + f (x − 2h)
                          f (4) (x) =                                                               + O(h2 )       (5.4)
                                                                    h4
                      is available from Eqs. (e) and (g) after eliminating f (x). Table 5.1 summarizes the
                      results.


                      First Noncentral Finite Difference Approximations
                      Central finite difference approximations are not always usable. For example, con-
                      sider the situation where the function is given at the n discrete points x0 , x1 , . . . , xn .
                      Because central differences use values of the function on each side of x, we would
                      be unable to compute the derivatives at x0 and xn . Clearly, there is a need for finite
                      difference expressions that require evaluations of the function on only one side of x.
                      These expressions are called forward and backward finite difference approximations.
                           Noncentral finite differences can also be obtained from Eqs. (a)–(h). Solving Eq.
                      (a) for f (x), we get
                                                f (x + h) − f (x) h        h2         h3 (4)
                                    f (x) =                      − f (x) −    f (x) −    f (x) − · · ·
                                                        h         2        6          4!
                      Keeping only the first term on the right-hand side leads to the first forward difference
                      approximation
                                                                  f (x + h) − f (x)
                                                        f (x) =                     + O(h)                         (5.5)
                                                                          h
                      Similarly, Eq. (b) yields the first backward difference approximation
                                                                  f (x) − f (x − h)
                                                        f (x) =                     + O(h)                         (5.6)
                                                                          h
                      Note that the truncation error is now O(h), which is not as good as O(h2 ) in central
                      difference approximations.
                           We can derive the approximations for higher derivatives in the same manner. For
                      example, Eqs. (a) and (c) yield
                                                            f (x + 2h) − 2f (x + h) + f (x)
                                                  f (x) =                                   + O(h)                 (5.7)
                                                                          h2
                      The third and fourth derivatives can be derived in a similar fashion. The results are
                      shown in Tables 5.2a and 5.2b.
P1: PHB

CUUS884-Kiusalaas     CUUS884-05         978 0 521 19132 6                                       December 16, 2009      15:4




           180      Numerical Differentiation


                                                f (x)    f (x + h)   f (x + 2h)   f (x + 3h)    f (x + 4h)
                                  hf (x)         −1            1
                                 h2 f (x)         1           −2            1
                                 h3 f (x)        −1            3           −3              1
                                 h4 f (4) (x)     1           −4            6             −4               1

                             Table 5.2a Coefficients of forward finite difference approxima-
                             tions of O(h)

                                                f (x − 4h)    f (x − 3h)    f (x − 2h)    f (x − h)   f (x)
                                  hf (x)                                                        −1         1
                                 h2 f (x)                                           1           −2         1
                                 h3 f (x)                            −1             3           −3         1
                                 h4 f (4) (x)             1          −4             6           −4         1

                             Table 5.2b Coefficients of backward finite difference approxima-
                             tions of O(h)


                    Second Noncentral Finite Difference Approximations
                    Finite difference approximations of O(h) are not popular, for reasons to be explained
                    shortly. The common practice is to use expressions of O(h2 ). To obtain noncentral
                    difference formulas of this order, we have to retain more terms in the Taylor series. As
                    an illustration, we derive the expression for f (x). We start with Eqs. (a) and (c), which
                    are
                                                                h2         h3         h4 (4)
                             f (x + h) = f (x) + hf (x) +          f (x) +    f (x) +    f (x) + · · ·
                                                                2          6          24
                                                                              4h3         2h4 (4)
                            f (x + 2h) = f (x) + 2hf (x) + 2h2 f (x) +            f (x) +    f (x) + · · ·
                                                                               3           3
                    We eliminate f (x) by multiplying the first equation by 4 and subtracting it from the
                    second equation. The result is
                                                                                         h4 (4)
                                    f (x + 2h) − 4f (x + h) = −3f (x) − 2hf (x) +          f (x) + · · ·
                                                                                         2
                    Therefore,
                                                −f (x + 2h) + 4f (x + h) − 3f (x) h2 (4)
                                      f (x) =                                    +   f (x) + · · ·
                                                              2h                   4
                    or


                                                      −f (x + 2h) + 4f (x + h) − 3f (x)
                                            f (x) =                                     + O(h2 )                (5.8)
                                                                    2h
                    Equation (5.8) is called the second forward finite difference approximation.
                        The derivation of finite difference approximations for higher derivatives involve
                    additional Taylor series. Thus the forward difference approximation for f (x) utilizes
P1: PHB

CUUS884-Kiusalaas    CUUS884-05      978 0 521 19132 6                                        December 16, 2009        15:4




               181     5.2 Finite Difference Approximations


                                         f (x)   f (x + h)      f (x + 2h)    f (x + 3h)    f (x + 4h)    f (x + 5h)
                           2hf (x)        −3               4            −1
                           h2 f (x)        2              −5             4            −1
                          2h3 f (x)       −5              18           −24            14           −3
                          h4 f (4) (x)     3             −14            26           −24           11             −2

                         Table 5.3a Coefficients of forward finite difference approximations of O(h2 )

                                         f (x − 5h)       f (x − 4h)    f (x − 3h)    f (x − 2h)   f (x − h)   f (x)
                           2hf (x)                                                            1           −4       3
                           h2 f (x)                                           −1              4           −5       2
                          2h3 f (x)                               3          −14             24          −18       5
                          h4 f (4) (x)           −2              11          −24             26          −14       3

                         Table 5.3b Coefficients of backward finite difference approximations of O(h2 )


                      series for f (x + h), f (x + 2h), and f (x + 3h); the approximation for f (x) involves
                      Taylor expansions for f (x + h), f (x + 2h), f (x + 3h), f (x + 4h), and so on. As you can
                      see, the computations for high-order derivatives can become rather tedious. The re-
                      sults for both the forward and backward finite differences are summarized in Tables
                      5.3a and 5.3b.


                      Errors in Finite Difference Approximations
                      Observe that in all finite difference expressions, the sum of the coefficients is zero.
                      The effect on the roundoff error can be profound. If h is very small, the values of
                      f (x), f (x ± h), f (x ± 2h), and so forth will be approximately equal. When they are
                      multiplied by the coefficients and added, several significant figures can be lost. On
                      the other hand, we cannot make h too large, because then the truncation error would
                      become excessive. This unfortunate situation has no remedy, but we can obtain some
                      relief by taking the following precautions:

                        • Use double-precision arithmetic.
                        • Employ finite difference formulas that are accurate to at least O(h2 ).

                           To illustrate the errors, let us compute the second derivative of f (x) = e−x at
                      x = 1 from the central difference formula, Eq. (5.2). We carry out the calculations
                      with six- and eight-digit precision, using different values of h. The results, shown in
                      Table 5.4, should be compared with f (1) = e−1 = 0.367 879 44.
                           In the six-digit computations, the optimal value of h is 0.08, yielding a result ac-
                      curate to three significant figures. Hence, three significant figures are lost because of a
                      combination of truncation and roundoff errors. Above optimal h, the dominant error
                      is due to truncation; below it, the roundoff error becomes pronounced. The best re-
                      sult obtained with the eight-digit computation is accurate to four significant figures.
P1: PHB

CUUS884-Kiusalaas     CUUS884-05     978 0 521 19132 6                                          December 16, 2009         15:4




           182      Numerical Differentiation


                                           h        6-digit precision       8-digit precision
                                          0.64           0.380 610               0.380 609 11
                                          0.32           0.371 035               0.371 029 39
                                          0.16           0.368 711               0.368 664 84
                                          0.08           0.368 281               0.368 076 56
                                          0.04            0.368 75               0.367 831 25
                                          0.02              0.37                    0.3679
                                          0.01              0.38                    0.3679
                                          0.005             0.40                    0.3676
                                         0.0025             0.48                    0.3680
                                        0.00125             1.28                    0.3712

                                      Table 5.4 (e−x ) at x = 1 from central finite differ-
                                      ence approximation


                    Because the extra precision decreases the roundoff error, the optimal h is smaller
                    (about 0.02) than in the six-figure calculations.



          5.3       Richardson Extrapolation

                    Richardson extrapolation is a simple method for boosting the accuracy of certain nu-
                    merical procedures, including finite difference approximations (we also use it later in
                    other applications).
                         Suppose that we have an approximate means of computing some quantity G.
                    Moreover, assume that the result depends on a parameter h. Denoting the approxi-
                    mation by g(h), we have G = g(h) + E (h), where E (h) represents the error. Richard-
                    son extrapolation can remove the error, provided that it has the form E (h) = chp , c
                    and p being constants. We start by computing g(h) with some value of h, say, h = h1 .
                    In that case we have
                                                                             p
                                                         G = g(h1 ) + ch1                                           (i)

                    Then we repeat the calculation with h = h2 , so that
                                                                             p
                                                         G = g(h2 ) + ch2                                           (j)

                    Eliminating c and solving for G, Eqs. (i) and (j) yield
                                                         (h1 / h2 ) p g(h2 ) − g(h1 )
                                                   G=                                                          (5.8)
                                                               (h1 / h2 ) p − 1
                    which is the Richardson extrapolation formula. It is common practice to use h2 =
                    h1 /2, in which case Eq. (5.8) becomes
                                                           2 p g(h1 /2) − g(h1 )
                                                     G=                                                        (5.9)
                                                                  2p − 1
P1: PHB

CUUS884-Kiusalaas    CUUS884-05    978 0 521 19132 6                                        December 16, 2009   15:4




               183     5.3 Richardson Extrapolation

                           Let us illustrate Richardson extrapolation by applying it to the finite difference
                      approximation of (e−x ) at x = 1. We work with six-digit precision and utilize the re-
                      sults in Table 5.4. Because the extrapolation works only on the truncation error, we
                      must confine h to values that produce negligible roundoff. In Table 5.4 we have

                                                 g(0.64) = 0.380 610       g(0.32) = 0.371 035

                      The truncation error in central difference approximation is E (h) = O(h2 ) = c1 h2 +
                      c2 h4 + c3 h6 + . . .. Therefore, we can eliminate the first (dominant) error term if we
                      substitute p = 2 and h1 = 0.64 in Eq. (5.9). The result is

                                       22 g(0.32) − g(0.64)   4(0.371 035) − 0.380 610
                                  G=                        =                          = 0. 367 84 3
                                              2 −1
                                               2                         3
                      which is an approximation of (e−x ) with the error O(h4 ). Note that it is as accurate as
                      the best result obtained with eight-digit computations in Table 5.4.

                      EXAMPLE 5.1
                      Given the evenly spaced data points

                                           x           0        0.1        0.2       0.3      0.4
                                         f (x)      0.0000    0.0819     0.1341    0.1646   0.1797

                      compute f (x) and f (x) at x = 0 and 0.2 using finite difference approximations of
                      O(h2 ).

                      Solution We use finite difference approximations of O(h2 ). From the forward differ-
                      ence tables Table 5.3a, we get


                                       −3f (0) + 4f (0.1) − f (0.2)   −3(0) + 4(0.0819) − 0.1341
                            f (0) =                                 =                            = 0.967
                                                 2(0.1)                           0.2

                                                   2f (0) − 5f (0.1) + 4f (0.2) − f (0.3)
                                       f (0) =
                                                                   (0.1)2
                                                   2(0) − 5(0.0819) + 4(0.1341) − 0.1646
                                             =                                           = −3.77
                                                                   (0.1)2
                          The central difference approximations in Table 5.1 yield
                                                   −f (0.1) + f (0.3)   −0.0819 + 0.1646
                                       f (0.2) =                      =                  = 0.4135
                                                        2(0.1)                0.2

                                        f (0.1) − 2f (0.2) + f (0.3)   0.0819 − 2(0.1341) + 0.1646
                           f (0.2) =                                 =                             = −2.17
                                                   (0.1)2                         (0.1)2
                      EXAMPLE 5.2
                      Use the data in Example 5.1 to compute f (0) as accurately as you can.

                      Solution One solution is to apply Richardson extrapolation to finite difference ap-
                      proximations. We start with two forward difference approximations of O(h2 ) for f (0):
P1: PHB

CUUS884-Kiusalaas     CUUS884-05       978 0 521 19132 6                                          December 16, 2009   15:4




           184      Numerical Differentiation

                    one using h = 0.2 and the other one h = 0.1. Referring to the formulas of O(h2 ) in Ta-
                    ble 5.3a, we get
                                     −3f (0) + 4f (0.2) − f (0.4)   3(0) + 4(0.1341) − 0.1797
                          g(0.2) =                                =                           = 0.8918
                                               2(0.2)                           0.4

                                     −3f (0) + 4f (0.1) − f (0.2)   −3(0) + 4(0.0819) − 0.1341
                         g(0.1) =                                 =                            = 0.9675
                                               2(0.1)                           0.2
                    Recall that the error in both approximations is of the form E (h) = c1 h2 + c2 h4 +
                    c3 h6 + . . .. We can now use Richardson extrapolation, Eq. (5.9), to eliminate the dom-
                    inant error term. With p = 2 we obtain

                                                22 g(0.1) − g(0.2)   4(0.9675) − 0.8918
                                f (0) ≈ G =                        =                    = 0.9927
                                                      22 − 1                 3
                    which is a finite difference approximation of O(h4˙).

                    EXAMPLE 5.3


                                                                     b             C
                                                       B                   β

                                                                                         c
                                                  a

                                                       α
                                            A                                                 D
                                                                    d

                        The linkage shown has the dimensions a = 100 mm, b = 120 mm, c = 150 mm,
                    and d = 180 mm. It can be shown by geometry that the relationship between the
                    angles α and β is
                                                               2                         2
                                       d − a cos α − b cos β       + a sin α + b sin β       − c2 = 0

                    For a given value of α, we can solve this transcendental equation for β by one of the
                    root-finding methods in Chapter 2. This was done with α = 0◦ , 5◦ , 10◦ , . . . , 30◦ , the
                    results being

                          α (deg)         0         5        10            15       20           25        30
                          β (rad)      1.6595    1.5434    1.4186        1.2925   1.1712       1.0585    0.9561

                    If link A B rotates with the constant angular velocity of 25 rad/s, use finite difference
                    approximations of O(h2 ) to tabulate the angular velocity dβ/dt of link BC against α.

                    Solution The angular speed of BC is
                                                      dβ   dβ dα      dβ
                                                         =       = 25    rad/s
                                                      dt   dα dt      dα
P1: PHB

CUUS884-Kiusalaas    CUUS884-05         978 0 521 19132 6                                            December 16, 2009   15:4




               185     5.4 Derivatives by Interpolation

                      where dβ/dα can be computed from finite difference approximations using the data
                      in the table. Forward and backward differences of O(h2 ) are used at the endpoints,
                      central differences elsewhere. Note that the increment of α is
                                                                   π
                                                 h = 5 deg            rad / deg = 0.087 266 rad
                                                                  180
                      The computations yield
                                           −3β(0◦ ) + 4β(5◦ ) − β(10◦ )      −3(1.6595) + 4(1.5434) − 1.4186
                             ˙ ◦ ) = 25
                             β(0                                        = 25
                                                       2h                             2 (0.087 266)
                                    = −32.01 rad/s
                                           β(10◦ ) − β(0◦ )      1.4186 − 1.6595
                             ˙ ◦ ) = 25
                             β(5                            = 25                 = −34.51 rad/s
                                                 2h                2(0.087 266)
                                        and so forth.

                      The complete set of results is

                             α (deg)        0           5            10           15           20          25       30
                           β˙ (rad/s)     −32.01      −34.51       −35.94       −35.44       −33.52      −30.81   −27.86



              5.4     Derivatives by Interpolation

                      If f (x) is given as a set of discrete data points, interpolation can be a very effective
                      means of computing its derivatives. The idea is to approximate the derivative of f (x)
                      by the derivative of the interpolant. This method is particularly useful if the data
                      points are located at uneven intervals of x, when the finite difference approximations
                      listed in the last section are not applicable.1


                      Polynomial Interpolant
                      The idea here is simple: fit the polynomial of degree n

                                                    Pn−1 (x) = a 0 + a 1 x + a 2 x 2 + · · · + a n x n

                      through n + 1 data points and then evaluate its derivatives at the given x. As pointed
                      out in Section 3.2, it is generally advisable to limit the degree of the polynomial to
                      less than 6 in order to avoid spurious oscillations of the interpolant. Because these
                      oscillations are magnified with each differentiation, their effect can devastating. In
                      view of this limitation, the interpolation is usually a local one, involving no more than
                      a few nearest-neighbor data points.
                           For evenly spaced data points, polynomial interpolation and finite difference
                      approximations produce identical results. In fact, the finite difference formulas are
                      equivalent to polynomial interpolation.

                      1   It is possible to derive finite difference approximations for unevenly spaced data, but they would
                          not be as accurate as the formulas derived in Section 5.2.
P1: PHB

CUUS884-Kiusalaas     CUUS884-05         978 0 521 19132 6                                          December 16, 2009      15:4




           186      Numerical Differentiation

                         Several methods of polynomial interpolation were introduced in Section 3.2. Un-
                    fortunately, none of them are suited for the computation of derivatives of the inter-
                    polant. The method that we need is one that determines the coefficients a 0 , a 1 , . . . , a n
                    of the polynomial. There is only one such method discussed in Chapter 3: the least-
                    squares fit. Although this method is designed mainly for smoothing of data, it will
                    carry out interpolation if we use m = n in Eq. (3.22) – recall that m is the degree of the
                    interpolating polynomial and n + 1 represents the number of data points to be fitted.
                    If the data contains noise, then the least-squares fit should be used in the smoothing
                    mode, that is, with m < n. After the coefficients of the polynomial have been found,
                    the polynomial and its first two derivatives can be evaluated efficiently by the func-
                    tion evalPoly listed in Section 4.7.


                    Cubic Spline Interpolant
                    Because of to its stiffness, the cubic spline is a good global interpolant; moreover, it
                    is easy to differentiate. The first step is to determine the second derivatives ki of the
                    spline at the knots by solving Eqs. (3.11). This can be done with the function curva-
                    tures in the module cubicSpline listed in Section 3.3. The first and second deriva-
                    tives are then computed from

                                                     ki     3(x − xi+1 )2
                                     fi,i+1 (x) =                         − (xi − xi+1 )
                                                     6        xi − xi+1
                                                          ki+1   3(x − xi )2                  yi − yi+1
                                                    −                        − (xi − xi+1 ) +                     (5.10)
                                                           6      xi − xi+1                   xi − xi+1

                                                                   x − xi+1          x − xi
                                                 fi,i+1 (x) = ki             − ki+1                               (5.11)
                                                                   xi − xi+1        xi − xi+1
                    which are obtained by differentiation of Eq. (3.10).

                    EXAMPLE 5.4
                    Given the data

                                   x          1.5           1.9        2.1       2.4         2.6       3.1
                                 f (x)      1.0628        1.3961     1.5432    1.7349      1.8423    2.0397

                    compute f (2) and f (2) using (1) polynomial interpolation over three nearest-
                    neighbor points and (2) the natural cubic spline interpolant spanning all the data
                    points.

                    Solution of Part (1) The interpolant is P2 (x) = a 0 + a 1 x + a 2 x 2 passing through the
                    points at x = 1.9, 2.1, and 2.4. The normal equations, Eqs. (3.22), of the least-squares
                    fit are
                                          ⎡                      ⎤⎡ ⎤ ⎡                 ⎤
                                              n        xi    xi2     a0            yi
                                          ⎢                      ⎥⎢ ⎥ ⎢                 ⎥
                                          ⎣ xi         xi2   xi3 ⎦ ⎣ a 1 ⎦ = ⎣    yi xi ⎦
                                               xi2     xi3   xi4     a2          yi xi2
P1: PHB

CUUS884-Kiusalaas    CUUS884-05    978 0 521 19132 6                                      December 16, 2009       15:4




               187     5.4 Derivatives by Interpolation

                      After substituting the data, we get
                                         ⎡                             ⎤⎡ ⎤ ⎡               ⎤
                                             3      6.4         13.78     a0         4.6742
                                         ⎢                             ⎥⎢ ⎥ ⎢               ⎥
                                         ⎣ 6.4     13.78       29.944 ⎦ ⎣ a 1 ⎦ = ⎣ 10.0571 ⎦
                                           13.78 29.944        65.6578    a2        21.8385
                                                                         T
                      which yields a = −0.7714 1.5075 −0.1930 .
                           The derivatives of the interpolant are P2 (x) = a 1 + 2a 2 x and P2 (x) = 2a 2 . There-
                      fore,

                                         f (2) ≈ P2 (2) = 1.5075 + 2(−0.1930)(2) = 0.7355

                                                 f (2) ≈ P2 (2) = 2(−0.1930) = −0.3860

                      Solution of Part (2) We must first determine the second derivatives ki of the spline
                      at its knots, after which the derivatives of f (x) can be computed from Eqs. (5.10) and
                      (5.11). The first part can be carried out by the following small program:

                      #!/usr/bin/python
                      ## example5_4
                      from cubicSpline import curvatures
                      from LUdecomp3 import *
                      from numpy import array


                      xData = array([1.5, 1.9, 2.1, 2.4, 2.6, 3.1])
                      yData = array([1.0628, 1.3961, 1.5432, 1.7349, 1.8423, 2.0397])
                      print curvatures(xData,yData)
                      raw_input("Press return to exit")

                          The output of the program, consisting of k 0 to k 5 , is

                      [ 0.     -0.4258431 -0.37744139 -0.38796663 -0.55400477                       0.        ]
                      Press return to exit

                          Because x = 2 lies between knots 1 and 2, we must use Eqs. (5.10) and (5.11) with
                      i = 1. This yields


                                                       k1   3(x − x2 )2
                                  f (2) ≈ f1,2 (2) =                    − (x1 − x2 )
                                                       6     x1 − x2
                                                k2   3(x − x1 )2                y1 − y2
                                            −                    − (x1 − x2 ) +
                                                6     x1 − x2                   x1 − x2
                                            (−0.4258) 3(2 − 2.1)2
                                        =                         − (−0.2)
                                                6       (−0.2)
                                                (−0.3774) 3(2 − 1.9)2            1.3961 − 1.5432
                                            −                         − (−0.2) +
                                                    6       (−0.2)                    (−0.2)
                                        = 0.7351
P1: PHB

CUUS884-Kiusalaas     CUUS884-05      978 0 521 19132 6                                     December 16, 2009       15:4




           188      Numerical Differentiation

                                                            x − x2       x − x1
                                   f (2) ≈ f1,2 (2) = k 1           − k2
                                                            x1 − x2      x1 − x2
                                                          2 − 2.1             2 − 1.9
                                         = (−0.4258)              − (−0.3774)         = −0. 4016
                                                          (−0.2)              (−0.2)
                    Note that the solutions for f (2) in parts (1) and (2) differ only in the fourth signif-
                    icant figure, but the values of f (2) are much further apart. This is not unexpected,
                    considering the general rule: The higher the order of the derivative, the lower the pre-
                    cision with which it can be computed. It is impossible to tell which of the two results
                    is better without knowing the expression for f (x). In this particular problem, the data
                    points fall on the curve f (x) = x 2 e−x/2 , so that the “true” values of the derivatives are
                    f (2) = 0.7358 and f (2) = −0.3679.

                    EXAMPLE 5.5
                    Determine f (0) and f (1) from the following noisy data:

                                              x        0           0.2      0.4      0.6
                                            f (x)   1.9934       2.1465   2.2129   2.1790
                                              x       0.8          1.0      1.2      1.4
                                            f (x)   2.0683       1.9448   1.7655   1.5891

                    Solution We used the program listed in Example 3.10 to find the best polynomial fit
                    (in the least-squares sense) to the data. The program was run three times with the
                    following results:

                    Degree of polynomial ==> 2
                    Coefficients are:
                    [ 2.0261875        0.64703869 -0.70239583]
                    Std. deviation = 0.0360968935809


                    Degree of polynomial ==> 3
                    Coefficients are:
                    [ 1.99215          1.09276786 -1.55333333              0.40520833]
                    Std. deviation = 0.0082604082973


                    Degree of polynomial ==> 4
                    Coefficients are:
                    [ 1.99185568       1.10282373 -1.59056108              0.44812973 -0.01532907]
                    Std. deviation = 0.00951925073521


                    Degree of polynomial ==>
                    Finished. Press return to exit


                         Based on standard deviation, the cubic seems to be the best candidate for the
                    interpolant. Before accepting the result, we compare the plots of the data points and
                    the interpolant – see the figure. The fit does appear to be satisfactory.
P1: PHB

CUUS884-Kiusalaas    CUUS884-05     978 0 521 19132 6                                             December 16, 2009   15:4




               189     5.4 Derivatives by Interpolation

                                           2.3
                                           2.2
                                           2.1
                                           2.0



                                    f(x)
                                           1.9
                                           1.8
                                           1.7
                                           1.6
                                           1.5
                                            0.00   0.20      0.40     0.60       0.80     1.00    1.20   1.40
                                                                             x


                          Approximating f (x) by the interpolant, we have

                                                        f (x) ≈ a 0 + a 1 x + a 2 x 2 + a 3 x 3

                      so that

                                                          f (x) ≈ a 1 + 2a 2 x + 3a 3 x 2

                      Therefore,

                                  f (0) ≈ a 1 = 1.093

                                  f (1) = a 1 + 2a 2 + 3a 3 = 1.093 + 2(−1.553) + 3(0.405) = −0. 798

                          In general, derivatives obtained from noisy data are at best rough approxima-
                      tions. In this problem, the data represents f (x) = (x + 2)/ cosh x with added random
                      noise. Thus, f (x) = 1 − (x + 2) tanh x / cosh x, so that the “correct” derivatives are
                      f (0) = 1.000 and f (1) = −0.833.

                      PROBLEM SET 5.1

                       1. Given the values of f (x) at the points x, x − h1 , and x + h2 , where h1 = h2 , de-
                          termine the finite difference approximation for f (x). What is the order of the
                          truncation error?
                       2. Given the first backward finite difference approximations for f (x) and f (x), de-
                          rive the first backward finite difference approximation for f (x) using the oper-
                          ation f (x) = f (x) .
                       3. Derive the central difference approximation for f (x) accurate to O(h4 ) by apply-
                          ing Richardson extrapolation to the central difference approximation of O(h2 ).
                       4. Derive the second forward finite difference approximation for f (x) from the
                          Taylor series.
                       5. Derive the first central difference approximation for f (4) (x) from the Taylor series.
P1: PHB

CUUS884-Kiusalaas    CUUS884-05           978 0 521 19132 6                                              December 16, 2009   15:4




           190      Numerical Differentiation

                     6. Use finite difference approximations of O(h2 ) to compute f (2.36) and f (2.36)
                        from the data
                                                 x           2.36         2.37         2.38        2.39
                                               f (x)       0.85866      0.86289      0.86710     0.87129

                     7. Estimate f (1) and f (1) from the following data:

                                                         x         0.97          1.00       1.05
                                                       f (x)     0.85040       0.84147    0.82612

                     8. Given the data
                                     x          0.84             0.92           1.00           1.08          1.16
                                   f (x)      0.431711         0.398519       0.367879       0.339596      0.313486

                        calculate f (1) as accurately as you can.
                     9. Use the data in the table to compute f (0.2) as accurately as possible.

                                    x            0                0.1            0.2            0.3           0.4
                                  f (x)      0.000 000         0.078 348      0.138 910      0.192 916     0.244 981

                    10. Using five significant figures in the computations, determine d(sin x)/dx at x =
                        0.8 from (a) the first forward difference approximation and (b) the first central
                        difference approximation. In each case, use h that gives the most accurate result
                        (this requires experimentation).
                    11.   Use polynomial interpolation to compute f and f at x = 0, using the data

                                                    x           −2.2         −0.3     0.8       1.9
                                                  f (x)        15.180       10.962   1.920     −2.040

                          Given that f (x) = x 3 − 0. 3x 2 − 8. 56x + 8. 448, gauge the accuracy of the result.
                    12.

                                                          B
                                                                             2.5
                                                 R                              R

                                                       θ                x
                                           A                                                        C


                          The crank A B of length R = 90 mm is rotating at constant angular speed of
                          dθ /dt = 5000 rev/min. The position of the piston C can be shown to vary with
                          the angle θ as

                                                           x = R cos θ +         2.52 − sin2 θ

                          Write a program that computes the acceleration of the piston at θ = 0◦ , 5◦ ,
                          10◦ , . . ., 180◦ by numerical differentiation.
P1: PHB

CUUS884-Kiusalaas    CUUS884-05     978 0 521 19132 6                                                December 16, 2009   15:4




               191     5.4 Derivatives by Interpolation

                      13.                                                                         v
                                                                                                           γ
                                                                                   C




                                                                                   y



                                                 α                  β
                                        A               B
                                                  a
                                                                x

                            The radar stations A and B, separated by the distance a = 500 m, track the plane
                            C by recording the angles α and β at 1-second intervals. If three successive read-
                            ings are

                                                        t (s)         9         10            11
                                                          α         54.80◦    54.06◦        53.34◦
                                                          β         65.59◦    64.59◦        63.62◦

                            calculate the speed v of the plane and the climb angle γ at t = 10 s. The coordi-
                            nates of the plane can be shown to be
                                                            tan β                           tan α tan β
                                                x =a                              y =a
                                                        tan β − tan α                      tan β − tan α
                      14.

                                                                              20

                                                                        D
                                                                             70
                                                                                       β
                                                                                           C
                                                 Dimensions                                 190
                                                 in mm
                                                                             0
                                                                            19




                                                                      α
                                                                                  θA
                                                                B            60
P1: PHB

CUUS884-Kiusalaas    CUUS884-05     978 0 521 19132 6                                       December 16, 2009   15:4




           192      Numerical Differentiation

                        Geometric analysis of the linkage shown resulted in the following table relating
                        the angles θ and β:

                                  θ (deg)      0          30        60      90       120      150
                                  β (deg)    59.96       56.42     44.10   25.72    −0.27    −34.29

                        Assuming that member A B of the linkage rotates with the constant angular ve-
                        locity dθ/dt = 1 rad/s, compute dβ/dt in rad/s at the tabulated values of θ. Use
                        cubic spline interpolation.
                    15.   The relationship between stress σ and strain ε of some biological materials in
                        uniaxial tension is
                                                              dσ
                                                                 = a + bσ
                                                              dε
                        where a and b are constants. The following table gives the results of a tension
                        test on such a material:
                                                        Strain ε   Stress σ (MPa)
                                                            0             0
                                                          0.05          0.252
                                                          0.10          0.531
                                                          0.15          0.840
                                                          0.20          1.184
                                                          0.25          1.558
                                                          0.30          1.975
                                                          0.35          2.444
                                                          0.40          2.943
                                                          0.45          3.500
                                                          0.50          4.115
                        Write a program that plots the tangent modulus dσ /dε versus σ and computes
                        the parameters a and b by linear regression.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                          December 16, 2009   15:4




              6       Numerical Integration




                                                        b
                                       Compute         a    f (x) dx, where f (x) is a given function




              6.1     Introduction

                      Numerical integration, also known as quadrature, is intrinsically a much more accu-
                      rate procedure than numerical differentiation. Quadrature approximates the definite
                      integral

                                                                        b
                                                                            f (x) dx
                                                                    a


                      by the sum

                                                                            n
                                                                 I=             A i f (xi )
                                                                        i=0


                      where the nodal abscissas xi and weights A i depend on the particular rule used for the
                      quadrature. All rules of quadrature are derived from polynomial interpolation of the
                      integrand. Therefore, they work best if f (x) can be approximated by a polynomial.
                          Methods of numerical integration can be divided into two groups: Newton–
                      Cotes formulas and Gaussian quadrature. Newton–Cotes formulas are characterized
                      by equally spaced abscissas and include well-known methods such as the trape-
                      zoidal rule and Simpson’s rule. They are most useful if f (x) has already been com-
                      puted at equal intervals or can be computed at low cost. Because Newton–Cotes
                      formulas are based on local interpolation, they require only a piecewise fit to a
                      polynomial.
                          In Gaussian quadrature, the locations of the abscissas are chosen to yield the best
                      possible accuracy. Because Gaussian quadrature requires fewer evaluations of the in-
                      tegrand for a given level of precision, it is popular in cases where f (x) is expensive to
                      evaluate. Another advantage of Gaussian quadrature is ability to handle integrable

               193
P1: PHB

CUUS884-Kiusalaas     CUUS884-06      978 0 521 19132 6                                                                       December 16, 2009      15:4




           194      Numerical Integration

                    singularities, enabling us to evaluate expressions such as
                                                                            1
                                                                                        g(x)
                                                                                √                      dx
                                                                        0               1 − x2
                    provided that g(x) is a well-behaved function.



          6.2       Newton–Cotes Formulas

                    Consider the definite integral
                                                                                    b
                                                                                        f (x) dx                                             (6.1)
                                                                                a

                    We divide the range of integration (a, b) into n equal intervals of length h = (b − a)/n,
                    as shown in Fig. 6.1, and denote the abscissas of the resulting nodes by x0 , x1 , . . . , xn .
                    Next, we approximate f (x) by a polynomial of degree n that intersects all the nodes.
                    Lagrange’s form of this polynomial, Eq. (3.1a), is
                                                                                        n
                                                            Pn (x) =                          f (xi ) i (x)
                                                                                        i=0

                    where i (x) are the cardinal functions defined in Eq. (3.1b). Therefore, an approxima-
                    tion to the integral in Eq. (6.1) is
                                               b                    n                              b                 n
                                     I=            Pn (x)dx =                       f (xi )             i (x)dx =          A i f (xi )      (6.2a)
                                           a                    i=0                            a                     i=0

                    where
                                                                b
                                                     Ai =           i (x)dx,                   i = 0, 1, . . . , n                          (6.2b)
                                                            a

                    Equations (6.2) are the Newton–Cotes formulas. Classical examples of these formulas
                    are the trapezoidal rule (n = 1), Simpson’s rule (n = 2), and 3/8 Simpson’s rule (n = 3).
                    The most important of these is the trapezoidal rule. It can be combined with Richard-
                    son extrapolation into an efficient algorithm known as Romberg integration, which
                    makes the other classical rules somewhat redundant.


                                    f(x)                                                                      Pn(x)

                                                                                        h


                                                     x0                                                                            x
                                                            x1              x2                x3               xn -1 xn
                                                     a                                                               b
                                    Figure 6.1. Polynomial approximation of f (x).
P1: PHB

CUUS884-Kiusalaas    CUUS884-06        978 0 521 19132 6                                                                 December 16, 2009    15:4




               195     6.2 Newton–Cotes Formulas


                      f(x)             E



                                                Area =I                                        Figure 6.2. Trapezoidal rule.


                                                           h
                                                                                          x
                              x0 = a                                         x1 = b

                      Trapezoidal Rule
                      If n = 1 (one panel), as illustrated in Fig 6.2, we have                            0   = (x − x1 )/(x0 − x1 ) = −(x −
                      b)/ h. Therefore,
                                                                         b
                                                                 1                              1            h
                                                          A0 =               x − b dx =           (b − a)2 =
                                                                 h   a                         2h            2
                      Also,   1   = (x − x0 )/(x1 − x0 ) = (x − a)/ h, so that
                                                                         b
                                                                 1                              1            h
                                                          A1 =               (x − a) dx =         (b − a)2 =
                                                                 h   a                         2h            2
                      Substitution in Eq. (6.2a) yields


                                                                                                  h
                                                                         I = f (a) + f (b)                                                   (6.3)
                                                                                                  2
                      which is known as the trapezoidal rule. It represents the area of the trapezoid in
                      Fig. 6.2.
                           The error in the trapezoidal rule
                                                                                     b
                                                                         E=              f (x)dx − I
                                                                                 a

                      is the area of the region between f (x) and the straight-line interpolant, as indicated
                      in Figure 6.2. It can be obtained by integrating the interpolation error in Eq. (3.3):
                                                     b                                                            b
                                           1                                                   1
                                    E =                  (x − x0 )(x − x1 )f (ξ )dx =            f (ξ )               (x − a)(x − b)dx
                                           2!    a                                             2              a

                                                 1                    h3
                                       =−          (b − a)3 f (ξ ) = − f (ξ )                                                                (6.4)
                                                12                    12


                      Composite Trapezoidal Rule
                      In practice the trapezoidal rule is applied in a piecewise fashion. Figure 6.3 shows the
                      region (a, b) divided into n panels, each of width h. The function f (x) to be integrated
                      is approximated by a straight line in each panel. From the trapezoidal rule we obtain
P1: PHB

CUUS884-Kiusalaas       CUUS884-06       978 0 521 19132 6                                                      December 16, 2009         15:4




           196      Numerical Integration


                                     f (x)

                                                                                      Ii

                                                                                      h
                                                  x0                                                                   x
                                                             x1                  xi        x i+1            xn -1 xn
                                                  a                                                               b
                                     Figure 6.3. Composite trapezoidal rule.


                    for the approximate area of a typical (ith) panel
                                                                                                  h
                                                              Ii = f (xi ) + f (xi+1 )
                                                                                                  2
                                                                         b
                    Hence, the total area, representing                 a    f (x) dx, is

                                       n−1
                                                                                                                           h
                                  I=         Ii = f (x0 ) + 2f (x1 ) + 2f (x2 ) + . . . + 2f (xn−1 ) + f (xn )                 (6.5)
                                                                                                                           2
                                       i=0

                    which is the composite trapezoidal rule.
                       The truncation error in the area of a panel is – see Eq. (6.4)
                                                                                 h3
                                                                   Ei = −           f (ξ i )
                                                                                 12
                    where ξ i lies in (xi , xi+1 ). Hence, the truncation error in Eq. (6.5) is
                                                                  n−1                      n−1
                                                                                      h3
                                                         E=              Ei = −                  f (ξ i )                           (a)
                                                                                      12
                                                                  i=0                      i=0

                    But
                                                                   n−1
                                                                             f (ξ i ) = n f¯
                                                                   i=0

                    where f¯ is the arithmetic mean of the second derivatives. If f (x) is continuous,
                    there must be a point ξ in (a, b) at which f (ξ ) = f¯ , enabling us to write
                                                       n−1
                                                                                           b −a
                                                             f (ξ i ) = nf (ξ ) =               f (ξ )
                                                                                             h
                                                       i=0

                    Therefore, Eq. (a) becomes
                                                                             (b − a)h2
                                                               E =−                    f (ξ )                                  (6.6)
                                                                                12
                        It would be incorrect to conclude from Eq. (6.6) that E = ch2 (c being a constant),
                    because f (ξ ) is not entirely independent of h. A deeper analysis of the error1 shows
                    1   The analysis requires familiarity with the Euler-Maclaurin summation formula, which is covered
                        in advanced texts.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                           December 16, 2009     15:4




               197     6.2 Newton–Cotes Formulas

                      that if f (x) and its derivatives are finite in (a, b), then

                                                          E = c1 h2 + c2 h4 + c3 h6 + . . .                            (6.7)


                      Recursive Trapezoidal Rule
                      Let Ik be the integral evaluated with the composite trapezoidal rule using 2k−1 panels.
                      Note that if k is increased by 1, the number of panels is doubled. Using the notation

                                                                        H = b −a

                      Equation (6.5) yields the following results for k = 1, 2, and 3.
                         k = 1 (1 panel):
                                                                                      H
                                                                I1 = f (a) + f (b)                                     (6.8)
                                                                                      2
                          k = 2 (2 panels):
                                                                  H                H  1           H          H
                                    I2 = f (a) + 2f a +                 + f (b)      = I1 + f a +
                                                                  2                4  2           2          2
                          k = 3 (4 panels):
                                                            H                     H               3H             H
                               I3 = f (a) + 2f a +                  + 2f a +          + 2f a +         + f (b)
                                                            4                     2                4             8
                                      1            H                        3H        H
                                  =     I2 + f a +              +f a+
                                      2            4                         4        4
                          We can now see that for arbitrary k > 1 we have
                                                                 2k−2
                                            1        H                         (2i − 1)H
                                        Ik = Ik−1 + k−1                 f a+             , k = 2, 3, . . .            (6.9a)
                                            2      2                              2k−1
                                                                 i=1

                      which is the recursive trapezoidal rule. Observe that the summation contains only
                      the new nodes that were created when the number of panels was doubled. Therefore,
                      the computation of the sequence I1 , I2 , I3 , . . . , Ik from Eqs. (6.8) and (6.9) involves the
                      same amount of algebra as the calculation of Ik directly from Eq. (6.5). The advantage
                      of using the recursive trapezoidal rule is that it allows us to monitor convergence and
                      terminate the process when the difference between Ik−1 and Ik becomes sufficiently
                      small. A form of Eq. (6.9a) that is easier to remember is
                                                                    1
                                                          I (h) =     I (2h) + h      f (xnew )                       (6.9b)
                                                                    2
                      where h = H/n is the width of each panel.


                         trapezoid

                      The function trapezoid computes Ik (Inew), given Ik−1 (Iold) using Eqs. (6.8) and
                                                 b
                      (6.9). We can compute a f (x) dx by calling trapezoid with k = 1, 2, . . . until the
                      desired precision is attained.
P1: PHB

CUUS884-Kiusalaas     CUUS884-06       978 0 521 19132 6                                      December 16, 2009         15:4




           198      Numerical Integration

                    ## module trapezoid
                    ’’’ Inew = trapezoid(f,a,b,Iold,k).
                            Recursive trapezoidal rule:
                            Iold = Integral of f(x) from x = a to b computed by
                            trapezoidal rule with 2ˆ(k-1) panels.
                            Inew = Same integral computed with 2ˆk panels.
                    ’’’
                    def trapezoid(f,a,b,Iold,k):
                            if k == 1:Inew = (f(a) + f(b))*(b - a)/2.0
                            else:
                                n = 2**(k -2 )                 # Number of new points
                                h = (b - a)/n                  # Spacing of new points
                                x = a + h/2.0
                                sum = 0.0
                                for i in range(n):
                                    sum = sum + f(x)
                                    x = x + h
                                Inew = (Iold + h*sum)/2.0
                            return Inew




                    Simpson’s Rules
                    Simpson’s 1/3 rule can be obtained from the Newton–Cotes formulas with n = 2,
                    that is, by passing a parabolic interpolant through three adjacent nodes, as shown
                    in Fig.6.4. The area under the parabola, which represents an approximation of
                      b
                     a f (x) dx, is (see derivation in Example 6.1)

                                                                    a +b              h
                                                 I = f (a) + 4f             + f (b)                               (a)
                                                                      2               3

                        To obtain the composite Simpson’s 1/3 rule, the integration range (a, b) is divided
                    into n panels (n even) of width h = (b − a)/n each, as indicated in Fig. 6.5. Applying



                              Parabola
                    f (x)


                                                           ξ                Figure 6.4. Simpson’s 1/3 rule.


                                       h               h

                                               x1                       x
                              x0 = a                           x2 = b
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                                             December 16, 2009     15:4




               199     6.2 Newton–Cotes Formulas


                                     f (x )

                                                                                     h       h


                                                              x0                                                         x
                                                                                xi       xi+ 1 xi+ 2                xn
                                                              a                                                     b
                                     Figure 6.5. Composite Simpson’s 1/3 rule.


                      Eq. (a) to two adjacent panels, we have
                                                       xi+2
                                                                                                                    h
                                                              f (x) dx ≈ f (xi ) + 4f (xi+1 ) + f (xi+2 )                              (b)
                                                      xi                                                            3

                      Substituting Eq. (b) into
                                                  b                      xm                   n         xi+2
                                                      f (x)dx =               f (x) dx =                       f (x)dx
                                              a                         x0                 i=0,2,...   xi


                      yields
                                          b
                                              f (x) dx ≈ I = [f (x0 ) + 4f (x1 ) + 2f (x2 ) + 4f (x3 ) + . . .                       (6.10)
                                      a

                                                                                                                  h
                                                                   · · · + 2f (xn−2 ) + 4f (xn−1 ) + f (xn )]
                                                                                                                  3
                      The composite Simpson’s 1/3 rule in Eq. (6.10) is perhaps the best-known method of
                      numerical integration. Its reputation is somewhat undeserved, since the trapezoidal
                      rule is more robust, and Romberg integration is more efficient.
                          The error in the composite Simpson’s rule is

                                                                               (b − a)h4 (4)
                                                                       E=               f (ξ )                                       (6.11)
                                                                                  180
                      from which we conclude that Eq. (6.10) is exact if f (x) is a polynomial of degree 3 or
                      less.
                           Simpson’s 1/3 rule requires the number of panels n to be even. If this condition
                      is not satisfied, we can integrate over the first (or last) three panels with Simpson’s 3/8
                      rule:
                                                                                                               3h
                                                       I = f (x0 ) + 3f (x1 ) + 3f (x2 ) + f (x3 )                                   (6.12)
                                                                                                                8
                      and use Simpson’s 1/3 rule for the remaining panels. The error in Eq. (6.12) is of the
                      same order as in Eq. (6.10).

                      EXAMPLE 6.1
                      Derive Simpson’s 1/3 rule from the Newton–Cotes formulas.
P1: PHB

CUUS884-Kiusalaas     CUUS884-06     978 0 521 19132 6                                                       December 16, 2009   15:4




           200      Numerical Integration

                    Solution Referring to Figure 6.4, Simpson’s 1/3 rule uses three nodes located at x0 =
                    a, x1 = a + b /2, and x2 = b. The spacing of the nodes is h = (b − a)/2. The cardinal
                    functions of Lagrange’s three-point interpolation are – see Section 3.2


                                                 (x − x1 )(x − x2 )                             (x − x0 )(x − x2 )
                                    0 (x)   =                                     1 (x)   =
                                                (x0 − x1 )(x0 − x2 )                           (x1 − x0 )(x1 − x2 )


                                                                          (x − x0 )(x − x1 )
                                                             2 (x)   =
                                                                         (x2 − x0 )(x2 − x1 )

                    The integration of these functions is easier if we introduce the variable ξ with origin
                    at x1 . Then the coordinates of the nodes are ξ 0 = −h, ξ 1 = 0, ξ 2 = h, and Eq. (6.2b)
                                     b         h
                    becomes A i = a i (x) = −h i (ξ )dξ . Therefore,

                                                h
                                                    (ξ − 0)(ξ − h)       1                h
                                                                                                                 h
                                   A0 =                            dξ =                        (ξ 2 − hξ )dξ =
                                            −h        (−h)(−2h)         2h2               −h                     3

                                                h
                                                    (ξ + h)(ξ − h)        1                h
                                                                                                                 4h
                                   A1 =                            dξ = − 2                    (ξ 2 − h2 )dξ =
                                            −h         (h)(−h)           h                −h                      3

                                                h
                                                    (ξ + h)(ξ − 0)       1                h
                                                                                                                 h
                                   A2 =                            dξ =                        (ξ 2 + hξ )dξ =
                                            −h         (2h)(h)          2h2               −h                     3

                    Equation (6.2a) then yields

                                                    2
                                                                                     a +b                    h
                                        I=              A i f (xi ) = f (a) + 4f                   + f (b)
                                                                                       2                     3
                                                i=0


                    which is Simpson’s 1/3 rule.

                    EXAMPLE 6.2
                                               π
                    Evaluate the bounds on 0 sin(x) dx with the composite trapezoidal rule using (1)
                    eight panels and (2) 16 panels.

                    Solution of Part (1) With eight panels there are nine nodes spaced at h = π/8. The
                    abscissas of the nodes are xi = iπ /8, i = 0, 1, . . . , 8. From Eq. (6.5) we get

                                                                 7
                                                                            iπ                 π
                                        I = sin 0 + 2                 sin      + sin π            = 1.97423
                                                                             8                 16
                                                                i=1


                    The error is given by Eq. (6.6):

                                        (b − a)h2            (π − 0)(π /8)2              π3
                                 E =−             f (ξ ) = −                (− sin ξ ) =     sin ξ
                                           12                     12                     768

                    where 0 < ξ < π. Because we do not know the value of ξ , we cannot evaluate E , but
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                                   December 16, 2009   15:4




               201     6.2 Newton–Cotes Formulas

                      we can determine its bounds:
                                                π3                                    π3     π
                                      E min =       sin(0) = 0              E max =       sin = 0.040 37
                                                768                                   768    2
                                                  π
                      Therefore, I + E min <     0    sin(x) dx < I + E max , or
                                                                       π
                                                      1.974 23 <           sin(x) dx < 2.014 60
                                                                   0

                      The exact integral is, of course, 2.

                      Solution of Part (2) The new nodes created by doubling of the panels are located at
                      the midpoints of the old panels. Their abscissas are

                                          x j = π/16 + j π/8 = (1 + 2 j )π/16,               j = 0, 1, . . . , 7

                      Using the recursive trapezoidal rule in Eq. (6.9b), we get
                                                                       7
                                                 1.974 23   π                     (1 + 2 j )π
                                           I=             +                 sin               = 1. 993 58
                                                    2       16                        16
                                                                   j =0

                      and the bounds on the error become (note that E is quartered when h is halved)
                      E min = 0, E max = 0.040 37/4 = 0.010 09. Hence,
                                                                       π
                                                      1.993 58 <           sin(x) dx < 2.003 67
                                                                   0

                      EXAMPLE 6.3
                                2.5
                      Estimate 0 f (x) dx from the data

                                    x          0           0.5       1.0              1.5         2.0          2.5
                                  f (x)     1.5000       2.0000    2.0000           1.6364      1.2500       0.9565

                      Solution We use Simpson’s rules because they are more accurate than the trape-
                      zoidal rule. Because the number of panels is odd, we compute the integral over the
                      first three panels by Simpson’s 3/8 rule, and use the 1/3 rule for the last two panels:
                                                                                                   3(0.5)
                                             I = f (0) + 3f (0.5) + 3f (1.0) + f (1.5)
                                                                                                     8
                                                                                          0.5
                                                  + f (1.5) + 4f (2.0) + f (2.5)
                                                                                           3
                                                = 2.8381 + 1.2655 = 4.1036

                      EXAMPLE 6.4
                                                                      π √
                      Use the recursive trapezoidal rule to evaluate 0 x cos x dx to six decimal places.
                      How many panels are needed to achieve this result?

                      Solution The program listed here utilizes the function trapezoid.

                      #!/usr/bin/python
                      ## example6_4
                      from math import sqrt,cos,pi
                      from trapezoid import *
P1: PHB

CUUS884-Kiusalaas     CUUS884-06     978 0 521 19132 6                                                  December 16, 2009   15:4




           202      Numerical Integration



                    def f(x): return sqrt(x)*cos(x)


                    Iold = 0.0
                    for k in range(1,21):
                         Inew = trapezoid(f,0.0,pi,Iold,k)
                         if (k > 1) and (abs(Inew - Iold)) < 1.0e-6: break
                         Iold = Inew
                    print ’’Integral =’’,Inew
                    print ’’nPanels =’’,2**(k-1)
                    raw_input(’’\nPress return to exit’’))


                        The output from the program is:

                    Integral = -0.894831664853
                    nPanels = 32768

                                   π √
                         Hence, 0 x cos x dx = −0.894 832, requiring 32,768 panels. The slow conver-
                    gence is the result of all the derivatives of f (x) being singular at x = 0. Consequently,
                    the error does not behave as shown in Eq. (6.7): E = c1 h2 + c2 h4 + · · · , but is unpre-
                    dictable. Difficulties of this nature can often be remedied by a change in variable. In
                                                  √                        √
                    this case, we introduce t = x so that dt = dx/(2 x) = dx/(2t ), or dx = 2t dt . Thus,
                                                                              √
                                                   π   √                          π
                                                           x cos x dx =               2t 2 cos t 2 dt
                                               0                          0

                    Evaluation of the integral on the right-hand side was completed with 4096 panels.



          6.3       Romberg Integration

                    Romberg integration combines the trapezoidal rule with Richardson extrapolation
                    (see Section 5.3). Let us first introduce the notation

                                                                  Ri,1 = Ii
                                                                                                   b
                    where, as before, Ii represents the approximate value of a f (x)dx computed by the
                    recursive trapezoidal rule using 2i−1 panels. Recall that the error in this approxima-
                    tion is E = c1 h2 + c2 h4 + . . ., where

                                                                      b −a
                                                                 h=
                                                                       2i−1
                    is the width of a panel.
                         Romberg integration starts with the computation of R1,1 = I1 (one panel) and
                    R2,1 = I2 (two panels) from the trapezoidal rule. The leading error term c1 h2 is then
                    eliminated by Richardson extrapolation. Using p = 2 (the exponent in the leading
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                                     December 16, 2009      15:4




               203     6.3 Romberg Integration

                      error term) in Eq. (5.9) and denoting the result by R2,2 , we obtain

                                                           22 R2,1 − R1,1  4      1
                                                  R2,2 =                  = R2,1 − R1,1                                          (a)
                                                                22−1       3      3

                      It is convenient to store the results in an array of the form

                                                                       R1,1
                                                                       R2,1   R2,2

                          The next step is to calculate R3,1 = I3 (four panels) and repeat the Richardson
                      extrapolation with R2,1 and R3,1 , storing the result as R3,2 :

                                                                        4       1
                                                              R3,2 =      R3,1 − R2,1                                            (b)
                                                                        3       3

                      The elements of array R calculated so far are
                                                                   ⎡                 ⎤
                                                                     R1,1
                                                                   ⎢               ⎥
                                                                   ⎣ R2,1     R2,2 ⎦
                                                                     R3,1     R3,2

                      Both elements of the second column have an error of the form c2 h4 , which can also
                      be eliminated with Richardson extrapolation. Using p = 4 in Eq. (5.9), we get

                                                          24 R3,2 − R2,2   16         1
                                                 R3,3 =                  =    R3,2 −    R2,2                                     (c)
                                                               24−1        15        15

                      This result has an error of O(h6 ). The array has now expanded to
                                                              ⎡                          ⎤
                                                                R1,1
                                                              ⎢                          ⎥
                                                              ⎣ R2,1      R2,2           ⎦
                                                                R3,1      R3,2   R3,3

                          After another round of calculations we get
                                                          ⎡                                     ⎤
                                                            R1,1
                                                          ⎢R                                    ⎥
                                                          ⎢ 2,1        R2,2                     ⎥
                                                          ⎢                                     ⎥
                                                          ⎣ R3,1       R3.2   R3,3              ⎦
                                                            R4,1       R4,2   R4,3       R4,4

                      where the error in R4,4 is O(h8 ). Note that the most accurate estimate of the integral is
                      always the last diagonal term of the array. This process is continued until the differ-
                      ence between two successive diagonal terms becomes sufficiently small. The general
                      extrapolation formula used in this scheme is

                                                 4 j −1 Ri, j −1 − Ri−1, j −1
                                       Ri, j =                                , i > 1,          j = 2, 3, . . . , i          (6.13a)
                                                          4 j −1 − 1
P1: PHB

CUUS884-Kiusalaas     CUUS884-06     978 0 521 19132 6                                                    December 16, 2009      15:4




           204      Numerical Integration

                    A pictorial representation of Eq. (6.13a) is

                                                         Ri−1, j −1


                                                                            α                                          (6.13b)

                                                          Ri, j −1        → β → Ri, j

                    where the multipliers α and β depend on j in the following manner:

                                     j     2         3             4               5              6
                                     α    −1/3     −1/15         −1/63          −1/255         −1/1023                 (6.13c)
                                     β    4/3      16/15         64/63          256/255       1024/1023

                        The triangular array is convenient for hand computations, but computer imple-
                    mentation of the Romberg algorithm can be carried out within a one-dimensional
                    array R . After the first extrapolation – see Eq. (a) – R1,1 is never used again, so it can
                    be replaced with R2,2 . As a result, we have the array

                                                                 R1 = R2,2
                                                                 R2 = R2,1

                    In the second extrapolation round, defined by Eqs. (b) and (c), R3,2 overwrites R2,1
                    and R3,3 replaces R2,2 , so the array contains
                                                           ⎡           ⎤
                                                             R1 = R3,3
                                                           ⎢           ⎥
                                                           ⎣ R2 = R3,2 ⎦
                                                             R3 = R3,1

                    and so on. In this manner, R1 always contains the best current result. The extrapola-
                    tion formula for the kth round is
                                                4k− j R j +1 − R j
                                         Rj =                         ,     j = k − 1, k − 2, . . . , 1                 (6.14)
                                                   4k− j − 1


                      romberg

                    The algorithm for Romberg integration is implemented in the function romberg. It
                    returns the integral and the number of panels used. Richardson’s extrapolation is car-
                    ried out by the subfunction richardson.

                    ## module romberg
                    ’’’ I,nPanels = romberg(f,a,b,tol=1.0e-6).
                          Romberg intergration of f(x) from x = a to b.
                          Returns the integral and the number of panels used.
                    ’’’
                    from numpy import zeros
                    from trapezoid import *
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                              December 16, 2009   15:4




               205     6.3 Romberg Integration

                      def romberg(f,a,b,tol=1.0e-6):


                           def richardson(r,k):
                                  for j in range(k-1,0,-1):
                                      const = 4.0**(k-j)
                                      r[j] = (const*r[j+1] - r[j])/(const - 1.0)
                                  return r


                           r = zeros(21)
                           r[1] = trapezoid(f,a,b,0.0,1)
                           r_old = r[1]
                           for k in range(2,21):
                                  r[k] = trapezoid(f,a,b,r[k-1],k)
                                  r = richardson(r,k)
                                  if abs(r[1]-r_old) < tol*max(abs(r[1]),1.0):
                                      return r[1],2**(k-1)
                                  r_old = r[1]
                           print "Romberg quadrature did not converge"


                      EXAMPLE 6.5
                      Show that Rk,2 in Romberg integration is identical to the composite Simpson’s 1/3
                      rule in Eq. (6.10) with 2k−1 panels.

                      Solution Recall that in Romberg integration Rk,1 = Ik denoted the approximate in-
                      tegral obtained by the composite trapezoidal rule with n = 2k−1 panels. Denoting the
                      abscissas of the nodes by x0 , x1 , . . . , xn , we have from the composite trapezoidal rule
                      in Eq. (6.5)

                                                                       n−1
                                                                                                      h
                                             Rk,1 = Ik = f (x0 ) + 2         f (xi ) + f (xn )
                                                                                                      2
                                                                       i=1

                      When we halve the number of panels (panel width 2h), only the even-numbered ab-
                      scissas enter the composite trapezoidal rule, yielding
                                                        ⎡                         ⎤
                                                                           n−2
                                         Rk−1,1 = Ik−1 = ⎣ f (x0 ) + 2               f (xi ) + f (xn )⎦ h
                                                                         i=2,4,...


                      Applying Richardson extrapolation yields

                                          4         1
                                    Rk,2 =  Rk,1 − Rk−1,1
                                          3         3
                                          ⎡                                                                  ⎤
                                                        n−1                      n−2
                                            1         4               2                               1
                                        = ⎣ f (x0 ) +       f (xi ) +                     f (xi ) +     f (xn )⎦ h
                                            3         3               3                               3
                                                         i=1,3,...            i=2,4,...


                      which agrees with Eq. (6.10).
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                           December 16, 2009      15:4




           206      Numerical Integration

                    EXAMPLE 6.6
                                                                 π
                    Use Romberg integration to evaluate         0      f (x) dx, where f (x) = sin x. Work with four
                    decimal places.

                    Solution From the recursive trapezoidal rule in Eq. (6.9b) we get
                                             π
                           R1,1 = I (π ) =      f (0) + f (π ) = 0
                                             2
                                               1          π
                           R2,1 =   I (π /2) = I (π ) + f (π/2) = 1.5708
                                               2          2
                                               1            π
                           R3,1 =   I (π /4) = I (π /2) +       f (π/4) + f (3π /4) = 1.8961
                                               2             4
                                               1            π
                           R4,1 =   I (π /8) = I (π /4) +       f (π/8) + f (3π /8) + f (5π/8) + f (7π /8)
                                               2             8
                               =    1.9742

                    Using the extrapolation formulas in Eqs. (6.13), we can now construct the following
                    table:
                              ⎡                      ⎤ ⎡                                    ⎤
                                R1,1                          0
                              ⎢R                     ⎥ ⎢1.5708 2.0944                       ⎥
                              ⎢ 2,1 R2,2             ⎥ ⎢                                    ⎥
                              ⎢                      ⎥=⎢                                    ⎥
                              ⎣ R3,1 R3.2 R3,3       ⎦ ⎣1.8961 2.0046 1.9986                ⎦
                                R4,1 R4,2 R4,3 R4,4        1.9742 2.0003 2.0000 2.0000
                                                                                         π
                    It appears that the procedure has converged. Therefore,             0    sin x dx = R4,4 = 2.0000,
                    which is, of course, the correct result.

                    EXAMPLE 6.7                                √
                                                                   π
                    Use Romberg integration to evaluate        0       2x 2 cos x 2 dx and compare the results with
                    Example 6.4.

                    Solution

                    #!/usr/bin/python
                    ## example6_7
                    from math import cos,sqrt,pi
                    from romberg import *


                    def f(x): return 2.0*(x**2)*cos(x**2)


                    I,n = romberg(f,trapezoid,0,sqrt(pi))
                    print ’’Integral =’’,I
                    print ’’nPanels =’’,n
                    raw_input(’’\nPress return to exit’’)

                        The results of running the program are:

                    Integral = -0.894831469504
                    nPanels = 64
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                           December 16, 2009   15:4




               207     6.3 Romberg Integration

                          It is clear that Romberg integration is considerably more efficient than the trape-
                      zoidal rule – it required 64 panels as compared to 4096 panels for the trapezoidal rule
                      in Example 6.4.

                      PROBLEM SET 6.1
                                                                                 π /4
                       1. Use the recursive trapezoidal rule to evaluate 0 ln(1 + tan x)dx. Explain the
                          results.
                       2. The table shows the power P supplied to the driving wheels of a car as a function
                          of the speed v. If the mass of the car is m = 2000 kg, determine the time t it
                          takes for the car to accelerate from 1 m/s to 6 m/s. Use the trapezoidal rule for
                                                       6s
                          integration. Hint: t = m 1s (v/P) dv, which can be derived from Newton’s law
                          F = m(dv/dt ) and the definition of power P = Fv.

                                     v (m/s)     0     1.0     1.8     2.4     3.5       4.4     5.1      6.0
                                     P (kW)      0     4.7    12.2    19.0    31.8      40.1    43.8     43.2
                                     1
                       3. Evaluate −1 cos(2 cos−1 x)dx with Simpson’s 1/3 rule using 2, 4, and 6 panels.
                          Explain the results.
                                       ∞
                       4. Determine 1 (1 + x 4 )−1 dx with the trapezoidal rule using five panels and com-
                          pare the result with the “exact” integral 0.243 75. Hint: use the transformation
                          x 3 = 1/t .




                                                                                          F
                                                                       x




                       5. The following table gives the pull F of the bow as a function of the draw x. If the
                          bow is drawn 0.5 m, determine the speed of the 0.075-kg arrow when it leaves
                          the bow. Hint: the kinetic energy of the arrow equals the work done in drawing
                                                       0.5m
                          the bow; that is, mv 2 /2 = 0     F dx.

                                           x (m)       0.00    0.05    0.10    0.15      0.20     0.25
                                           F (N)          0      37      71     104       134      161
                                           x (m)       0.30    0.35    0.40    0.45      0.50
                                           F (N)        185     207     225     239       250
P1: PHB

CUUS884-Kiusalaas    CUUS884-06         978 0 521 19132 6                                                         December 16, 2009   15:4




           208      Numerical Integration

                                    2
                     6. Evaluate   0    x 5 + 3x 3 − 2 dx by Romberg integration.
                                    π
                     7. Estimate   0    f (x) dx as accurately as possible, where f (x) is defined by the data

                                            x          0             π /4                  π/2          3π /4       π
                                          f (x)     1.0000          0.3431                0.2500       0.3431    1.0000

                     8. Evaluate
                                                                                   1
                                                                                       sin x
                                                                                        √ dx
                                                                               0          x
                        with Romberg integration. Hint: use transformation of variables to eliminate the
                        singularity at x = 0.
                                                                 b
                     9. Newton–Cotes formulas for evaluating a f (x) dx were based on polynomial ap-
                        proximations of f (x). Show that if y = f (x) is approximated by a natural cubic
                        spline with evenly spaced knots at x0 , x1 , . . . , xn , the quadrature formula be-
                        comes
                                                        h
                                                  I =     y0 + 2y1 + 2y2 + · · · + 2yn−1 + yn
                                                        2
                                                            h3
                                                        −      k 0 + 2k 1 + k 2 + · · · + 2kn−1 + kn
                                                            24
                        where h is the distance between the knots and ki = yi . Note that the first part is
                        the composite trapezoidal rule; the second part may be viewed as a “correction”
                        for curvature.
                    10.   Evaluate
                                                                               π /4
                                                                                         dx
                                                                                        √
                                                                           0              sin x
                        with Romberg integration. Hint: use the transformation sin x = t 2 .
                                                                                 √
                    11.   The period of a simple pendulum of length L is τ = 4 L/gh(θ 0 ), where g is the
                        gravitational acceleration, θ 0 represents the angular amplitude, and
                                                                        π /2
                                                                                               dθ
                                                        h(θ 0 ) =
                                                                    0
                                                                                       1 − sin (θ 0 /2) sin2 θ
                                                                                               2


                          Compute h(15◦ ), h(30◦ ), and h(45◦ ) and compare these values with h(0) = π/2
                          (the approximation used for small amplitudes).
                    12.

                                                                                                   r
                                                               q
                                                                                         a                 P
P1: PHB

CUUS884-Kiusalaas    CUUS884-06      978 0 521 19132 6                                                    December 16, 2009   15:4




               209     6.3 Romberg Integration

                            The figure shows an elastic half-space that carries uniform loading of intensity q
                            over a circular area of radius a. The vertical displacement of the surface at point
                            P can be shown to be
                                                                  π/2
                                                                               cos2 θ
                                              w(r ) = w0                                            dθ,   r ≥a
                                                              0
                                                                          (r/a)2 − sin2 θ

                            where w0 is the displacement at r = a. Use numerical integration to determine
                            w/w0 at r = 2a.
                      13.

                                                                           x

                                                                                             m

                                                   b                            k




                            The mass m is attached to a spring of free length b and stiffness k. The coefficient
                            of friction between the mass and the horizontal rod is µ. The acceleration of the
                            mass can be shown to be (you may wish to prove this) x¨ = −f (x), where
                                                                        k                 b
                                                  f (x) = µg +            (µb + x) 1 − √
                                                                        m               b2 + x 2
                            If the mass is released from rest at x = b, its speed at x = 0 is given by
                                                                                      b
                                                                  v0 =      2             f (x)dx
                                                                                 0

                          Compute v0 by numerical integration using the data m = 0.8 kg, b = 0.4 m, µ =
                          0.3, k = 80 N/m, and g = 9.81 m/s2 .
                      14.   Debye’s formula for the heat capacity C V of a solid is C V = 9Nkg(u), where
                                                                                1/u
                                                                                           x 4 ex
                                                            g(u) = u3                              dx
                                                                            0         (ex    − 1)2
                            The terms in this equation are

                                                       N = number of particles in the solid
                                                       k = Boltzmann constant
                                                       u = T/       D

                                                       T = absolute temperature
                                                       D   = Debye temperature

                            Compute g(u) from u = 0 to 1.0 in intervals of 0.05 and plot the results.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06        978 0 521 19132 6                                               December 16, 2009   15:4




           210      Numerical Integration

                    15.     A power spike in an electric circuit results in the current


                                                           i(t ) = i0 e−t /t0 sin(2t /t0 )


                          across a resistor. The energy E dissipated by the resistor is

                                                                         ∞
                                                                                            2
                                                            E=               R i(t )            dt
                                                                     0


                        Find E using the data i0 = 100 A, R = 0.5 , and t0 = 0.01 s.
                    16.   An alternating electric current is described by

                                                                             πt         2πt
                                                     i(t ) = i0 sin             − β sin
                                                                             t0          t0

                          where i0 = 1 A, t0 = 0.05 s, and β = 0.2. Compute the root-mean-square current,
                          defined as

                                                                                    t0
                                                                         1
                                                           irms =                        i 2 (t ) dt
                                                                         t0     0


                    17.      (a) Derive the composite trapezoidal rule for unevenly spaced data. (b) Con-
                          sider the stress–strain diagram obtained from a uniaxial tension test.


                                             σ

                                                                                             Rupture
                                                                Ar


                                               0                                            εr ε

                          The area under the diagram is

                                                                               εr
                                                                Ar =                 σ dε
                                                                              ε=0


                          where εr is the strain at rupture. This area represents the work that must be
                          performed on a unit volume of the test specimen in order to cause rupture;
                          it is called the modulus of toughness. Use the result of Part (a) to estimate the
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                                    December 16, 2009     15:4




               211     6.4 Gaussian Integration

                          modulus of toughness for nickel steel from the following test data:

                                                                  σ (MPa)                  ε
                                                                          586           0.001
                                                                          662           0.025
                                                                          765           0.045
                                                                          841           0.068
                                                                          814           0.089
                                                                          122           0.122
                                                                          150           0.150

                          Note that the spacing of data is uneven.



              6.4     Gaussian Integration
                      Gaussian Integration Formulas
                                                                                                            b
                      We found that the Newton–Cotes formulas for approximating a f (x)dx work best if
                      f (x) is a smooth function, such as a polynomial. This is also true for Gaussian quadra-
                      ture. However, Gaussian formulas are also good at estimating integrals of the form
                                                                      b
                                                                          w(x)f (x)dx                                       (6.15)
                                                                  a

                      where w(x), called the weighting function, can contain singularities, as long as they
                                                                         1
                      are integrable. An example of such an integral is 0 (1 + x 2 ) ln x dx. Sometimes infi-
                                          ∞ −x
                      nite limits, as in 0 e sin x dx, can also be accommodated.
                           Gaussian integration formulas have the same form as the Newton–Cotes rules,
                                                                            n
                                                                  I=             A i f (xi )                                (6.16)
                                                                          i=0

                      where, as before, I represents the approximation to the integral in Eq. (6.15). The dif-
                      ference lies in the way that the weights A i and nodal abscissas xi are determined. In
                      Newton–Cotes integration the nodes were evenly spaced in (a, b), that is, their loca-
                      tions were predetermined. In Gaussian quadrature the nodes and weights are chosen
                      so that Eq. (6.16) yields the exact integral if f (x) is a polynomial of degree 2n + 1 or
                      less, that is,
                                                b                           n
                                                    w(x)Pm (x)dx =               A i Pm (xi ), m ≤ 2n + 1                   (6.17)
                                            a                              i=0

                      One way of determining the weights and abscissas is to substitute P0 (x) = 1, P1 (x) =
                      x, . . . , P2n+1 (x) = x 2n+1 in Eq. (6.17) and solve the resulting 2n + 2 equations
                                               b                  n
                                                                                 j
                                                   w(x)x j dx =           A i xi ,    j = 0, 1, . . . , 2n + 1
                                           a                      i=0

                      for the unknowns A i and xi .
P1: PHB

CUUS884-Kiusalaas     CUUS884-06         978 0 521 19132 6                                       December 16, 2009   15:4




           212      Numerical Integration

                        As an illustration, let w(x) = e−x , a = 0, b = ∞, and n = 1. The four equations
                    determining x0 , x1 , A 0 , and A 1 are
                                                                 ∞
                                                                     e−x dx = A 0 + A 1
                                                            0
                                                             1
                                                                 e−x x dx = A 0 x0 + A 1 x1
                                                        0
                                                            1
                                                                e−x x 2 dx = A 0 x02 + A 1 x12
                                                        0
                                                            1
                                                                e−x x 3 dx = A 0 x03 + A 1 x13
                                                        0

                    After evaluating the integrals, we get

                                                                          A0 + A1 = 1

                                                                     A 0 x0 + A 1 x1 = 1

                                                                     A 0 x02 + A 1 x12 = 2

                                                                     A 0 x03 + A 1 x13 = 6

                    The solution is
                                                                                      √
                                                                        √              2+1
                                                     x0 = 2 −               2    A0 =   √
                                                                                       2 2
                                                                                      √
                                                                        √              2−1
                                                     x1 = 2 +               2    A1 =   √
                                                                                       2 2
                    so that the integration formula becomes
                                  ∞
                                                    1  √            √     √            √
                                      e−x f (x)dx ≈ √ ( 2 + 1) f 2 − 2 + ( 2 − 1) f 2 + 2
                              0                    2 2
                        Because of the nonlinearity of the equations, this approach will not work well for
                    large n. Practical methods of finding xi and A i require some knowledge of orthogo-
                    nal polynomials and their relationship to Gaussian quadrature. There are, however,
                    several “classical” Gaussian integration formulas for which the abscissas and weights
                    have been computed with great precision and tabulated. These formulas can be used
                    without knowing the theory behind them, because all one needs for Gaussian inte-
                    gration are the values of xi and A i . If you do not intend to venture outside the classical
                    formulas, you can skip the next two topics of this article.


                    *Orthogonal Polynomials
                    Orthogonal polynomials are employed in many areas of mathematics and numerical
                    analysis. They have been studied thoroughly and many of their properties are known.
                    What follows is a very small compendium of a large topic.
                        The polynomials ϕn (x), n = 0, 1, 2, . . . (n is the degree of the polynomial) are
                    said to form an orthogonal set in the interval (a, b) with respect to the weighting
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                             December 16, 2009     15:4




               213     6.4 Gaussian Integration

                                                                                                b             2
                               Name           Symbol          a      b       w(x)              a w(x)ϕn (x) dx
                             Legendre          pn (x)         −1      1        1                 2/(2n + 1)
                             Chebyshev         Tn (x)         −1      1   (1 − x 2 )−1/2        π/2 (n > 0)
                             Laguerre          L n (x)          0    ∞        e−x                    1
                                                                                                  √ n
                                                                              e−x
                                                                                   2
                             Hermite           Hn (x)        −∞      ∞                              π 2 n!

                            Table 6.1


                      function w(x) if

                                                      b
                                                          w(x)ϕm (x)ϕn (x)dx = 0, m = n                              (6.18)
                                                  a


                      The set is determined, except for a constant factor, by the choice of the weighting
                      function and the limits of integration. That is, each set of orthogonal polynomials
                      is associated with certain w(x), a, and b. The constant factor is specified by stan-
                      dardization. Some of the classical orthogonal polynomials, named after well-known
                      mathematicians, are listed in Table 6.1. The last column in the table shows the stan-
                      dardization used.
                           Orthogonal polynomials obey recurrence relations of the form

                                              a n ϕn+1 (x) = (bn + cn x)ϕn (x) − dn ϕn−1 (x)                         (6.19)

                      If the first two polynomials of the set are known, the other members of the set can be
                      computed from Eq. (6.19). The coefficients in the recurrence formula, together with
                      ϕ 0 (x) and ϕ 1 (x), are given in Table 6.2.
                            The classical orthogonal polynomials are also obtainable from the formulas

                                                                  (−1)n d n                n
                                                      pn (x) =                  1 − x2
                                                                   2n n! dx n
                                                      Tn (x) = cos(n cos−1 x), n > 0
                                                               ex d n
                                                      L n (x) =           x n e−x                                    (6.20)
                                                               n! dx n
                                                                              n
                                                                         2 d
                                                                                 (e−x )
                                                                                     2
                                                      Hn (x) = (−1)n e x       n
                                                                          dx



                                     Name          ϕ 0 (x)     ϕ 1 (x)     an        bn           cn     dn
                                   Legendre           1           x       n+1         0         2n + 1   n
                                   Chebyshev          1           x         1         0           2      1
                                   Laguerre           1        1−x        n+1      2n + 1        −1      n
                                   Hermite            1          2x         1         0           2      2

                                  Table 6.2
P1: PHB

CUUS884-Kiusalaas       CUUS884-06            978 0 521 19132 6                                                            December 16, 2009         15:4




           214      Numerical Integration

                    and their derivatives can be calculated from

                                                    (1 − x 2 )pn (x) = n −xpn (x) + pn−1 (x)

                                                    (1 − x 2 )Tn (x) = n −xTn (x) + nTn−1 (x)

                                                              x L n (x) = n L n (x) − L n−1 (x)                                            (6.21)

                                                                      Hn (x) = 2nHn−1 (x)

                        Other properties of orthogonal polynomials that have relevance to Gaussian in-
                    tegration are:

                        • ϕn (x) has n real, distinct zeroes in the interval (a, b).
                        • The zeroes of ϕn (x) lie between the zeroes of ϕn+1 (x).
                        • Any polynomial Pn (x) of degree n can be expressed in the form
                                                                                        n
                                                                           Pn (x) =           ci ϕi (x)                                    (6.22)
                                                                                        i=0

                        • It follows from Eq. (6.22) and the orthogonality property in Eq. (6.18) that
                                                              b
                                                                  w(x)Pn (x)ϕn+m (x)dx = 0, m ≥ 0                                          (6.23)
                                                          a



                    *Determination of Nodal Abscissas and Weights
                    Theorem The nodal abscissas x0 , x1 , . . . , xn are the zeroes of the polynomial ϕn+1 (x)
                        that belongs to the orthogonal set defined in Eq. (6.18).
                    Proof We start the proof by letting f (x) = P2n+1 (x) be a polynomial of degree 2n + 1.
                         Because the Gaussian integration with n + 1 nodes is exact for this polynomial,
                         we have
                                                                  b                                 n
                                                                      w(x)P2n+1 (x)dx =                  A i P2n+1 (xi )                       (a)
                                                              a                                  i=0

                            A polynomial of degree 2n + 1 can always be written in the form

                                                                  P2n+1 (x) = Qn (x) + Rn (x)ϕn+1 (x)                                          (b)

                            where Qn (x), Rn (x), and ϕn+1 (x) are polynomials of the degree indicated by the
                            subscripts.2 Therefore,
                                          b                                    b                                b
                                              w(x)P2n+1 (x)dx =                    w(x)Qn (x)dx +                   w(x)Rn (x)ϕn+1 (x)dx
                                      a                                    a                                a

                            But according to Eq. (6.23) the second integral on the right-hand side vanishes,
                            so that
                                                              b                                     b
                                                                  w(x)P2n+1 (x)dx =                     w(x)Qn (x)dx                           (c)
                                                         a                                      a

                    2   It can be shown that Qn (x) and Rn (x) are unique for a given P2n+1 (x) and ϕn+1 (x).
P1: PHB

CUUS884-Kiusalaas    CUUS884-06     978 0 521 19132 6                                                                              December 16, 2009     15:4




               215     6.4 Gaussian Integration

                            Because a polynomial of degree n is uniquely defined by n + 1 points, it is al-
                            ways possible to find A i such that

                                                                    b                                          n
                                                                        w(x)Qn (x)dx =                              A i Qn (xi )                         (d)
                                                                a                                             i=0

                            In order to arrive at Eq. (a), we must choose for the nodal abscissas xi the roots
                            of ϕn+1 (x) = 0. According to Eq. (b), we then have

                                                           P2n+1 (xi ) = Qn (xi ), i = 0, 1, . . . , n                                                    (e)

                            which, together with Eqs. (c) and (d), leads to

                                              b                                              b                                 n
                                                  w(x)P2n+1 (x)dx =                              w(x)Qn (x)dx =                     A i P2n+1 (xi )
                                          a                                              a                                    i=0

                            This completes the proof.
                      Theorem
                                                                            b
                                                          Ai =                  w(x) i (x)dx, i = 0, 1, . . . , n                                      (6.24)
                                                                        a

                            where i (x) are the Lagrange’s cardinal functions spanning the nodes at
                            x0 , x1 , . . . xn . These functions were defined in Eq. (4.2).
                      Proof Applying Lagrange’s formula, Eq. (4.1), to Qn (x) yields
                                                                                                 n
                                                                        Qn (x) =                          Qn (xi ) i (x)
                                                                                                 i=0

                            which, upon substitution in Eq. (d), gives us
                                                     n                              b                                   n
                                                              Qn (xi )                  w(x) i (x)dx =                       A i Qn (xi )
                                                    i=0                         a                                      i=0

                            or
                                                          n                                           b
                                                                Qn (xi ) A i −                            w(x) i (x)dx = 0
                                                          i=0                                     a


                            This equation can be satisfied for arbitrary Q(x) of degree n only if
                                                                    b
                                                     Ai −               w(x) i (x)dx = 0, i = 0, 1, . . . , n
                                                                a

                            which is equivalent to Eq. (6.24).

                          It is not difficult to compute the zeroes xi , i = 0, 1, . . . , n of a polynomial ϕn+1 (x)
                      belonging to an orthogonal set by one of the methods discussed in Chapter 4. Once
                      the zeroes are known, the weights A i , i = 0, 1, . . . , n could be found from Eq. (6.24).
P1: PHB

CUUS884-Kiusalaas       CUUS884-06       978 0 521 19132 6                                                            December 16, 2009      15:4




           216      Numerical Integration

                    However, the following formulas (given without proof) are easier to compute:
                                                                                                     2
                                              Gauss–Legendre A i =                                                2
                                                                                    (1 −     xi2 )   pn+1 (xi )
                                                                                                1
                                               Gauss–Laguerre A i =                                       2
                                                                                                                                    (6.25)
                                                                                    xi L n+1 (xi )
                                                                                                 √
                                                                                    2n+2 (n + 1)! π
                                               Gauss–Hermite              Ai =                            2
                                                                                          Hn+1 (xi )


                    Abscissas and Weights for Classical Gaussian Quadratures
                    Here we list some classical Gaussian integration formulas. The tables of nodal ab-
                    scissas and weights, covering n = 1 to 5, have been rounded off to six decimal places.
                    These tables should be adequate for hand computation, but in programming you
                    may need more precision or a larger number of nodes. In that case you should consult
                    other references3 or use a subroutine to compute the abscissas and weights within
                    the integration program.4
                         The truncation error in Gaussian quadrature
                                                                 b                          n
                                                     E=              w(x)f (x)dx −              A i f (xi )
                                                             a                            i=0

                    has the form E = K (n)f (2n+2) (c), where a < c < b (the value of c is unknown; only
                    its bounds are given). The expression for K (n) depends on the particular quadrature
                    being used. If the derivatives of f (x) can be evaluated, the error formulas are useful
                    in estimating the error bounds.

                    Gauss–Legendre Quadrature

                                                                 1                  n
                                                                     f (ξ )dξ ≈          A i f (ξ i )                               (6.26)
                                                             −1                    i=0

                         This is the most-often-used Gaussian integration formula (see Table 6.3). The
                    nodes are arranged symmetrically about ξ = 0, and the weights associated with a
                    symmetric pair of nodes are equal. For example, for n = 1 we have ξ 0 = −ξ 1 and
                    A 0 = A 1 . The truncation error in Eq. (6.26) is
                                                                          4
                                                   22n+3 (n + 1)!
                                           E=                                 3
                                                                                  f (2n+2) (c),          −1<c <1                    (6.27)
                                                 (2n + 3) (2n + 2)!
                                                                                                          b
                         To apply Gauss–Legendre quadrature to the integral a f (x)dx, we must first map
                    the integration range (a, b) into the “standard” range (−1, 1˙). We can accomplish this
                    3   M. Abramowitz, and I. A. Stegun, Handbook of Mathematical Functions (Dover Publications, 1965).
                        A. H. Stroud and D. Secrest, Gaussian Quadrature Formulas (Prentice-Hall, 1966).
                    4   Several such subroutines are listed in W. H. Press et al, Numerical Recipes in Fortran 90 (Cambridge
                        University Press, 1996).
P1: PHB

CUUS884-Kiusalaas    CUUS884-06     978 0 521 19132 6                                                                    December 16, 2009     15:4




               217     6.4 Gaussian Integration


                                      ±ξ i                              Ai                  ±ξ i                                 Ai
                                                  n=1                                                         n=4
                                   0.577 350                      1.000 000           0.000 000                               0.568 889
                                                  n=2                                 0.538 469                               0.478 629
                                   0.000 000                      0.888 889           0.906 180                               0.236 927
                                   0.774 597                      0.555 556                                   n=5
                                                  n=3                                 0.238 619                               0.467 914
                                   0.339 981                      0.652 145           0.661 209                               0.360 762
                                   0.861 136                      0.347 855           0.932 470                               0.171 324

                                  Table 6.3


                      by the transformation
                                                                         b +a   b −a
                                                                  x=          +      ξ                                                       (6.28)
                                                                           2      2
                      Now dx = dξ (b − a)/2, and the quadrature becomes
                                                                                                 n
                                                              b
                                                                                b −a
                                                                  f (x)dx ≈                           A i f (xi )                            (6.29)
                                                          a                       2
                                                                                             i=1

                      where the abscissas xi must be computed from Eq. (6.28). The truncation error here
                      is
                                                                                      4
                                                  (b − a)2n+3 (n + 1)!
                                             E=                                   3
                                                                                          f (2n+2) (c),            a <c<b                    (6.30)
                                                  (2n + 3) (2n + 2)!

                      Gauss–Chebyshev Quadrature

                                                                                                              n
                                                   1
                                                                       −1/2                       π
                                                        1 − x2                f (x)dx ≈                             f (xi )                  (6.31)
                                                  −1                                             n+1
                                                                                                             i=0

                      Note that all the weights are equal: A i = π / (n + 1). The abscissas of the nodes, which
                      are symmetric about x = 0, are given by
                                                                                 (2i + 1)π
                                                                      xi = cos                                                               (6.32)
                                                                                  2n + 2
                      The truncation error is
                                                             2π
                                               E=                      f (2n+2) (c),                     −1<c <1                             (6.33)
                                                       22n+2 (2n + 2)!

                      Gauss–Laguerre Quadrature

                                                                  ∞                          n
                                                                      e−x f (x)dx ≈                  A i f (xi )                             (6.34)
                                                              0                            i=0

                      where the weights and the abscissas are given in Table 6.4.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06      978 0 521 19132 6                                           December 16, 2009        15:4




           218      Numerical Integration


                            xi                            Ai               xi                         Ai
                                      n=1                                                 n=4
                         0.585 786                    0.853 554         0.263 560                   0.521 756
                         3.414 214                    0.146 447         1.413 403                   0.398 667
                                      n=2                               3.596 426               (−1)0.759 424
                         0.415 775                  0.711 093           7.085 810               (−2)0.361 175
                         2.294 280                  0.278 517          12.640 801               (−4)0.233 670
                         6.289 945              (−1)0.103 892                             n=5
                                      n=3                               0.222 847                   0.458 964
                         0.322 548                  0.603 154           1.188 932                   0.417 000
                         1.745 761                  0.357 418           2.992 736                   0.113 373
                         4.536 620              (−1)0.388 791           5.775 144               (−1)0.103 992
                         9.395 071              (−3)0.539 295           9.837 467               (−3)0.261 017
                                                                       15.982 874               (−6)0.898 548

                       Table 6.4 Multiply numbers by 10k , where k is given in parentheses



                                                                   2
                                                   (n + 1)! (2n+2)
                                             E=              f     (c), 0 < c < ∞                                (6.35)
                                                   (2n + 2)!


                    Gauss–Hermite Quadrature

                                                      ∞                    n
                                                          e−x f (x)dx ≈
                                                               2
                                                                                A i f (xi )                      (6.36)
                                                   −∞                     i=0


                       The nodes are placed symmetrically abut x = 0 as indicated in Table 6.5.

                                                  √
                                                    π (n + 1)! (2n+2)
                                            E=                 f      (c), 0 < c < ∞                             (6.37)
                                                  22 (2n + 2)!



                            ±xi                           Ai              ±xi                         Ai
                                       n=1                                              n=4
                         0.707 107                       0.886 227     0.000 000                     0.945 308
                                       n=2                             0.958 572                     0.393 619
                         0.000 000                       1.181 636     2.020 183                (−1) 0.199 532
                         1.224745                        0.295 409                      n=5
                                       n=3                             0.436 077                    0.724 629
                         0.524 648                  0.804 914          1.335 849                    0.157 067
                         1.650 680              (−1)0.813 128          2.350 605                (−2)0.453 001

                        Table 6.5 Multiply numbers by 10k , where k is given in parentheses
P1: PHB

CUUS884-Kiusalaas    CUUS884-06        978 0 521 19132 6                                                    December 16, 2009     15:4




               219     6.4 Gaussian Integration

                      Gauss Quadrature with Logarithmic Singularity

                                                               1                         n
                                                                   f (x) ln(x)dx ≈ −          A i f (xi )                       (6.38)
                                                           0                           i=0


                             The weights and the abscissas are given in Table 6.6.

                                                                 k(n)
                                                      E=                 f (2n+1) (c),       0<c<1                              (6.39)
                                                               (2n + 1)!

                      where k(1) = 0.00 285, k(2) = 0.000 17, k(3) = 0.000 01.


                                  xi                                 Ai                      xi                          Ai
                                              n=1                                                           n=4
                                0.112 009                          0.718 539     (−1)0.291 345                         0.297 893
                                0.602 277                          0.281 461         0.173 977                         0.349 776
                                              n=2                                    0.411 703                         0.234 488
                           (−1)0.638 907                    0.513 405                0.677314                      (−1)0.989 305
                               0.368 997                    0.391 980                0.894 771                     (−1)0.189 116
                               0.766 880                (−1)0.946 154                                       n=5
                                              n=3                                (−1)0.216 344                         0.238 764
                           (−1)0.414 485                    0.383 464                0.129 583                         0.308 287
                               0.245 275                    0.386 875                0.314 020                         0.245 317
                               0.556 165                    0.190 435                0.538 657                         0.142 009
                               0.848 982                (−1)0.392 255                0.756 916                     (−1)0.554 546
                                                                                     0.922 669                     (−1)0.101 690

                          Table 6.6 Multiply numbers by 10k , where k is given in parentheses


                           gaussNodes

                      The function gaussNodes listed here5 computes the nodal abscissas xi and the corre-
                      sponding weights A i used in Gauss–Legendre quadrature over the “standard” interval
                      (−1, 1). It can be shown that the approximate values of the abscissas are
                                                                               π(i + 0.75)
                                                                    xi = cos
                                                                                m + 0.5
                      where m = n + 1 is the number of nodes, also called the integration order. Using these
                      approximations as the starting values, the nodal abscissas are computed by finding
                      the nonnegative zeroes of the Legendre polynomial pm (x) with Newton’s method (the
                      negative zeroes are obtained from symmetry). Note that gaussNodes calls the sub-
                      function legendre, which returns pm (t ) and its derivative as the tuple (p,dp).
                      5    This function is an adaptation of a routine in W. H. Press et al., Numerical Recipes in Fortran 90
                           (Cambridge University Press, 1996).
P1: PHB

CUUS884-Kiusalaas    CUUS884-06     978 0 521 19132 6                               December 16, 2009    15:4




           220      Numerical Integration

                    ## module gaussNodes
                    ’’’ x,A = gaussNodes(m,tol=10e-9)
                          Returns nodal abscissas {x} and weights {A} of
                          Gauss--Legendre m-point quadrature.
                    ’’’
                    from math import cos,pi
                    from numpy import zeros


                    def gaussNodes(m,tol=10e-9):


                          def legendre(t,m):
                              p0 = 1.0; p1 = t
                              for k in range(1,m):
                                  p = ((2.0*k + 1.0)*t*p1 - k*p0)/(1.0 + k )
                                  p0 = p1; p1 = p
                              dp = m*(p0 - t*p1)/(1.0 - t**2)
                              return p,dp


                          A = zeros(m)
                          x = zeros(m)
                          nRoots = (m + 1)/2               # Number of non-neg. roots
                          for i in range(nRoots):
                              t = cos(pi*(i + 0.75)/(m + 0.5))         # Approx. root
                              for j in range(30):
                                  p,dp = legendre(t,m)                 # Newton-Raphson
                                  dt = -p/dp; t = t + dt               # method
                                  if abs(dt) < tol:
                                         x[i] = t; x[m-i-1] = -t
                                         A[i] = 2.0/(1.0 - t**2)/(dp**2) # Eq.(6.25)
                                         A[m-i-1] = A[i]
                                         break
                          return x,A


                      gaussQuad
                                                                                b
                    The function gaussQuad utilizes gaussNodes to evaluate a f (x) dx with Gauss–
                    Legendre quadrature using m nodes. The function routine for f (x) must be supplied
                    by the user.
                    ## module gaussQuad
                    ’’’ I = gaussQuad(f,a,b,m).
                          Computes the integral of f(x) from x = a to b
                          with Gauss--Legendre quadrature using m nodes.
                    ’’’
                    from gaussNodes import *
P1: PHB

CUUS884-Kiusalaas    CUUS884-06     978 0 521 19132 6                                                                     December 16, 2009   15:4




               221     6.4 Gaussian Integration

                      def gaussQuad(f,a,b,m):
                           c1 = (b + a)/2.0
                           c2 = (b - a)/2.0
                           x,A = gaussNodes(m)
                           sum = 0.0
                           for i in range(len(x)):
                                  sum = sum + A[i]*f(c1 + c2*x[i])
                           return c2*sum

                      EXAMPLE 6.8
                               1
                      Evaluate −1 (1 − x 2 )3/2 dx as accurately as possible with Gaussian integration.

                      Solution As the integrand is smooth and free of singularities, we could use Gauss–
                      Legendre quadrature. However, the exact integral can be obtained with the Gauss–
                      Chebyshev formula. We write
                                                                                                                  2
                                                             1
                                                                          2 3/2
                                                                                                       1   1 − x2
                                                                 1−x                   dx =                √        dx
                                                            −1                                        −1     1 − x2
                      The numerator f (x) = (1 − x ) is a polynomial of degree 4, so that Gauss–Chebyshev
                                                                 2 2

                      quadrature is exact with three nodes.
                          The abscissas of the nodes are obtained from Eq. (6.32). Substituting n = 2, we
                      get
                                                                              (2i + 1)π
                                                             xi = cos                   , i = 0, 1, 2
                                                                                  6
                      Therefore,
                                                                                       √
                                                                                  π      3
                                                                         x0 = cos    =
                                                                                  6     2
                                                                                  π
                                                                         x1 = cos = 0
                                                                                  2
                                                                                        √
                                                                                  5π       3
                                                                         x2 = cos     =
                                                                                   6     2
                      and Eq. (6.31) yields
                                                                          2
                                    1
                                                      3/2            π                            2
                                            1 − x2          dx ≈                      1 − xi2
                                   −1                                3
                                                                         i=0

                                                                                              2                                 2
                                                                     π                   3                                  3           3π
                                                                 =             1−                 + (1 − 0)2 + 1 −                  =
                                                                     3                   4                                  4            8

                      EXAMPLE 6.9
                                                                                       0.5
                      Use Gaussian integration to evaluate                            0      cos π x ln x dx.

                      Solution We split the integral into two parts:

                                            0.5                                   1                               1
                                                  cos π x ln x dx =                   cos π x ln x dx −                cos π x ln x dx
                                        0                                     0                                  0.5
P1: PHB

CUUS884-Kiusalaas    CUUS884-06           978 0 521 19132 6                                                                 December 16, 2009   15:4




           222      Numerical Integration

                    The first integral on the right-hand side, which contains a logarithmic singularity at
                    x = 0, can be computed with the special Gaussian quadrature in Eq. (6.38). Choosing
                    n = 3, we have
                                                              1                                    3
                                                                  cos π x ln x dx ≈ −                   A i cos π xi
                                                          0                                    i=0

                    The sum is evaluated in the following table:

                                              xi                   cos π xi              Ai                    A i cos π xi
                                          0.041 448                0.991 534          0.383 464                    0.380 218
                                          0.245 275                0.717 525          0.386 875                    0.277 592
                                          0.556 165               −0.175 533          0.190 435                 −0.033 428
                                          0.848 982               −0.889 550          0.039 225                 −0.034 892
                                                                                                               = 0.589 490

                    Thus,
                                                                      1
                                                                          cos π x ln x dx ≈ −0.589 490
                                                                  0

                        The second integral is free of singularities, so it can be evaluated with Gauss–
                    Legendre quadrature. Choosing n = 3, we have
                                                    1                                          3
                                                         cos π x ln x dx ≈ 0.25                        A i cos π xi ln xi
                                                   0.5                                     i=0

                    where the nodal abscissas are – see Eq. (6.28)
                                                              1 + 0.5 1 − 0.5
                                                   xi =              +        ξ i = 0.75 + 0.25ξ i
                                                                 2       2
                    Looking up ξ i and A i in Table 6.3 leads to the following computations:

                                 ξi                     xi                    cos π xi ln xi              Ai           A i cos π xi ln xi
                             −0.861 136             0.534 716                   0.068 141              0.347 855             0.023 703
                             −0.339 981             0.665 005                   0.202 133              0.652 145             0.131 820
                              0.339 981             0.834 995                   0.156 638              0.652 145             0.102 151
                              0.861 136             0.965 284                   0.035 123              0.347 855             0.012 218
                                                                                                                           = 0.269 892

                    from which


                                              1
                                                   cos π x ln x dx ≈ 0.25(0.269 892) = 0.067 473
                                             0.5

                        Therefore,
                                      1
                                          cos π x ln x dx ≈ −0. 589 490 − 0.067 473 = −0. 656 96 3
                                  0

                    which is correct to six decimal places.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06     978 0 521 19132 6                                                           December 16, 2009   15:4




               223     6.4 Gaussian Integration

                      EXAMPLE 6.10
                      Evaluate as accurately as possible
                                                                                 ∞
                                                                                     x + 3 −x
                                                                 F =                  √ e dx
                                                                             0          x

                      Solution In its present form, the integral is not suited to any of the Gaussian quadra-
                      tures listed in this section. But using the transformation

                                                                 x = t2                  dx = 2t dt

                      the integral becomes
                                                            ∞                                    ∞
                                                                (t 2 + 3)e−t dt =                    (t 2 + 3)e−t dt
                                                                                     2                          2
                                              F =2
                                                        0                                       −∞

                      which can be evaluated exactly with the Gauss–Hermite formula using only two
                      nodes (n = 1). Thus,

                                  F = A 0 (t02 + 3) + A 1 (t12 + 3)

                                    = 0.886 227 (0.707 107)2 + 3 + 0.886 227 (−0.707 107)2 + 3

                                    = 6. 203 59

                      EXAMPLE 6.11
                      Determine how many nodes are required to evaluate
                                                                         π                  2
                                                                                 sin x
                                                                                                dx
                                                                     0             x
                      with Gauss–Legendre quadrature to six decimal places. The exact integral, rounded
                      to six places, is 1.418 15.

                      Solution The integrand is a smooth function, hence it is suited for Gauss–Legendre
                      integration. There is an indeterminacy at x = 0, but this does not bother the quadra-
                      ture because the integrand is never evaluated at that point. We used the following
                      program that computes the quadrature with 2, 3, . . . nodes until the desired accuracy
                      is reached:

                      ## example 6_11
                      from math import pi,sin
                      from gaussQuad import *


                      def f(x): return (sin(x)/x)**2


                      a = 0.0; b = pi;
                      Iexact = 1.41815
                      for m in range(2,12):
                           I = gaussQuad(f,a,b,m)
                           if abs(I - Iexact) < 0.00001:
P1: PHB

CUUS884-Kiusalaas    CUUS884-06        978 0 521 19132 6                                                December 16, 2009   15:4




           224      Numerical Integration

                               print ’’Number of nodes =’’,m
                               print ’’Integral =’’, gaussQuad(f,a,b,m)
                               break
                    raw_input(’’\nPress return to exit’’)

                        The program output is

                    Number of nodes = 5
                    Integral = 1.41815026778

                    EXAMPLE 6.12
                                               3
                    Evaluate numerically      1.5   f (x) dx, where f (x) is represented by the unevenly spaced
                    data

                            x         1.2              1.7          2.0          2.4                2.9        3.3
                          f (x)    −0.362 36        0.128 84     0.416 15     0.737 39           0.970 96   0.987 48

                    Knowing that the data points lie on the curve f (x) = − cos x, evaluate the accuracy of
                    the solution.

                    Solution We approximate f (x) by the polynomial P5 (x) that intersects all the data
                                                 3             3
                    points, and then evaluate 1.5 f (x)dx ≈ 1.5 P5 (x)dx with the Gauss–Legendre for-
                    mula. Because the polynomial is of degree 5, only three nodes (n = 2) are required
                    in the quadrature.
                         From Eq. (6.28) and Table 6.6, we obtain for the abscissas of the nodes
                                               3 + 1.5 3 − 1.5
                                          x0 =         +       (−0.774597) = 1. 6691
                                                  2         2
                                               3 + 1.5
                                          x1 =         = 2.25
                                                  2
                                               3 + 1.5 3 − 1.5
                                          x2 =         +       (0.774597) = 2. 8309
                                                  2         2
                    We now compute the values of the interpolant P5 (x) at the nodes. This can be done
                    using the modules newtonPoly or neville listed in Section 3.2. The results are

                                  P5 (x0 ) = 0.098 08         P5 (x1 ) = 0.628 16           P5 (x2 ) = 0.952 16

                    From Gauss–Legendre quadrature
                                                                                    2
                                                        3
                                                                         3 − 1.5
                                              I=            P5 (x)dx =                   A i P5 (xi )
                                                      1.5                   2
                                                                                   i=0

                    we get

                          I = 0.75 [0.555 556(0.098 08) + 0.888 889(0.628 16) + 0.555 556(0.952 16)]

                             = 0.856 37
                                             3
                    Comparison with −       1.5   cos x dx = 0. 856 38 shows that the discrepancy is within the
                    roundoff error.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                                   December 16, 2009   15:4




               225     6.4 Gaussian Integration

                      PROBLEM SET 6.2
                       1. Evaluate
                                                                 π
                                                                             ln(x)
                                                                                      dx
                                                             1           x 2 − 2x + 2
                          with Gauss–Legendre quadrature. Use (a) two nodes, and (b) four nodes.
                                                                     ∞
                       2. Use Gauss–Laguerre quadrature to evaluate 0 (1 − x 2 )3 e−x dx.
                       3. Use Gauss–Chebyshev quadrature with six nodes to evaluate
                                                                             π/2
                                                                                    dx
                                                                                   √
                                                                         0          sin x
                          Compare the result with the “exact” value 2.62206. Hint: substitute sin x = t 2 .
                                        π
                       4. The integral 0 sin x dx is evaluated with Gauss–Legendre quadrature using four
                          nodes. What are the bounds on the truncation error resulting from the quadra-
                          ture?
                       5. How many nodes are required in Gauss–Laguerre quadrature to evaluate
                            ∞ −x
                           0 e   sin x dx to six decimal places?
                       6. Evaluate as accurately as possible
                                                                     1
                                                                          2x + 1
                                                                         √          dx
                                                                 0         x(1 − x)
                          Hint: substitute x = (1 + t )/2.
                                      π
                       7. Compute 0 sin x ln x dx to four decimal places.
                                                                           π
                       8. Calculate the bounds on the truncation error if 0 x sin x dx is evaluated with
                          Gauss–Legendre quadrature using three nodes. What is the actual error?
                                    2
                       9. Evaluate 0 sinh x/x dx to four decimal places.
                      10.   Evaluate the integral
                                                                             ∞
                                                                                    x dx
                                                                         0         ex + 1
                          by Gauss–Legendre quadrature to six decimal places. Hint: substitute ex =
                          ln(1/t ).
                      11.   The equation of an ellipse is x 2 /a 2 + y 2 /b 2 = 1. Write a program that computes
                          the length
                                                                     a
                                                       S=2                     1 + (dy/dx)2 dx
                                                                 −a

                          of the circumference to five decimal places for a given a and b. Test the program
                          with a = 2 and b = 1.
                      12.   The error function, which is of importance in statistics, is defined as
                                                                                          x
                                                                   2
                                                                                              e−t dt
                                                                                                2
                                                         erf(x) = √
                                                                    π                 0

                          Write a program that uses Gauss–Legendre quadrature to evaluate erf(x) for a
                          given x to six decimal places. Note that erf(x) = 1.000 000 (correct to six decimal
                          places) when x > 5. Test the program by verifying that erf(1.0) = 0.842 701.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                                       December 16, 2009   15:4




           226      Numerical Integration

                    13.

                                                                       L                 B
                                                   A
                                                                                      m

                                               L                               k




                          The sliding weight of mass m is attached to a spring of stiffness k that has an
                          undeformed length L. When the mass is released from rest at B, the time it takes
                          to reach A can be shown to be t = C m/k, where
                                                       1   √           2                             2 −1/2
                                           C=               2−1            −       1 + z2 − 1                 dz
                                                   0

                          Compute C to six decimal places. Hint: the integrand has singularity at z = 1 that
                          behaves as (1 − z2 )−1/2 .
                    14.

                                                           x                                 A

                                                                                                P

                                                                                             h


                                                   B
                                                                           b               y
                          A uniform beam forms the semiparabolic cantilever arch A B. The vertical dis-
                          placement of A due to the force P can be shown to be
                                                                           Pb 3      h
                                                               δA =             C
                                                                           EI        b
                          where E I is the bending rigidity of the beam and
                                                                       1                         2
                                                           h                             2h
                                                       C       =           z2 1 +           z        dz
                                                           b       0                     b
                          Write a program that computes C(h/b) for any given value of h/b to four decimal
                          places. Use the program to compute C(0.5), C(1.0), and C(2.0).
P1: PHB

CUUS884-Kiusalaas       CUUS884-06       978 0 521 19132 6                                   December 16, 2009   15:4




                          ∗
               227            6.5 Multiple Integrals

                                                                                       π/2
                         15.       There is no elegant way to compute I = 0 ln(sin x) dx. A “brute force”
                                method that works is to split the integral into several parts: from x = 0 to 0.01,
                                from 0.01 to 0.2, and from x = 0.02 to π/2. In the first part, we can use the approx-
                                imation sin x ≈ x, which allows us to obtain the integral analytically. The other
                                two parts can be evaluated with Gauss–Legendre quadrature. Use this method to
                                evaluate I to six decimal places.
                         16.

                                                             h (m)
                                                              112         620
                                                                          612
                                                               80
                                                                   575
                                                               52
                                                                   530
                                                               35
                                                                  425
                                                               15
                                                                  310            p (Pa)
                                                                0

                                The pressure of wind was measured at various heights on a vertical wall, as shown
                                on the diagram. Find the height of the pressure center, which is defined as
                                                                         112 m
                                                                                h p(h) dh
                                                                 h¯ =   0
                                                                           112 m
                                                                          0      p(h) dh
                             Hint: fit a cubic polynomial to the data and then apply Gauss–Legendre quadra-
                             ture.
                                                                  x
                         17.   Write a function that computes x1n y(x) dx from a given set of data points of
                             the form
                                                                x1   x2    x3    ···    xn
                                                                y1   y2    y3    ···    yn

                                The function must work for unevenly spaced x-values. Test the function with the
                                data given in Prob. 17, Problem Set 6.1. Hint: fit a cubic spline to the data points
                                and apply Gauss–Legendre quadrature to each segment of the spline.


              ∗
                  6.5    Multiple Integrals

                         Multiple integrals, such as the area integral    A f (x, y) dx dy, can also be evaluated
                         by quadrature. The computations are straightforward if the region of integration has
                         a simple geometric shape, such as a triangle or a quadrilateral. Because of complica-
                         tions in specifying the limits of integration on x and y, quadrature is not a practical
                         means of evaluating integrals over irregular regions. However, an irregular region A
                         can always be approximated as an assembly triangular or quadrilateral subregions
P1: PHB

CUUS884-Kiusalaas     CUUS884-06       978 0 521 19132 6                                               December 16, 2009    15:4




           228      Numerical Integration


                                                                             Boundary of region A
                                                    Ai




                                Figure 6.6. Finite element model of an irregular region.


                    A 1 , A 2 , . . ., called finite elements, as illustrated in Fig. 6.6. The integral over A can then
                    be evaluated by summing the integrals over the finite elements:

                                                    f (x, y) dx dy ≈                  f (x, y) dx dy
                                                A                        i       Ai


                    Volume integrals can be computed in a similar manner, using tetrahedra or rectan-
                    gular prisms for the finite elements.


                    Gauss–Legendre Quadrature over a Quadrilateral Element
                    Consider the double integral
                                                               1   1
                                                         I=             f (ξ , η) dη dξ
                                                              −1   −1

                    over the rectangular element shown in Fig. 6.7(a). Evaluating each integral in turn
                    by Gauss–Legendre quadrature using n + 1 integration points in each coordinate

                                      η
                                                                                             η=1              3
                      1
                                                                                 4


                     0                                     ξ ξ = −1                                                   ξ=1
                                                                        y
                                                                             1
                     1                                                                                            2
                         1             0               1                              x η = −1
                                      (a)                                                  (b)
                    Figure 6.7. Mapping a quadrilateral into the standard rectangle.
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                                             December 16, 2009      15:4




                       ∗
               229         6.5 Multiple Integrals

                      direction, we obtain
                                                    1   n                               n             n
                                           I=                   A i f (ξ i , η) dη =          Aj           A i f (ξ i , ηi )
                                                 −1 i=0                                j =0         i=0

                      or
                                                                        n    n
                                                                 I=               A i A j f (ξ i , η j )                                 (6.40)
                                                                       i=0 j =0

                      As noted previously, the number of integration points in each coordinate direction,
                      m = n + 1, is called the integration order. Figure 6.7(a) shows the locations of the inte-
                      gration points used in third-order integration (m = 3). Because the integration limits
                      were the “standard” limits (−1, 1) of Gauss–Legendre quadrature, the weights and the
                      coordinates of the integration points are as listed in Table 6.3.
                           In order to apply quadrature to the quadrilateral element in Fig. 6.7(b), we must
                      first map the quadrilateral into the “standard” rectangle in Fig. 6.7(a). By mapping
                      we mean a coordinate transformation x = x(ξ , η), y = y(ξ , η) that results in one-to-
                      one correspondence between points in the quadrilateral and in the rectangle. The
                      transformation that does the job is
                                                            4                                              4
                                           x(ξ , η) =            Nk (ξ , η)xk          y(ξ , η) =              Nk (ξ , η)yk              (6.41)
                                                        k=1                                            k=1

                      where (xk , yk ) are the coordinates of corner k of the quadrilateral and
                                                                            1
                                                                N1 (ξ , η) =  (1 − ξ )(1 − η)
                                                                            4
                                                                            1
                                                                N2 (ξ , η) = (1 + ξ )(1 − η)                                             (6.42)
                                                                            4
                                                                            1
                                                                N3 (ξ , η) = (1 + ξ )(1 + η)
                                                                            4
                                                                            1
                                                                N4 (ξ , η) = (1 − ξ )(1 + η)
                                                                            4
                      The functions Nk (ξ , η), known as the shape functions, are bilinear (linear in each co-
                      ordinate). Consequently, straight lines remain straight upon mapping. In particular,
                      note that the sides of the quadrilateral are mapped into the lines ξ = ±1 and η = ±1.
                          Because mapping distorts areas, an infinitesimal area element dA = dx dy of the
                      quadrilateral is not equal to its counterpart dA = dξ dη of the rectangle. It can be
                      shown that the relationship between the areas is

                                                                  dx dy = |J (ξ , η)| dξ dη                                              (6.43)

                      where
                                                                                  ⎡ ∂x        ∂y ⎤
                                                                               ⎢ ∂ξ           ∂ξ ⎥
                                                                   J (ξ , η) = ⎣ ∂x           ∂y ⎦                                      (6.44a)
                                                                                    ∂η        ∂η
P1: PHB

CUUS884-Kiusalaas     CUUS884-06      978 0 521 19132 6                                                          December 16, 2009      15:4




           230      Numerical Integration

                    is the known as the Jacobian matrix of the mapping. Substituting from Eqs. (6.41) and
                    (6.42) and differentiating, the components of the Jacobian matrix are
                                            1
                                    J 11 =    [−(1 − η)x1 + (1 − η)x2 + (1 + η)x3 − (1 − η)x4 ]
                                            4
                                            1
                                    J 12   = [−(1 − η)y1 + (1 − η)y2 + (1 + η)y3 − (1 − η)y4 ]                                (6.44b)
                                            4
                                            1
                                    J 21   = [−(1 − ξ )x1 − (1 + ξ )x2 + (1 + ξ )x3 + (1 − ξ )x4 ]
                                            4
                                            1
                                    J 22   = [−(1 − ξ )y1 − (1 + ξ )y2 + (1 + ξ )y3 + (1 − ξ )y4 ]
                                            4
                    We can now write
                                                                   1     1
                                           f (x, y) dx dy =                  f [x(ξ , η), y(ξ , η)] |J (ξ , η)| dξ dη          (6.45)
                                     A                            −1    −1

                    Because the right-hand-side integral is taken over the “standard” rectangle, it can be
                    evaluated using Eq. (6.40). Replacing f (ξ , η) in Eq. (6.40) by the integrand in Eq. (6.45),
                    we get the following formula for Gauss–Legendre quadrature over a quadrilateral re-
                    gion:
                                                 n   n
                                           I=              A i A j f x(ξ i , η j ), y(ξ i , η j )   J (ξ i , η j )             (6.46)
                                                i=0 j =0

                    The ξ and η coordinates of the integration points and the weights can again be ob-
                    tained from Table 6.3.


                      gaussQuad2

                    The function gaussQuad2 in this module computes           A f (x, y) dx dy over a quadri-
                    lateral element with Gauss–Legendre quadrature of integration order m. The quadri-
                    lateral is defined by the arrays x and y, which contain the coordinates of the four cor-
                    ners ordered in a counterclockwise direction around the element. The determinant of
                    the Jacobian matrix is obtained by calling the function jac; mapping is performed by
                    map. The weights and the values of ξ and η at the integration points are computed by
                    gaussNodes listed in the previous section (note that ξ and η appear as s and t in the
                    listing).

                    from gaussNodes import *
                    from numpy import zeros,dot


                    def gaussQuad2(f,x,y,m):


                         def jac(x,y,s,t):
                               J = zeros((2,2))
                               J[0,0] = -(1.0 - t)*x[0] + (1.0 - t)*x[1]                                         \
                                             + (1.0 + t)*x[2] - (1.0 + t)*x[3]
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                    December 16, 2009   15:4




                       ∗
               231         6.5 Multiple Integrals

                                   J[0,1] = -(1.0 - t)*y[0] + (1.0 - t)*y[1]                  \
                                              + (1.0 + t)*y[2] - (1.0 + t)*y[3]
                                   J[1,0] = -(1.0 - s)*x[0] - (1.0 + s)*x[1]                  \
                                              + (1.0 + s)*x[2] + (1.0 - s)*x[3]
                                   J[1,1] = -(1.0 - s)*y[0] - (1.0 + s)*y[1]                  \
                                              + (1.0 + s)*y[2] + (1.0 - s)*y[3]
                                   return (J[0,0]*J[1,1] - J[0,1]*J[1,0])/16.0


                              def map(x,y,s,t):
                                   N = zeros(4)
                                   N[0] = (1.0 - s)*(1.0 - t)/4.0
                                   N[1] = (1.0 + s)*(1.0 - t)/4.0
                                   N[2] = (1.0 + s)*(1.0 + t)/4.0
                                   N[3] = (1.0 - s)*(1.0 + t)/4.0
                                   xCoord = dot(N,x)
                                   yCoord = dot(N,y)
                                   return xCoord,yCoord


                              s,A = gaussNodes(m)
                              sum = 0.0
                              for i in range(m):
                                   for j in range(m):
                                        xCoord,yCoord = map(x,y,s[i],s[j])
                                        sum = sum + A[i]*A[j]*jac(x,y,s[i],s[j])                  \
                                                          *f(xCoord,yCoord)
                              return sum


                      EXAMPLE 6.13


                                                          y                        3


                                                          4
                                                                                3
                                                          2


                                                                       2               x
                                                              1                2

                             Evaluate the integral

                                                              I=       x 2 + y dx dy
                                                                   A
P1: PHB

CUUS884-Kiusalaas     CUUS884-06        978 0 521 19132 6                                             December 16, 2009   15:4




           232      Numerical Integration

                    analytically by first transforming it from the quadrilateral region A shown to the
                    “standard” rectangle.

                    Solution The corner coordinates of the quadrilateral are

                                              xT = 0     2       2    0          yT = 0   0   3   2

                    The mapping is

                                                   4
                                    x(ξ , η) =         Nk (ξ , η)xk
                                                 k=1

                                                   (1 + ξ )(1 − η)       (1 + ξ )(1 + η)
                                               = 0+                (2) +                 (2) + 0
                                                          4                     4
                                               = 1+ξ
                                                   4
                                    y(ξ , η) =         Nk (ξ , η)yk
                                                 k=1

                                                          (1 + ξ )(1 + η)       (1 − ξ )(1 + η)
                                               = 0+0+                     (3) +                 (2)
                                                                 4                     4
                                                 (5 + ξ )(1 + η)
                                               =
                                                        4
                    which yields for the Jacobian matrix
                                                                 ⎡ ∂x     ∂y ⎤ ⎡              ⎤
                                                                                          1+η
                                                             ⎢ ∂ξ                  1
                                                 J (ξ , η) = ⎣ ∂x         ∂ξ ⎥ ⎢           4 ⎥
                                                                          ∂y ⎦ = ⎣        5+ξ⎦
                                                                                   0
                                                                     ∂η   ∂η               4

                    Thus, the area scale factor is

                                                                                 5+ξ
                                                                 |J (ξ , η)| =
                                                                                  4
                    Now we can map the integral from the quadrilateral to the standard rectangle. Refer-
                    ring to Eq. (6.45), we obtain

                                                         2
                                   1      1
                                                1+ξ              (5 + ξ )(1 + η)     5+ξ
                             I =                             +                           dξ dη
                                   −1    −1      2                      4             4
                                   1      1
                                              15 21      1     1 3 25       5      1 2
                              =                  +    ξ + ξ2 +    ξ +    η + ξη +    ξ η dξ dη
                                   −1    −1    8   16    2     16     16    8     16

                    If we note that only even powers of ξ and η contribute to the integral, the integral
                    simplifies to
                                                         1       1
                                                                      15 1 2         49
                                                 I=                      + ξ dξ dη =
                                                        −1   −1        8  2          6
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                                   December 16, 2009   15:4




                       ∗
               233         6.5 Multiple Integrals

                      EXAMPLE 6.14
                      Evaluate the integral
                                                           1        1
                                                                              πx     πy
                                                                        cos      cos    dx dy
                                                          −1       −1          2      2
                      by Gauss–Legendre quadrature of order 3.

                      Solution From the quadrature formula in Eq. (6.40), we have
                                                               2        2
                                                                                          π xi     π yj
                                                     I=                     A i A j cos        cos
                                                                                           2        2
                                                           i=0 j =0




                                                                               y
                                                     1
                                                                   a               b        a

                                                                   b                        b
                                                     0                                                x

                                                                   a               b        a
                                                    −1
                                                     −1                            0             1
                          The integration points are shown in the figure; their coordinates, and the cor-
                      responding weights are listed in Table 6.3. Note that the integrand, the integration
                      points, and the weights are all symmetric about the coordinate axes. It follows that
                      the points labeled a contribute equal amounts to I ; the same is true for the points
                      labeled b. Therefore,
                                                               π (0.774 597)
                                        I = 4(0.555 556)2 cos2
                                                                      2
                                                                          π (0.774 597)     π (0)
                                            +4(0.555 556)(0.888 889) cos                cos
                                                                                2             2
                                                               π  (0)
                                            +(0.888 889)2 cos2
                                                                  2
                                          = 1.623 391

                      The exact value of the integral is 16/π 2 ≈ 1.621 139.

                      EXAMPLE 6.15
                      Utilize gaussQuad2 to evaluate I =                       A   f (x, y) dx dy over the quadrilateral shown,
                      where

                                                          f (x, y) = (x − 2)2 (y − 2)2

                      Use enough integration points for an “exact” answer.
P1: PHB

CUUS884-Kiusalaas     CUUS884-06     978 0 521 19132 6                                                 December 16, 2009         15:4




           234      Numerical Integration


                                               y

                                               4                                        3
                                                            4
                                               3



                                               1                                        2

                                             1                                                x
                                                            1                       4

                    Solution The required integration order is determined by the integrand in Eq. (6.45):
                                                   1    1
                                          I=                f [x(ξ , η), y(ξ , η)] |J (ξ , η)| dξ dη                       (a)
                                                 −1    −1

                    We note that |J (ξ , η)|, defined in Eqs. (6.44), is biquadratic. Because the specified
                    f (x, y) is also biquadratic, the integrand in Eq. (a) is a polynomial of degree 4 in both
                    ξ and η. Thus, third-order integration is sufficient for an “exact” result.

                    #!/usr/bin/python
                    ## example 6_15
                    from gaussQuad2 import *
                    from numpy import array


                    def f(x,y): return ((x - 2.0)**2)*((y - 2.0)**2)


                    x = array([0.0, 4.0, 4.0, 1.0])
                    y = array([0.0, 1.0, 4.0, 3.0])
                    m = eval(raw_input(’’Integration order ==> ’’))
                    print ’’Integral =’’, gaussQuad2(gaussNodes,f,x,y,m)
                    raw_input(’’\nPress return to exit’’


                        Running the preceding program produced the following result:

                    Integration order ==> 3
                    Integral = 11.3777777778



                    Quadrature over a Triangular Element
                    A triangle may be viewed as a degenerate quadrilateral with two of its corners occu-
                    pying the same location, as illustrated in Fig. 6.8. Therefore, the integration formulas
                    over a quadrilateral region can also be used for a triangular element. However, it is
P1: PHB

CUUS884-Kiusalaas    CUUS884-06        978 0 521 19132 6                                                        December 16, 2009     15:4




                       ∗
               235         6.5 Multiple Integrals


                                 3 4

                                                                Figure 6.8. Degenerate quadrilateral.


                       1
                                                        2

                      computationally advantageous to use integration formulas specially developed for
                      triangles, which we present without derivation.6

                                      3
                          y
                                 x
                                      A2 P A1                                Figure 6.9. Triangular element.


                                           A3
                                                                2
                       1

                          Consider the triangular element in Fig. 6.9. Drawing straight lines from the point
                      P in the triangle to each of the corners, we divide the triangle into three parts with
                      areas A 1 , A 2 , and A 3 . The area coordinates of P are defined as

                                                                             Ai
                                                                    αi =        , i = 1, 2, 3                                       (6.47)
                                                                             A
                      where A is the area of the element. Because A 1 + A 2 + A 3 = A , the area coordinates
                      are related by

                                                                     α1 + α2 + α3 = 1                                               (6.48)

                      Note that αi ranges from 0 (when P lies on the side opposite to corner i) to 1 (when P
                      is at corner i).
                           A convenient formula for computing A from the corner coordinates (xi , yi ) is

                                                                                  1    1     1
                                                                         1
                                                              A=                  x1   x2    x3                                     (6.49)
                                                                         2
                                                                                  y1   y2    y3
                      The area coordinates are mapped into the Cartesian coordinates by
                                                                     3                                          3
                                            x(α 1 , α 2 , α 3 ) =         αi xi        y(α 1 , α 2 , α 3 ) =         αi yi          (6.50)
                                                                    i=1                                        i=1

                      6    The triangle formulas are extensively used in finite method analysis. See, for example, O. C.
                           Zienkiewicz and R. L. Taylor, The Finite Element Method, Vol. 1, 4th ed. (McGraw-Hill, 1989).
P1: PHB

CUUS884-Kiusalaas     CUUS884-06        978 0 521 19132 6                                           December 16, 2009      15:4




           236      Numerical Integration



                                                                                                  b
                                                            a
                                         a                                     c                         a
                                                                                            c                d
                                                                   b
                                 (a) Linear                  (b) Quadratic                      (c) Cubic
                            Figure 6.10. Integration points of triangular elements.


                          The integration formula over the element is

                                                  f [x(α), y(α)] dA = A        Wk f [x(α k ), y(α k )]            (6.51)
                                              A                            k

                    where α k represents the area coordinates of the integration point k, and Wk are the
                    weights. The locations of the integration points are shown in Figure 6.10, and the
                    corresponding values of α k and Wk are listed in Table 6.7. The quadrature in Eq. (6.51)
                    is exact if f (x, y) is a polynomial of the degree indicated.


                                       Degree of f (x, y)       Point              αk            Wk
                                       (a) Linear                a       1/3, 1/3, 1/3          1
                                       (b) Quadratic             a        1/2, 0 , 1/2         1/3
                                                                 b        1/2, 1/2, 0          1/3
                                                                 c        0, 1/2 , 1/2         1/3
                                       (c) Cubic                 a       1/3, 1/3, 1/3        −27/48
                                                                 b       1/5, 1/5, 3/5         25/48
                                                                 c       3/5. 1/5 , 1/5        25/48
                                                                 d       1/5, 3/5 , 1/5        25/48

                                      Table 6.7


                      triangleQuad

                    The function triangleQuad computes A f (x, y) dx dy over a triangular region us-
                    ing the cubic formula – case (c) in Fig. 6.10. The triangle is defined by its corner coor-
                    dinate arrays xc and yc, where the coordinates are listed in a counterclockwise order
                    around the triangle.

                    ## module triangleQuad
                    ’’’ integral = triangleQuad(f,xc,yc).
                          Integration of f(x,y) over a triangle using
                          the cubic formula.
                          {xc},{yc} are the corner coordinates of the triangle.
                    ’’’
                    from numpy import array,dot
P1: PHB

CUUS884-Kiusalaas    CUUS884-06         978 0 521 19132 6                                              December 16, 2009      15:4




                       ∗
               237         6.5 Multiple Integrals

                      def triangleQuad(f,xc,yc):
                               alpha = array([[1.0/3, 1.0/3.0, 1.0/3.0],                           \
                                                        [0.2, 0.2, 0.6],                           \
                                                        [0.6, 0.2, 0.2],                           \
                                                        [0.2, 0.6, 0.2]])
                               W = array([-27.0/48.0 ,25.0/48.0, 25.0/48.0, 25.0/48.0])
                               x = dot(alpha,xc)
                               y = dot(alpha,yc)
                               A = (xc[1]*yc[2] - xc[2]*yc[1]                           \
                                   - xc[0]*yc[2] + xc[2]*yc[0]                          \
                                   + xc[0]*yc[1] - xc[1]*yc[0])/2.0
                               sum = 0.0
                               for i in range(4):
                                     sum = sum + W[i] * f(x[i],y[i])
                               return A*sum

                      EXAMPLE 6.16


                                                      1              y


                                                               1
                                                                                               3
                                                                                                    x
                                                                            3



                                                      2

                             Evaluate I =         A   f (x, y) dx dy over the equilateral triangle shown, where7

                                                                   1 2           1                 2
                                                      f (x, y) =     (x + y 2 ) − (x 3 − 3xy 2 ) −
                                                                   2             6                 3
                      Use the quadrature formulas for (1) a quadrilateral and (2) a triangle.

                      Solution of Part (1) Let the triangle be formed by collapsing corners 3 and 4 of a
                      quadrilateral. The corner coordinates of this quadrilateral are x = [−1, −1, 2, 2]T
                                √      √        T
                      and y =     3, − 3, 0, 0 . To determine the minimum required integration order
                      for an exact result, we must examine f [x(ξ , η), y(ξ , η)] |J (ξ , η)|, the integrand in Eqs.

                      7    This function is identical to the Prandtl stress function for torsion of a bar with the cross section
                           shown; the integral is related to the torsional stiffness of the bar. See, for example, S. P. Timoshenko
                           and J. N. Goodier, Theory of Elasticity, 3rd ed. (McGraw-Hill, 1970).
P1: PHB

CUUS884-Kiusalaas     CUUS884-06      978 0 521 19132 6                                      December 16, 2009        15:4




           238      Numerical Integration

                    (6.44). Because |J (ξ , η)| is biquadratic and f (x, y) is cubic in x, the integrand is a poly-
                    nomial of degree 5 in x. Therefore, third-order integration will suffice. The program
                    used for the computations is similar to the one in Example 6.15:

                    #!/usr/bin/python
                    ## example6_16a
                    from gaussQuad2 import *
                    from numpy import array
                    from math import sqrt


                    def f(x,y):
                         return (x**2 + y**2)/2.0                         \
                                   - (x**3 - 3.0*x*y**2)/6.0              \
                                   - 2.0/3.0


                    x = array([-1.0,-1.0,2.0,2.0])
                    y = array([sqrt(3.0),-sqrt(3.0),0.0,0.0])
                    m = eval(raw_input(’’Integration order ==> ’’))
                    print ’’Integral =’’, gaussQuad2(gaussNodes,f,x,y,m)
                    raw_input(’’\nPress return to exit’’)

                        Here is the output:

                    Integration order ==> 3
                    Integral = -1.55884572681

                    Solution of Part (2) The following program utilizes triangleQuad:

                    #!/usr/bin/python
                    # example6_16b
                    from numpy import array
                    from math import sqrt
                    from triangleQuad import *


                    def f(x,y):
                         return (x**2 + y**2)/2.0                         \
                                   - (x**3 - 3.0*x*y**2)/6.0              \
                                   - 2.0/3.0


                    xCorner = array([-1.0, -1.0, 2.0])
                    yCorner = array([sqrt(3.0), -sqrt(3.0), 0.0])
                    print ’’Integral =’’,triangleQuad(f,xCorner,yCorner)
                    raw_input(’’Press return to              exit’’)

                        Because the integrand is a cubic, this quadrature is also exact, the result being

                    Integral = -1.55884572681
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                                 December 16, 2009   15:4




                       ∗
               239         6.5 Multiple Integrals

                          Note that only four function evaluations were required when using the triangle
                      formulas. In contrast, the function had to be evaluated at nine points in part (1).

                      EXAMPLE 6.17
                      The corner coordinates of a triangle are (0, 0), (16, 10), and (12, 20). Compute
                        A x −y
                           2    2
                                  dx dy over this triangle.



                                                          y
                                                                      12               4

                                                                                         c 10

                                                                    a

                                                                                            10
                                                                               b
                                                                                                    x
                      Solution Because f (x, y) is quadratic, quadrature over the three integration points
                      shown in Fig. 6.10(b) will be sufficient for an “exact” result. The integration points lie
                      in the middle of each side; their coordinates are (6, 10), (8, 5), and (14, 15). The area
                      of the triangle is obtained from Eq. (6.49):

                                                          1      1         1      1         1  1
                                                        1                       1
                                               A=         x1     x2        x3 =   0        16 12 = 100
                                                        2                       2
                                                          y1     y2        y3     0        10 20

                      From Eq. (6.51) we get
                                                    c
                                         I =A           Wk f (xk , yk )
                                                k=a

                                                        1            1          1
                                           = 100          f (6, 10) + f (8, 5) + f (14, 15)
                                                        3            3          3
                                               100
                                           =       (62 − 102 ) + (82 − 52 ) + (142 − 152 ) = 1800
                                                3
                      PROBLEM SET 6.3
                       1. Use Gauss–Legendre quadrature to compute
                                                                1      1
                                                                           (1 − x 2 )(1 − y 2 ) dx dy
                                                               −1     −1

                       2. Evaluate the following integral with Gauss–Legendre quadrature:
                                                                        2       3
                                                                                    x 2 y 2 dx dy
                                                                      y=0     x=0
P1: PHB

CUUS884-Kiusalaas    CUUS884-06    978 0 521 19132 6                                                     December 16, 2009   15:4




           240      Numerical Integration

                    3. Compute the approximate value of
                                                               1        1
                                                                             e−(x       +y 2 )
                                                                                    2
                                                                                                 dx dy
                                                               −1       −1

                       with Gauss–Legendre quadrature. Use integration order (a) 2 and (b) 3. (The “ex-
                       act” value of the integral is 2.230 985.)
                    4. Use third-order Gauss–Legendre quadrature to obtain an approximate value of
                                                           1        1
                                                                              π (x − y)
                                                                        cos             dx dy
                                                       −1       −1                2
                       (The “exact” value of the integral is 1.621 139.)
                    5. Map the integral    A xy dx dy from the quadrilateral region shown to the “stan-
                       dard” rectangle, and then evaluate it analytically.

                                                  y
                                                                             4


                                                  4


                                                                                                     x
                                                                   2
                    6. Compute       A x dx dy over the quadrilateral region shown by first mapping it
                       into the “standard” rectangle and then integrating analytically.

                                                       y                      4


                                                                                                     4


                                                                                                     x
                                                       2                          3
                    7. Use quadrature to compute               A    x 2 dx dy over the triangle shown.


                                                           y


                                                           4
                                                                         3                       x
                                                           2
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                    December 16, 2009   15:4




                       ∗
               241         6.5 Multiple Integrals

                                           3
                       8. Evaluate     A x dx dy over the triangle shown in Prob. 7.
                       9. Use quadrature to evaluate      A (3 − x)y dx dy over the region shown. Treat the
                          region as (a) a triangular element and (b) a degenerate quadrilateral.


                                                            y


                                                           4


                                                                                  x
                                                                     3

                      10. Evaluate        A   x 2 y dx dy over the triangle shown in Prob. 9.
                      11.

                                                                     y
                                                                    1         3

                                                                         2
                                                                                      x
                                                                         2

                                                                3         1

                                       A xy(2 − x )(2 − xy) dx dy over the region shown.
                                                 2
                          Evaluate
                      12.   Compute A xy exp(−x 2 ) dx dy over the region shown in Prob. 11 to four dec-
                          imal places.
                      13.

                                                       y


                                                                                  1


                                                                                      x
                                                                      1

                             Evaluate    A (1 − x)(y − x)y dx dy over the triangle shown.
                      14.       Estimate     A sin π x dx dy over the region shown in Prob. 13. Use the cubic
                             integration formula for a triangle. (The exact integral is 1/π .)
P1: PHB

CUUS884-Kiusalaas    CUUS884-06       978 0 521 19132 6                                 December 16, 2009      15:4




           242      Numerical Integration

                    15.     Compute A sin π x sin π (y − x) dx dy to six decimal places, where A is the tri-
                          angular region shown in Prob. 13. Consider the triangle as a degenerate quadri-
                          lateral.
                    16.

                                                              y

                                                                            1
                                                          1

                                                                            1

                                                                                x
                                                                   1

                          Write a program to evaluate     A f (x, y) dx dy over an irregular region that has
                          been divided into several triangular elements. Use the program to compute
                            A xy(y − x) dx dy over the region shown.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                               December 16, 2009     15:4




              7       Initial Value Problems




                                     Solve y = F(x, y) with the auxiliary conditions y(a) = α




              7.1     Introduction

                      The general form of a first-order differential equation is

                                                                  y = f (x, y)                                         (7.1a)

                      where y = dy/dx and f (x, y) is a given function. The solution of this equation con-
                      tains an arbitrary constant (the constant of integration). To find this constant, we
                      must know a point on the solution curve; that is, y must be specified at some value of
                      x, say, at x = a. We write this auxiliary condition as

                                                                    y(a) = α                                           (7.1b)

                          An ordinary differential equation of order n

                                                        y (n) = f x, y, y , . . . , y (n−1)                             (7.2)

                      can always be transformed into n first-order equations. Using the notation

                                        y0 = y         y1 = y         y2 = y         ...   yn−1 = y (n−1)               (7.3)

                      the equivalent first-order equations are

                              y0 = y1     y1 = y2          y2 = y3      ...    yn = f (x, y0 , y1 , . . . , yn−1 )     (7.4a)

                      The solution now requires the knowledge of n auxiliary conditions. If these condi-
                      tions are specified at the same value of x, the problem is said to be an initial value
                      problem. Then the auxiliary conditions, called initial conditions, have the form

                                         y0 (a) = α 0         y1 (a) = α 1     ...     yn−1 (a) = αn−1                 (7.4b)

                      If yi are specified at different values of x, the problem is called a boundary value prob-
                      lem.

               243
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                    December 16, 2009      15:4




           244      Initial Value Problems

                        For example,

                                               y = −y           y(0) = 1      y (0) = 0

                    is an initial value problem because both auxiliary conditions imposed on the solution
                    are given at x = 0. On the other hand,

                                               y = −y           y(0) = 1      y(π ) = 0

                    is a boundary value problem because the two conditions are specified at different
                    values of x.
                         In this chapter, we consider only initial value problems. Boundary value prob-
                    lems, which are more difficult to solve, are discussed in the next chapter. We also
                    make extensive use of vector notation, which allows us to manipulate sets of first-
                    order equations in a concise form. For example, Eqs. (7.4) are written as

                                                    y = F(x, y)          y(a) = α                       (7.5a)

                    where
                                                                    ⎡          ⎤
                                                                      y1
                                                                   ⎢y          ⎥
                                                                   ⎢ 2         ⎥
                                                         F(x, y) = ⎢
                                                                   ⎢ ..
                                                                               ⎥
                                                                               ⎥                        (7.5b)
                                                                   ⎣.          ⎦
                                                                      f (x, y)
                    A numerical solution of differential equations is essentially a table of x- and y-values
                    listed at discrete intervals of x.



          7.2       Taylor Series Method

                    The Taylor series method is conceptually simple and capable of high accuracy. Its
                    basis is the truncated Taylor series for y about x:
                                                         1           1                  1 (m)
                         y(x + h) ≈ y(x) + y (x)h +         y (x)h2 + y (x)h3 + . . . +    y (x)hm       (7.6)
                                                         2!          3!                 m!
                    Because Eq. (7.6) predicts y at x + h from the information available at x, it is also a
                    formula for numerical integration. The last term kept in the series determines the
                    order of integration. For the series in Eq. (7.6), the integration order is m.
                        The truncation error, due to the terms omitted from the series, is
                                                  1
                                         E=             y(m+1) (ξ )hm+1 ,    x <ξ <x+h
                                               (m + 1)!
                    Using the finite difference approximation
                                                                y(m) (x + h) − y(m) (x)
                                                y(m+1) (ξ ) ≈
                                                                           h
                    we obtain the more usable form
                                                     hm
                                              E≈            y(m) (x + h) − y(m) (x)                      (7.7)
                                                   (m + 1)!
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                    December 16, 2009     15:4




               245     7.2 Taylor Series Method

                      which could be incorporated in the algorithm to monitor the error in each integration
                      step.


                         taylor

                      The function taylor implements the Taylor series method of integration order 4.
                      It can handle any number of first-order differential equations yi = fi (x, y0 , y1 , . . .),
                      i = 0, 1, . . .. The user is required to supply the function deriv that returns the 4 × n
                      array
                                                     ⎡          ⎤ ⎡                          ⎤
                                                        (y )T         y0    y1    · · · yn−1
                                                     ⎢ (y )T ⎥ ⎢ y                · · · yn−1 ⎥
                                                     ⎢          ⎥ ⎢ 0       y1               ⎥
                                                D=⎢             ⎥ = ⎢                        ⎥
                                                     ⎣ (y )T ⎦ ⎣ y0         y1    · · · yn−1 ⎦
                                                       (y(4) )T       y0(4) y1(4) · · · yn−1
                                                                                         (4)


                      The function returns the arrays X and Y that contain the values of x and y at
                      intervals h.

                      ## module taylor
                      ’’’ X,Y = taylor(deriv,x,y,xStop,h).
                            4th-order Taylor series method for solving the initial
                            value problem {y}’ = {F(x,{y})}, where
                            {y} = {y[0],y[1],...y[n-1]}.
                            x,y    = initial conditions
                            xStop = terminal value of x
                            h       = increment of x used in integration
                            deriv = user-supplied function that returns the 4 x n array
                                  [y’[0]     y’[1]      y’[2] ...      y’[n-1]
                                   y"[0]     y"[1]      y"[2] ...      y"[n-1]
                                  y"’[0]    y"’[1]     y"’[2] ... y"’[n-1]
                                  y""[0]    y""[1]     y""[2] ... y""[n-1]]
                      ’’’
                      from numpy import array
                      def taylor(deriv,x,y,xStop,h):
                            X = []
                            Y = []
                            X.append(x)
                            Y.append(y)
                            while x < xStop:                      # Loop over integration steps
                                  h = min(h,xStop - x)
                                  D = deriv(x,y)                  # Derivatives of y
                                  H = 1.0
                                  for j in range(4):              # Build Taylor series
                                      H = H*h/(j + 1)
                                      y = y + D[j]*H              # H = hˆj/j!
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                   December 16, 2009   15:4




           246      Initial Value Problems

                              x = x + h
                              X.append(x)                           # Append results to
                              Y.append(y)                           # lists X and Y
                          return array(X),array(Y)                  # Convert lists into arrays




                      printSoln

                    We use this function to print X and Y obtained from numerical integration. The
                    amount of data is controlled by the parameter freq. For example, if freq = 5, ev-
                    ery fifth integration step would be displayed. If freq = 0, only the initial and final
                    values will be shown.



                    ## module printSoln
                    ’’’ printSoln(X,Y,freq).
                          Prints X and Y returned from the differential
                          equation solvers using printput frequency ’freq’.
                              freq = n prints every nth step.
                              freq = 0 prints initial and final values only.
                    ’’’
                    def printSoln(X,Y,freq):


                          def printHead(n):
                              print ’’\n                  x   ’’,
                              for i in range (n):
                                   print ’’               y[’’,i,’’] ’’,
                              print


                          def printLine(x,y,n):
                              print ’’%13.4e’’% x,
                              for i in range (n):
                                   print ’’%13.4e’’% y[i],
                              print


                          m = len(Y)
                          try: n = len(Y[0])
                          except TypeError: n = 1
                          if freq == 0: freq = m
                          printHead(n)
                          for i in range(0,m,freq):
                              printLine(X[i],Y[i],n)
                          if i != m - 1: printLine(X[m - 1],Y[m - 1],n)
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                   December 16, 2009   15:4




               247     7.2 Taylor Series Method

                      EXAMPLE 7.1
                      Given that

                                                         y + 4y = x 2      y(0) = 1

                      determine y(0.1) with the fourth-order Taylor series method using a single integra-
                      tion step. Also compute the estimated error from Eq. (7.7) and compare it with the
                      actual error. The analytical solution of the differential equation is
                                                            31 −4x 1 2 1   1
                                                       y=      e  + x − x+
                                                            32     4   8   32
                      Solution The Taylor series up to and including the term with h4 is
                                                               1           1         1
                                   y(h) = y(0) + y (0)h +         y (0)h2 + y (0)h3 + y (4) (0)h4          (a)
                                                               2!          3!        4!
                      Differentiation of the differential equation yields

                                           y = −4y + x 2

                                          y = −4y + 2x = 16y − 4x 2 + 2x

                                          y   = 16y − 8x + 2 = −64y + 16x 2 − 8x + 2

                                         y (4) = −64y + 32x − 8 = 256y − 64x 2 + 32x − 8

                      Thus, at x = 0 we have

                                                         y (0) = −4(1) = −4

                                                         y (0) = 16(1) = 16

                                                        y (0) = −64(1) + 2 = −62

                                                        y (4) (0) = 256(1) − 8 = 248

                      With h = 0.1, Eq. (a) becomes
                                                            1              1             1
                              y(0.2) = 1 + (−4)(0.1) +         (16)(0.1)2 + (−62)(0.1)3 + (248)(0.1)4
                                                            2!             3!            4!
                                     = 0.670700

                          According to Eq. (7.7), the approximate truncation error is
                                                             h4 (4)
                                                        E=      y (0.1) − y (4) (0)
                                                             5!
                      where

                                      y (4) (0) = 248

                                    y (4) (0.1) = 256(0.6707) − 64(0.1)2 + 32(0.1) − 8 = 166.259

                      Therefore,
                                                   (0.1)4
                                              E=          (166.259 − 248) = −6.8 × 10−5
                                                     5!
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                       December 16, 2009   15:4




           248      Initial Value Problems

                    The analytical solution yields
                                              31 −4(0.1) 1        1        1
                                   y(0.1) =      e      + (0.1)2 − (0.1) +    = 0.670623
                                              32         4        8        32
                    so that the actual error is 0.670623 − 0.670700 = −7.7 × 10−5 .

                    EXAMPLE 7.2
                    Solve

                                           y = −0.1y − x          y(0) = 0       y (0) = 1

                    from x = 0 to 2 with the Taylor series method of order 4. Use h = 0.25 and utilize the
                    functions taylor and printSoln.

                    Solution With the notation y0 = y and y1 = y the equivalent first-order equations
                    and the initial conditions are
                                                 y0           y1                        0
                                           y =        =                        y(0) =
                                                 y1       −0.1y1 − x                    1

                    Repeated differentiation of the differential equations yields

                                                     y1                    −0.1y1 − x
                                           y =                    =
                                                 −0.1y1 − 1             0.01y1 + 0.1x − 1


                                              −0.1y1 − 1                0.01y1 + 0.1x − 1
                                     y =                      =
                                              0.01y1 + 0.1            −0.001y1 − 0.01x + 0.1


                                             0.01y1 + 0.1               −0.001y1 − 0.01x + 0.1
                                  y(4) =                      =
                                            −0.001y1 − 0.01            0.0001y1 + 0.001x − 0.01

                    Thus, the derivative array that has to be computed by the function deriv is
                                      ⎡                                                  ⎤
                                                   y1                   −0.1y1 − x
                                      ⎢       −0.1y1 − x             0.01y1 + 0.1x − 1   ⎥
                                      ⎢                                                  ⎥
                                 D=⎢                                                     ⎥
                                      ⎣ 0.01y1 + 0.1x − 1         −0.001y1 − 0.01x + 0.1 ⎦
                                        −0.001y1 − 0.01x + 0.1 0.0001y1 + 0.001x − 0.01

                        Here is the program that performs the integration:

                    #!/usr/bin/python
                    ## example7_2
                    from numpy import array, zeros
                    from printSoln import *
                    from taylor import *


                    def deriv(x,y):
                         D = zeros((4,2))
                         D[0] = [y[1]        , -0.1*y[1] - x]
                         D[1] = [D[0,1],         0.01*y[1] + 0.1*x - 1.0]
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                  December 16, 2009    15:4




               249     7.3 Runge–Kutta Methods

                           D[2] = [D[1,1], -0.001*y[1] - 0.01*x + 0.1]
                           D[3] = [D[2,1],            0.0001*y[1] + 0.001*x - 0.01]
                           return D


                      x = 0.0                             # Start of integration
                      xStop = 2.0                         # End of integration
                      y = array([0.0, 1.0]) # Initial values of {y}
                      h = 0.25                            # Step size
                      freq = 1                            # Printout frequency
                      X,Y = taylor(deriv,x,y,xStop,h)
                      printSoln(X,Y,freq)
                      raw_input("\nPress return to exit")


                          The results are:

                                  x              y[ 0 ]             y[ 1 ]
                         0.0000e+000          0.0000e+000         1.0000e+000
                         2.5000e-001          2.4431e-001         9.4432e-001
                         5.0000e-001          4.6713e-001         8.2829e-001
                         7.5000e-001          6.5355e-001         6.5339e-001
                         1.0000e+000          7.8904e-001         4.2110e-001
                         1.2500e+000          8.5943e-001         1.3281e-001
                         1.5000e+000          8.5090e-001        -2.1009e-001
                         1.7500e+000          7.4995e-001        -6.0625e-001
                         2.0000e+000          5.4345e-001        -1.0543e+000


                          The analytical solution of the problem is

                                                     y = 100x − 5x 2 + 990(e−0.1x − 1)

                      from which we obtain y(2) = 0.543 446, which agrees well with the numerical solu-
                      tion.



              7.3     Runge–Kutta Methods

                      The main drawback of the Taylor series method is that it requires repeated differ-
                      entiation of the dependent variables. These expressions may become very long and
                      are, therefore, error-prone and tedious to compute. Moreover, there is the extra work
                      of coding each of the derivatives. The aim of Runge–Kutta methods is to eliminate
                      the need for repeated differentiation of the differential equations. Because no such
                      differentiation is involved in the first-order Taylor series integration formula

                                                y(x + h) = y(x) + y (x)h = y(x) + F(x, y)h                   (7.8)
P1: PHB

CUUS884-Kiusalaas     CUUS884-07         978 0 521 19132 6                                                     December 16, 2009         15:4




           250      Initial Value Problems


                    y' (x )
                                                             Error
                                                                                                Figure 7.1. Graphical representation
                                                             Euler's formula                    of Euler formula.
                              f (x,y )
                                                                x
                                     x             x+h

                    it can also be considered as the first-order Runge–Kutta method; it is also called
                    Euler’s method. Because of excessive truncation error, this method is rarely used in
                    practice.
                         Let us now take a look at the graphical interpretation of Euler’s equation. For
                    the sake of simplicity, we assume that there is a single dependent variable y, so that
                    the differential equation is y = f (x, y). The change in the solution y between x and
                    x + h is
                                                                         x+h                    x+h
                                           y(x + h) − y(h) =                   y dx =                 f (x, y)dx
                                                                     x                      x

                    which is the area of the panel under the y (x) plot, shown in Fig. 7.1. Euler’s formula
                    approximates this area by the area of the cross-hatched rectangle. The area between
                    the rectangle and the plot represents the truncation error. Clearly, the truncation er-
                    ror is proportional to the slope of the plot, that is, proportional to y (x).


                    Second-Order Runge–Kutta Method
                    To arrive at the second-order method, we assume an integration formula of the form

                                   y(x + h) = y(x) + c0 F(x, y)h + c1 F x + ph, y + qhF(x, y) h                                    (a)

                    and attempt to find the parameters c0 , c1 , p, and q by matching Eq. (a) to the Taylor
                    series
                                                                        1
                                          y(x + h) = y(x) + y (x)h +      y (x)h2 + O(h3 )
                                                                       2!
                                                                          1
                                                     = y(x) + F(x, y)h + F (x, y)h2 + O(h3 )                                       (b)
                                                                          2
                         Noting that
                                                               n−1                          n−1
                                                        ∂F           ∂F     ∂F                    ∂F
                                           F (x, y) =      +            y =    +                      Fi (x, y)
                                                        ∂x           ∂yi i  ∂x                    ∂yi
                                                               i=0                          i=0

                    where n is the number of first-order equations, Eq. (b) can be written as
                                                                                      n−1
                                                                     1         ∂F           ∂F
                                y(x + h) = y(x) + F(x, y)h +                      +             Fi (x, y) h2 + O(h3 )              (c)
                                                                     2         ∂x           ∂yi
                                                                                      i=0
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                          December 16, 2009     15:4




               251     7.3 Runge–Kutta Methods

                          Returning to Eq. (a), we can rewrite the last term by applying Taylor series in
                      several variables:
                                                                                    n−1
                                                                       ∂F                 ∂F
                             F x + ph, y + qhF(x, y) = F(x, y) +          ph + qh             Fi (x, y) + O(h2 )
                                                                       ∂x                 ∂yi
                                                                                    i=1

                      so that Eq. (a) becomes
                                                                                   n−1
                                                                      ∂F                 ∂F
                        y(x + h) = y(x) + (c0 + c1 ) F(x, y)h + c1       ph + qh             Fi (x, y) h + O(h3 ) (d)
                                                                      ∂x                 ∂yi
                                                                                   i=1

                          Comparing Eqs. (c) and (d), we find that they are identical if

                                                                          1               1
                                                c0 + c1 = 1      c1 p =        c1q =                                 (e)
                                                                          2               2
                      Because Eqs. (e) represent three equations in four unknown parameters, we can as-
                      sign any value to one of the parameters. Some of the popular choices and the names
                      associated with the resulting formulas are:

                              c0 = 0      c1 = 1       p = 1/2       q = 1/2   Modified Euler’s method
                              c0 = 1/2    c1 = 1/2     p=1           q =1      Heun’s method
                              c0 = 1/3    c1 = 2/3     p = 3/4       q = 3/4   Ralston’s method

                      All these formulas are classified as second-order Runge–Kutta methods, with no for-
                      mula having numerical superiority over the others. Choosing the modified Euler’s
                      method, substitution of the corresponding parameters into Eq. (a) yields

                                                                          h      h
                                            y(x + h) = y(x) + F x +         , y + F(x, y) h                          (f)
                                                                          2      2

                      This integration formula can be conveniently evaluated by the following sequence of
                      operations:

                                                        K0 = hF(x, y)
                                                                          h      1
                                                        K1 = hF x +         , y + K0                               (7.9)
                                                                          2      2
                                                  y(x + h) = y(x) + K1

                      Second-order methods are not popular in computer applications. Most program-
                      mers prefer integration formulas of order 4, which achieve a given accuracy with less
                      computational effort.
                           Figure 7.2 displays the graphical interpretation of the modified Euler formula for
                      a single differential equation y = f (x, y). The first of Eqs. (7.9) yields an estimate of
                      y at the midpoint of the panel by Euler’s formula: y(x + h/2) = y(x) + f (x, y)h/2 =
                      y(x) + K 0 /2. The second equation then approximates the area of the panel by the
                      area K 1 of the cross-hatched rectangle. The error here is proportional to the curvature
                      y of the plot.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                       December 16, 2009      15:4




           252      Initial Value Problems


                                  y' (x )



                                                           h/2 h/2 f (x + h /2, y +K 0 /2)
                                            f (x,y )
                                                                                  x
                                                       x           x+h
                                  Figure 7.2. Graphical representation of modified Euler formula.



                    Fourth-Order Runge–Kutta Method
                    The fourth-order Runge–Kutta method is obtained from the Taylor series along the
                    same lines as the second-order method. Because the derivation is rather long and not
                    very instructive, we skip it. The final form of the integration formula again depends
                    on the choice of the parameters, that is, there is no unique Runge–Kutta fourth-
                    order formula. The most popular version, which is known simply as the Runge–Kutta
                    method, entails the following sequence of operations:

                                                 K0 = hF(x, y)
                                                                  h     K0
                                                 K1 = hF x +        ,y+
                                                                  2     2
                                                                  h     K1
                                                 K2 = hF x +        ,y+                                   (7.10)
                                                                  2     2
                                                 K3 = hF(x + h, y + K2 )
                                                                 1
                                            y(x + h) = y(x) +      (K0 + 2K1 + 2K2 + K3 )
                                                                 6
                        The main drawback of this method is that is does not lend itself to an estimate of
                    the truncation error. Therefore, we must guess the integration step size h, or deter-
                    mine it by trial and error. In contrast, adaptive methods can evaluate the truncation
                    error in each integration step and adjust the value of h accordingly (but at a higher
                    cost of computation). One such adaptive method is introduced in the next section.


                      run kut4

                    The function integrate in this module implements the Runge–Kutta method of or-
                    der 4. The user must provide integrate with the function F(x,y) that defines the
                    first-order differential equations y = F(x, y).

                    ## module run_kut4
                    ’’’ X,Y = integrate(F,x,y,xStop,h).
                       4th-order Runge--Kutta method for solving the
                       initial value problem {y}’ = {F(x,{y})}, where
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                  December 16, 2009   15:4




               253     7.3 Runge–Kutta Methods

                            {y} = {y[0],y[1],...y[n-1]}.
                            x,y   = initial conditions.
                            xStop = terminal value of x.
                            h     = increment of x used in integration.
                            F     = user-supplied function that returns the
                                      array F(x,y) = {y’[0],y’[1],...,y’[n-1]}.
                      ’’’
                      from numpy import array
                      def integrate(F,x,y,xStop,h):


                             def run_kut4(F,x,y,h):
                         # Computes increment of y from Eqs. (7.10)
                                  K0 = h*F(x,y)
                                  K1 = h*F(x + h/2.0, y + K0/2.0)
                                  K2 = h*F(x + h/2.0, y + K1/2.0)
                                  K3 = h*F(x + h, y + K2)
                                  return (K0 + 2.0*K1 + 2.0*K2 + K3)/6.0


                             X = []
                             Y = []
                             X.append(x)
                             Y.append(y)
                             while x < xStop:
                                  h = min(h,xStop - x)
                                  y = y + run_kut4(F,x,y,h)
                                  x = x + h
                                  X.append(x)
                                  Y.append(y)
                             return array(X),array(Y)

                      EXAMPLE 7.3
                      Use the second-order Runge–Kutta method to integrate

                                                       y = sin y      y(0) = 1

                      from x = 0 to 0.5 in steps of h = 0.1. Keep four decimal places in the computations.

                      Solution In this problem, we have

                                                           F (x, y) = sin y

                      so that the integration formulas in Eqs. (7.9) are

                                            K 0 = hF (x, y) = 0.1 sin y
                                                            h      1                       1
                                            K 1 = hF x +      , y + K0     = 0.1 sin y +     K0
                                                            2      2                       2
                                      y(x + h) = y(x) + K 1
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                             December 16, 2009   15:4




           254      Initial Value Problems

                    We note that y(0) = 1; the integration then proceeds as follows:

                                             K 0 = 0.1 sin 1.0000 = 0.0841
                                                                              0.0841
                                             K 1 = 0.1 sin 1.0000 +                     = 0.0863
                                                                                 2
                                        y(0.1) = 1.0 + 0.0863 = 1.0863


                                             K 0 = 0.1 sin 1.0863 = 0.0885
                                                                              0.0885
                                             K 1 = 0.1 sin 1.0863 +                     = 0.0905
                                                                                 2
                                        y(0.2) = 1.0863 + 0.0905 = 1.1768

                    and so on. A summary of the computations is shown in the following table.

                                                 x         y             K0            K1
                                                0.0      1.0000        0.0841     0.0863
                                                0.1      1.0863        0.0885     0.0905
                                                0.2      1.1768        0.0923     0.0940
                                                0.3      1.2708        0.0955     0.0968
                                                0.4      1.3676        0.0979     0.0988
                                                0.5      1.4664

                        The exact solution can be shown to be

                                              x(y) = ln(csc y − cot y) + 0.604582

                    which yields x(1.4664) = 0.5000. Therefore, up to this point the numerical solution is
                    accurate to four decimal places. However, it is unlikely that this precision would be
                    maintained if we were to continue the integration. Because the errors (due to trun-
                    cation and roundoff) tend to accumulate, longer integration ranges require better
                    integration formulas and more significant figures in the computations.

                    EXAMPLE 7.4
                    Solve

                                          y = −0.1y − x                y(0) = 0        y (0) = 1

                    from x = 0 to 2 in increments of h = 0.25 with the Runge–Kutta method of order 4.
                    (This problem was solved by the Taylor series method in Example 7.2.)

                    Solution Letting y 0 = y and y1 = y , the equivalent first-order equations are

                                                                  y0               y1
                                             y = F(x, y) =               =
                                                                  y1           −0.1y1 − x

                    Comparing the function F(x,y)here with deriv(x,y)in Example 7.2, we note that it
                    is much simpler to input the differential equations in the Runge–Kutta method than
                    in the Taylor series method.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                December 16, 2009   15:4




               255     7.3 Runge–Kutta Methods

                      #!/usr/bin/python
                      ## example7_4
                      from numpy import array,zeros
                      from printSoln import *
                      from run_kut4 import *


                      def F(x,y):
                           F = zeros(2)
                           F[0] = y[1]
                           F[1] = -0.1*y[1] - x
                           return F


                      x = 0.0                            # Start of integration
                      xStop = 2.0                        # End of integration
                      y = array([0.0, 1.0])              # Initial values of {y}
                      h = 0.25                           # Step size
                      freq = 1                           # Printout frequency


                      X,Y = integrate(F,x,y,xStop,h)
                      printSoln(X,Y,freq)
                      raw_input("Press return to exit")




                          The output from the fourth-order method follows. The results are the same
                      as those obtained by the Taylor series method in Example 7.2. This was expected,
                      because both methods are of the same order.

                                x             y[ 0 ]             y[ 1 ]
                         0.0000e+000        0.0000e+000         1.0000e+000
                         2.5000e-001        2.4431e-001         9.4432e-001
                         5.0000e-001        4.6713e-001         8.2829e-001
                         7.5000e-001        6.5355e-001         6.5339e-001
                         1.0000e+000        7.8904e-001         4.2110e-001
                         1.2500e+000        8.5943e-001         1.3281e-001
                         1.5000e+000        8.5090e-001        -2.1009e-001
                         1.7500e+000        7.4995e-001        -6.0625e-001
                         2.0000e+000        5.4345e-001        -1.0543e+000


                      EXAMPLE 7.5
                      Use the fourth-order Runge–Kutta method to integrate

                                                        y = 3y − 4e−x     y(0) = 1

                      from x = 0 to 10 in steps of h = 0.1. Compare the result with the analytical solution
                      y = e−x .
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                 December 16, 2009       15:4




           256      Initial Value Problems

                    Solution We used the program shown here. Recalling that run kut4 assumes y to be
                    an array, we specified the initial value as y = array([1.0]) rather than y = 1.0.

                    #!/usr/bin/python
                    ## example7_5
                    from numpy import zeros,array
                    from run_kut4 import *
                    from printSoln import *
                    from math import exp


                    def F(x,y):
                         F = zeros(1)
                         F[0] = 3.0*y[0] - 4.0*exp(-x)
                         return F


                    x = 0.0                  # Start of integration
                    xStop = 10.0             # End of integration
                    y = array([1.0])         # Initial values of {y}
                    h = 0.1                  # Step size
                    freq = 10                # Printout frequency


                    X,Y = integrate(F,x,y,xStop,h)
                    printSoln(X,Y,freq)
                    raw_input("\nPress return to exit")

                        Running the program produced the following output:

                              x           y[ 0 ]
                      0.0000e+000        1.0000e+000
                      2.0000e+000        1.3250e-001
                      4.0000e+000      -1.1237e+000
                      6.0000e+000      -4.6056e+002
                      8.0000e+000      -1.8575e+005
                      1.0000e+001      -7.4912e+007

                         It is clear that something went wrong. According to the analytical solution, y
                    should approach zero with increasing x, but the output shows the opposite trend:
                    After an initial decrease, the magnitude of y increases dramatically. The explanation
                    is found by taking a closer look at the analytical solution. The general solution of the
                    given differential equation is

                                                         y = Ce3x + e−x

                    which can be verified by substitution. The initial condition y(0) = 1 yields C = 0, so
                    that the solution to the problem is indeed y = e−x .
                        The cause of trouble in the numerical solution is the dormant term Ce3x . Sup-
                    pose that the initial condition contains a small error ε, so that we have y(0) = 1 + ε.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                              December 16, 2009   15:4




               257     7.3 Runge–Kutta Methods

                      This changes the analytical solution to

                                                               y = εe3x + e−x

                      We now see that the term containing the error ε becomes dominant as x is increased.
                      Because errors inherent in the numerical solution have the same effect as small
                      changes in initial conditions, we conclude that our numerical solution is the victim
                      of numerical instability due to sensitivity of the solution to initial conditions. The
                      lesson is: Do not blindly trust the results of numerical integration.

                      EXAMPLE 7.6



                                                           Re            r                    v0
                                                                             θ




                          A spacecraft is launched at the altitude H = 772 km above the sea level with the
                      speed v0 = 6700 m/s in the direction shown. The differential equations describing
                      the motion of the spacecraft are
                                                           2      G Me                 r θ˙
                                                                                      2˙
                                                    r¨ = r θ˙ −              θ¨ = −
                                                                   r2                  r
                      where r and θ are the polar coordinates of the spacecraft. The constants involved in
                      the motion are

                                  G = 6.672 × 10−11 m3 kg−1 s−2 = universal gravitational constant

                                  Me = 5.9742 × 1024 kg = mass of the earth

                                  Re = 6378.14 km = radius of the earth at sea level

                      (1) Derive the first-order differential equations and the initial conditions of the form
                      y˙ = F(t, y), y(0) = b. (2) Use the fourth-order Runge–Kutta method to integrate the
                      equations from the time of launch until the spacecraft hits the earth. Determine θ at
                      the impact site.

                      Solution of Part (1) We have

                                   G Me = 6.672 × 10−11        5.9742 × 1024 = 3.9860 × 1014 m3s −2

                      Letting
                                                                  ⎡
                                                                 ⎤ ⎡ ⎤
                                                              y0     r
                                                            ⎢ y ⎥ ⎢ r˙ ⎥
                                                            ⎢ 1⎥ ⎢ ⎥
                                                          y=⎢ ⎥=⎢ ⎥
                                                            ⎣ y2 ⎦ ⎣ θ ⎦
                                                              y3     θ˙
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                  December 16, 2009   15:4




           258      Initial Value Problems

                    the equivalent first-order equations become
                                                ⎡ ⎤ ⎡                               ⎤
                                                  y˙0                y1
                                                ⎢ y˙ ⎥ ⎢ y y 2 − 3.9860 × 1014 /y 2 ⎥
                                                ⎢ 1⎥ ⎢ 0 3                       0⎥
                                           y˙ = ⎢ ⎥ = ⎢                             ⎥
                                                ⎣ y˙2 ⎦ ⎣            y3             ⎦
                                                  y˙3           −2y1 y3 /y0

                    and the initial conditions are

                              r (0) = Re + H = (6378.14 + 772) × 103 = 7. 15014 × 106 m

                              r˙(0) = 0

                              θ(0) = 0

                              θ˙ (0) = v0 /r (0) = (6700) /(7.15014 × 106 ) = 0.937045 × 10−3 rad/s

                    Therefore,
                                                         ⎡                  ⎤
                                                            7. 15014 × 106
                                                          ⎢0                ⎥
                                                          ⎢                 ⎥
                                                   y(0) = ⎢                 ⎥
                                                          ⎣0                ⎦
                                                            0.937045 × 10−3

                    Solution of Part (2) The program used for numerical integration is listed here.
                    Note that the independent variable t is denoted by x. The period of integration
                    xStop (the time when the spacecraft hits) was estimated from a previous run of the
                    program.

                    #!/usr/bin/python
                    ## example7_6
                    from numpy import zeros,array
                    from run_kut4 import *
                    from printSoln import *


                    def F(x,y):
                         F = zeros(4)
                         F[0] = y[1]
                         F[1] = y[0]*(y[3]**2) - 3.9860e14/(y[0]**2)
                         F[2] = y[3]
                         F[3] = -2.0*y[1]*y[3]/y[0]
                         return F


                    x = 0.0
                    xStop = 1200.0
                    y = array([7.15014e6, 0.0, 0.0, 0.937045e-3])
                    h = 50.0
                    freq = 2
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                   December 16, 2009    15:4




               259     7.3 Runge–Kutta Methods

                      X,Y = integrate(F,x,y,xStop,h)
                      printSoln(X,Y,freq)
                      raw_input("\nPress return to exit")


                          Here is the output:

                            x           y[ 0 ]             y[ 1 ]            y[ 2 ]          y[ 3 ]
                      0.0000e+000      7.1501e+006        0.0000e+000        0.0000e+000    9.3704e-004
                      1.0000e+002      7.1426e+006 -1.5173e+002              9.3771e-002    9.3904e-004
                      2.0000e+002      7.1198e+006 -3.0276e+002              1.8794e-001    9.4504e-004
                      3.0000e+002      7.0820e+006 -4.5236e+002              2.8292e-001    9.5515e-004
                      4.0000e+002      7.0294e+006 -5.9973e+002              3.7911e-001    9.6951e-004
                      5.0000e+002      6.9622e+006 -7.4393e+002              4.7697e-001    9.8832e-004
                      6.0000e+002      6.8808e+006 -8.8389e+002              5.7693e-001    1.0118e-003
                      7.0000e+002      6.7856e+006 -1.0183e+003              6.7950e-001    1.0404e-003
                      8.0000e+002      6.6773e+006 -1.1456e+003              7.8520e-001    1.0744e-003
                      9.0000e+002      6.5568e+006 -1.2639e+003              8.9459e-001    1.1143e-003
                      1.0000e+003      6.4250e+006 -1.3708e+003              1.0083e+000    1.1605e-003
                      1.1000e+003      6.2831e+006 -1.4634e+003              1.1269e+000    1.2135e-003
                      1.2000e+003      6.1329e+006 -1.5384e+003              1.2512e+000    1.2737e-003


                          The spacecraft hits the earth when r equals Re = 6.378 14 × 106 m. This occurs
                      between t = 1000 and 1100 s. A more accurate value of t can be obtained by polyno-
                      mial interpolation. If no great precision is needed, linear interpolation will do. Letting
                      1000 + t be the time of impact, we can write

                                                          r (1000 + t ) = Re

                      Expanding r in a two-term Taylor series, we get

                                                       r (1000) + r (1000) t = Re

                                        6.4250 × 106 + −1.3708 × 103 x = 6378.14 × 103

                      from which

                                                              t = 34.184 s


                          The coordinate θ of the impact site can be estimated in a similar manner. Again,
                      using two terms of the Taylor series, we have

                                        θ(1000 + t ) = θ (1000) + θ (1000) t

                                                        = 1.0083 + 1.1605 × 10−3 (34.184)

                                                        = 1.0480 rad = 60.00◦
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                        December 16, 2009   15:4




           260      Initial Value Problems

                    PROBLEM SET 7.1
                    1. Given

                                                         y + 4y = x 2       y(0) = 1

                       compute y(0.1) using two steps of the Taylor series method of order 2. Compare
                       the result with Example 7.1.
                    2. Solve Prob. 1 with one step of the Runge–Kutta method of order (a) 2 and (b) 4.
                    3. Integrate

                                                          y = sin y        y(0) = 1

                       from x = 0 to 0.5 with the second-order Taylor series method using h = 0.1.
                       Compare the result with Example 7.3.
                    4. Verify that the problem

                                                           y = y 1/3      y(0) = 0

                       has two solutions: y = 0 and y = (2x/3)3/2 . Which of the solutions would be re-
                       produced by numerical integration if the initial condition is set at (a) y = 0 and
                       (b) y = 10−16 ? Verify your conclusions by integrating with any numerical method.
                    5. Convert the following differential equations into first-order equations of the
                       form y = F(x, y):

                                                     (a)     ln y + y = sin x
                                                     (b)     y y − xy − 2y 2 = 0
                                                     (c)     y (4) − 4y 1 − y 2 = 0
                                                                  2
                                                     (d)      y       = 32y x − y 2

                    6. In the following sets of coupled differential equations, t is the independent vari-
                       able. Convert these equations into first-order equations of the form y˙ = F(t, y):

                                       (a)    y¨ = x − 2y                 x¨ = y − x
                                                                   1/4                      1/4
                                       (b)    y¨ = −y y˙ 2 + x˙ 2         x¨ = −x y˙ 2 + x˙     − 32
                                       (c)    y¨ 2 + t sin y = 4x˙        x x¨ + t cos y = 4 y˙

                    7.     The differential equation for the motion of a simple pendulum is
                                                              d 2θ    g
                                                                 2
                                                                   = − sin θ
                                                              dt      L
                         where

                                             θ = angular displacement from the vertical

                                             g = gravitational acceleration

                                           L = length of the pendulum
                                                      √
                         With the transformation τ = t g/L, the equation becomes
                                                               d 2θ
                                                                    = − sin θ
                                                               dτ 2
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                           December 16, 2009   15:4




               261     7.3 Runge–Kutta Methods

                          Use numerical integration to determine the period of the pendulum if the am-
                          plitude is θ 0 = 1 rad. Note that for small amplitudes (sin θ ≈ θ) the period is
                             √
                          2π L/g.
                       8.    A skydiver of mass m in a vertical free fall experiences an aerodynamic drag
                          force F D = c D y˙ 2 , where y is measured downward from the start of the fall. The
                          differential equation describing the fall is
                                                                              cD 2
                                                                   y¨ = g −     y˙
                                                                              m
                            Determine the time of a 5000-m fall. Use g = 9.80665 m/s2 , C D = 0.2028 kg/m,
                            and m = 80 kg.
                       9.

                                                                                     y
                                                         k                               P(t)
                                                                           m



                            The spring–mass system is at rest when the force P(t ) is applied, where

                                                                   10t N when t < 2 s
                                                        P(t ) =
                                                                   20 N when t ≥ 2 s

                            The differential equation of the ensuing motion is
                                                                         P(t )  k
                                                                  y¨ =         − y
                                                                          m     m
                            Determine the maximum displacement of the mass. Use m = 2.5 kg and k = 75
                            N/m.
                      10.




                                           Water level


                                                                    y




                            The conical float is free to slide on a vertical rod. When the float is disturbed
                            from its equilibrium position, it undergoes oscillating motion described by the
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                            December 16, 2009   15:4




           262      Initial Value Problems

                          differential equation

                                                                   y¨ = g 1 − ay 3

                          where a = 16 m−3 (determined by the density and dimensions of the float) and
                          g = 9.80665 m/s2 . If the float is raised to the position y = 0.1 m and released,
                          determine the period and the amplitude of the oscillations.
                    11.

                                                                                y(t)



                                                                          θ
                                                                               L


                                                                                       m

                          The pendulum is suspended from a sliding collar. The system is at rest when the
                          oscillating motion y(t ) = Y sin ωt is imposed on the collar, starting at t = 0. The
                          differential equation describing the motion of the pendulum is
                                                               g         ω2
                                                      θ¨ = −     sin θ +    Y cos θ sin ωt
                                                               L         L
                          Plot θ versus t from t = 0 to 10 s and determine the largest θ during this period.
                          Use g = 9.80665 m/s2 , L = 1.0 m, Y = 0.25 m, and ω = 2.5 rad/s.
                    12.




                                                                       2m
                                                               r

                                                                        θ(t)


                          The system consisting of a sliding mass and a guide rod is at rest with the mass
                          at r = 0.75 m. At time t = 0, a motor is turned on that imposes the motion
                          θ (t ) = (π/12) cos πt on the rod. The differential equation describing the result-
                          ing motion of the slider is
                                                               2
                                                        π2                             π
                                               r¨ =                r sin2 πt − g sin      cos πt
                                                        12                             12
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                          December 16, 2009   15:4




               263     7.3 Runge–Kutta Methods

                            Determine the time when the slider reaches the tip of the rod. Use g = 9.80665
                            m/s2 .
                      13.

                                            y
                                                    v0
                                          m         30o                 R                          x

                            A ball of mass m = 0.25 kg is launched with the velocity v0 = 50 m/s in the di-
                            rection shown. Assuming that the aerodynamic drag force acting on the ball is
                            F D = C Dv 3/2 , the differential equations describing the motion are
                                                           C D 1/2                    C D 1/2
                                                  x¨ = −      ˙
                                                              xv             y¨ = −      ˙
                                                                                         yv   −g
                                                           m                          m
                          where v = x˙ 2 + y˙ 2 . Determine the time of flight and the range R. Use C D = 0.03
                          kg/(m·s)1/2 and g = 9.80665 m/s2 .
                      14.   The differential equation describing the angular position θ of a mechanical
                          arm is
                                                                                         2
                                                                       a(b − θ) − θ θ˙
                                                                θ¨ =
                                                                           1 + θ2
                            where a = 100 s−2 and b = 15. If θ (0) = 2π and θ˙ (0) = 0, compute θ and θ˙ when
                            t = 0.5 s.
                      15.


                                                              L = undeformed length
                                                              k = stiffness
                                                   θ      r


                                                                  m

                            The mass m is suspended from an elastic cord with an extensional stiffness k and
                            undeformed length L. If the mass is released from rest at θ = 60◦ with the cord
                            unstretched, find the length r of the cord when the position θ = 0 is reached for
                            the first time. The differential equations describing the motion are
                                                                  2                k
                                                         r¨ = r θ˙ + g cos θ −       (r − L)
                                                                                   m
                                                                −2˙
                                                                  r θ˙ − g sin θ
                                                         θ¨ =
                                                                       r
                            Use g = 9.80665 m/s2 , k = 40 N/m, L = 0.5 m, and m = 0.25 kg.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                         December 16, 2009   15:4




           264      Initial Value Problems

                    16.      Solve Prob. 15 if the mass is released from the position θ = 60◦ with the cord
                          stretched by 0.075 m.
                    17.


                                                                                       y
                                                          k
                                                                          m

                                                                    µ

                          Consider the mass–spring system where dry friction is present between the block
                          and the horizontal surface. The frictional force has a constant magnitude µmg
                          (µ is the coefficient of friction) and always opposes the motion. The differential
                          equation for the motion of the block can be expressed as

                                                                    k          y˙
                                                           y¨ = −     y − µg
                                                                    m        | y|
                                                                                ˙

                        where y is measured from the position where the spring is unstretched. If
                        the block is released from rest at y = y0 , verify by numerical integration that
                        the next positive peak value of y is y0 − 4µmg/k (this relationship can be de-
                        rived analytically). Use k = 3000 N/m, m = 6 kg, µ = 0.5, g = 9.80665 m/s2 , and
                        y0 = 0.1 m.
                    18.    Integrate the following problems from x = 0 to 20 and plot y versus x:

                                       (a)    y + 0.5(y 2 − 1) + y = 0        y(0) = 1        y (0) = 0
                                       (b)    y = y cos 2x                    y(0) = 0        y (0) = 1

                          These differential equations arise in nonlinear vibration analysis.
                    19.     The solution of the problem

                                                   1        1
                                             y +     y + 1− 2       y     y(0) = 0         y (0) = 1
                                                   x       x

                        is the Bessel function J 1 (x). Use numerical integration to compute J 1 (5) and
                        compare the result with −0.327 579, the value listed in mathematical tables. Hint:
                        to avoid singularity at x = 0, start the integration at x = 10−12 .
                    20.    Consider the initial value problem

                                              y = 16.81y       y(0) = 1.0           y (0) = −4.1

                          (a) Derive the analytical solution. (b) Do you anticipate difficulties in numerical
                          solution of this problem? (c) Try numerical integration from x = 0 to 8 to see if
                          your concerns were justified.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                           December 16, 2009   15:4




               265     7.3 Runge–Kutta Methods

                      21.

                                                             2R
                                          i2
                                                   i1                 R                             R
                                          E(t)
                                                                     i1
                                                                            L                       C
                                                                                                    i2

                            Kirchoff’s equations for the circuit shown are
                                                             di1
                                                         L       + Ri1 + 2R(i1 + i2 ) = E (t )                       (a)
                                                             dt
                                                              q2
                                                                 + Ri2 + 2R(i2 + i1 ) = E (t )                       (b)
                                                              C
                            where i1 and i2 are the loop currents, and q2 is the charge of the condenser. Dif-
                            ferentiating Eq. (b) and substituting the charge–current relationship dq2 /dt = i2 ,
                            we get
                                                         di1   −3Ri1 − 2Ri2 + E (t )
                                                             =                                                       (c)
                                                         dt             L
                                                         di2    2 di1     i2     1 dE
                                                             =−       −      +                                       (d)
                                                         dt     3 dt    3RC     3R dt
                            We could substitute di1 /dt from Eq. (c) into Eq. (d), so that the latter would as-
                            sume the usual form di2 /dt = f (t, i1 , i2 ), but it is more convenient to leave the
                            equations as they are. Assuming that the voltage source is turned on at time t = 0,
                            plot the loop currents ii and i2 from t = 0 to 0.05 s. Use E (t ) = 240 sin(120πt ) V,
                            R = 1.0 , L = 0.2 × 10−3 H, and C = 3.5 × 10−3 F.
                      22.

                                                             L                     L

                                                 i1                        i2
                                           E                        C                              C
                                                                    i1                     i2

                                                             R                     R

                            The constant voltage source of the circuit shown is turned on at t = 0, causing
                            transient currents i1 and i2 in the two loops that last about 0.05 s. Plot these
                            currents from t = 0 to 0.05 s, using the following data: E = 9 V, R = 0.25 ,
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                   December 16, 2009      15:4




           266      Initial Value Problems

                        L = 1.2 × 10−3 H, and C = 5 × 10−3 F. Kirchoff’s equations for the two loops are
                                                          di1         q1 − q2
                                                           L  + Ri1 +         =E
                                                          dt             C
                                                     di2         q2 − q1 q2
                                                   L     + Ri2 +         +    =0
                                                     dt             C      C
                        Two additional equations are the current–charge relationships
                                                          dq1              dq2
                                                              = i1             = i2
                                                          dt               dt
                    23. Write a function for the second-order Runge–Kutta method of integration. You
                        may use runKut4 as a model. Use the function to solve the problem in Example
                        7.4. Compare your results with those in Example 7.4.



          7.4       Stability and Stiffness

                    Loosely speaking, a method of numerical integration is said to be stable if the effects
                    of local errors do not accumulate catastrophically, that is, if the global error remains
                    bounded. If the method is unstable, the global error will increase exponentially, even-
                    tually causing numerical overflow. Stability has nothing to do with accuracy; in fact,
                    an inaccurate method can be very stable.
                         Stability is determined by three factors: the differential equations, the method of
                    solution, and the value of the increment h. Unfortunately, it is not easy to determine
                    stability beforehand, unless the differential equation is linear.


                    Stability of Euler’s Method
                    As a simple illustration of stability, consider the linear problem

                                                         y = −λy       y(0) = β                        (7.11)

                    where λ is a positive constant. The analytical solution of this problem is

                                                               y(x) = βe−λx

                         Let us now investigate what happens when we attempt to solve Eq. (7.11) numer-
                    ically with Euler’s formula

                                                    y(x + h) = y(x) + hy (x)                           (7.12)

                    Substituting y (x) = −λy(x), we get

                                                     y(x + h) = (1 − λh)y(x)

                    If 1 − λh > 1, the method is clearly unstable because |y| increases in every integra-
                    tion step. Thus, Euler’s method is stable only if 1 − λh ≤ 1, or

                                                                 h ≤ 2/λ                               (7.13)
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                       December 16, 2009       15:4




               267     7.4 Stability and Stiffness

                            The results can be extended to a system of n differential equations of the form

                                                                    y =− y                                         (7.14)

                      where is a constant matrix with the positive eigenvalues λi , i = 1, 2, . . . , n. It can
                      be shown that Euler’s method of integration is stable if

                                                                    h < 2/λmax                                     (7.15)

                      where λmax is the largest eigenvalue of          .


                      Stiffness
                      An initial value problem is called stiff if some terms in the solution vector y(x) vary
                      much more rapidly with x than others. Stiffness can be easily predicted for the differ-
                      ential equations y = − y with constant coefficient matrix . The solution of these
                      equations is y(x) = i Ci vi exp(−λi x), where λi are the eigenvalues of and vi are
                      the corresponding eigenvectors. It is evident that the problem is stiff if there is a large
                      disparity in the magnitudes of the positive eigenvalues.
                          Numerical integration of stiff equations requires special care. The step size h
                      needed for stability is determined by the largest eigenvalue λmax , even if the terms
                      exp(−λmax x) in the solution decay very rapidly and become insignificant as we move
                      away from the origin.
                          For example, consider the differential equation1

                                                          y + 1001y + 1000y = 0                                    (7.16)

                      Using y0 = y and y1 = y , the equivalent first-order equations are

                                                                        y1
                                                          y =
                                                                 −1000y0 − 1001y1

                      In this case,

                                                                       0     −1
                                                                =
                                                                     1000   1001

                      The eigenvalues of        are the roots of

                                                                     −λ       −1
                                                      |   − λI| =                    =0
                                                                    1000    1001 − λ

                      Expanding the determinant we get

                                                          −λ(1001 − λ) + 1000 = 0

                      which has the solutions λ1 = 1 and λ2 = 1000. These equations are clearly stiff. Ac-
                      cording to Eq. (7.15), we would need h ≤ 2/λ2 = 0.002 for Euler’s method to be

                      1   This example is taken from C. E. Pearson, Numerical Methods in Engineering and Science (van Nos-
                          trand and Reinhold, 1986).
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                    December 16, 2009     15:4




           268      Initial Value Problems

                    stable. The Runge–Kutta method would have approximately the same limitation on
                    the step size.
                          When the problem is very stiff, the usual methods of solution, such as the Runge–
                    Kutta formulas, become impractical because of the very small h required for stabil-
                    ity. These problems are best solved with methods that are specially designed for stiff
                    equations. Stiff problem solvers, which are outside the scope of this text, have much
                    better stability characteristics; some of them are even unconditionally stable. How-
                    ever, the higher degree of stability comes at a cost – the general rule is that stability
                    can be improved only by reducing the order of the method (and thus increasing the
                    truncation error).

                    EXAMPLE 7.7
                    (1) Show that the problem

                                                19
                                         y =−      y − 10y       y(0) = −9        y (0) = 0
                                                 4
                    is moderately stiff and estimate hmax , the largest value of h for which the Runge–
                    Kutta method would be stable. (2) Confirm the estimate by computing y(10) with
                    h ≈ hmax /2 and h ≈ 2hmax .

                    Solution of Part (1) With the notation y = y0 and y = y1 , the equivalent first-order
                    differential equations are
                                                 ⎡               ⎤
                                                         y1               y0
                                             y = ⎣ 19            ⎦=−
                                                   − y0 − 10y1            y1
                                                     4
                    where
                                                             ⎡           ⎤
                                                                0   −1
                                                           = ⎣ 19        ⎦
                                                                    10
                                                                4
                    The eigenvalues of    are given by

                                                              −λ     −1
                                                  |   − λI| = 19             =0
                                                                    10 − λ
                                                               4
                    which yields λ1 = 1/2 and λ2 = 19/2. Because λ2 is quite a bit larger than λ1 , the equa-
                    tions are moderately stiff.

                    Solution of Part (2) An estimate for the upper limit of the stable range of h can be
                    obtained from Eq. (7.15):

                                                           2      2
                                                 hmax =        =      = 0.2153
                                                          λmax   19/2

                    Although this formula is strictly valid for Euler’s method, it is usually not too far off
                    for higher-order integration formulas.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                  December 16, 2009     15:4




               269     7.5 Adaptive Runge–Kutta Method

                          Here are the results from the Runge–Kutta method with h = 0.1 (by specifying
                      freq = 0 in printSoln, only the initial and final values were printed):



                             x                y[ 0 ]                 y[ 1 ]
                         0.0000e+000         -9.0000e+000            0.0000e+000
                         1.0000e+001         -6.4011e-002            3.2005e-002



                          The analytical solution is

                                                                     19 −x/2 1 −19x/2
                                                          y(x) = −      e   + e
                                                                      2      2

                      yielding y(10) = −0.0640 11, which agrees with the value obtained numerically.
                           With h = 0.5 we encountered instability, as expected:


                                  x              y[ 0 ]                y[ 1 ]
                         0.0000e+000         -9.0000e+000            0.0000e+000
                         1.0000e+001          2.7030e+020        -2.5678e+021




              7.5     Adaptive Runge–Kutta Method

                      Determination of a suitable step size h can be a major headache in numerical inte-
                      gration. If h is too large, the truncation error may be unacceptable; if h is too small,
                      we are squandering computational resources. Moreover, a constant step size may
                      not be appropriate for the entire range of integration. For example, if the solution
                      curve starts off with rapid changes before becoming smooth (as in a stiff problem),
                      we should use a small h at the beginning and increase it as we reach the smooth re-
                      gion. This is where adaptive methods come in. They estimate the truncation error at
                      each integration step and automatically adjust the step size to keep the error within
                      prescribed limits.
                           The adaptive Runge–Kutta methods use embedded integration formulas. These
                      formulas come in pairs: One formula has the integration order m, the other one is of
                      order m + 1. The idea is to use both formulas to advance the solution from x to x + h.
                      Denoting the results by ym (x + h) and ym+1 (x + h), an estimate of the truncation error
                      in the formula of order m is obtained from


                                                      E(h) = ym+1 (x + h) − ym (x + h)                       (7.17)


                      What makes the embedded formulas attractive is that they share the points where
                      F(x, y) is evaluated. This means that once ym (x + h) has been computed, relatively
                      little additional effort is required to calculate ym+1 (x + h).
P1: PHB

CUUS884-Kiusalaas       CUUS884-07       978 0 521 19132 6                                                      December 16, 2009       15:4




           270      Initial Value Problems


                              i     Ai                          Bij                                        Ci          Di

                                                                                                           37         2825
                              0     −        −         −         −                 −               −
                                                                                                           378       27 648

                                    1          1
                              1                        −         −                 −               −        0           0
                                    5          5

                                    3        3         9                                                   250       18 575
                              2                                  −                 −               −
                                   10       40        40                                                   621       48 384

                                    3        3            9        6                                       125       13 525
                              3                      −                             −               −
                                    5       10           10        5                                       594       55 296

                                               11        5         70             35                                  277
                              4     1      −                   −                                   −        0
                                               54        2         27             27                                 14 336

                                    7      1631       175      575              44275            253       512          1
                              5
                                    8     55296       512     13824             110592           4096     1771          4


                             Table 7.1 Cash-Karp coefficients for Runge–Kutta-Fehlberg formulas


                        Here are the Runge–Kutta embedded formulas of orders 5 and 4 that were origi-
                    nally derived by Fehlberg; hence, they are known as Runge–Kutta-Fehlberg formulas:

                                          K0 = hF(x, y)
                                                 ⎛                                     ⎞
                                                                        i−1
                                          Ki = hF ⎝x + A i h, y +               Bij K j ⎠ , i = 1, 2, . . . , 5                (7.18)
                                                                         j =0


                                                                 5
                                         y5 (x + h) = y(x) +           Ci Ki     (fifth-order formula)                        (7.19a)
                                                               i=0

                                                                 5
                                         y4 (x + h) = y(x) +           Di Ki      (fourth-order formula)                      (7.19b)
                                                               i=0

                    The coefficients appearing in these formulas are not unique. Table 6.1 gives the coef-
                    ficients proposed by Cash and Karp,2 which are claimed to be an improvement over
                    Fehlberg’s original values.
                         The solution is advanced with the fifth-order formula in Eq. (7.19a). The fourth-
                    order formula is used only implicitly in estimating the truncation error
                                                                                            5
                                             E(h) = y5 (x + h) − y4 (x + h) =                    (Ci − Di )Ki                  (7.20)
                                                                                           i=0

                    Because Eq. (7.20) actually applies to the fourth-order formula, it tends to over-
                    estimate the error in the fifth-order formula.
                    2   J. R. Cash and A. H. Carp, ACM Transactions on Mathematical Software, Vol. 16 (1990), p. 201.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                       December 16, 2009     15:4




               271     7.5 Adaptive Runge–Kutta Method

                          Note that E(h) is a vector, its components Ei (h) representing the errors in the
                      dependent variables yi . This brings up the question: what is the error measure e(h)
                      that we wish to control? There is no single choice that works well in all problems. If
                      we want to control the largest component of E(h), the error measure would be

                                                         e(h) = max Ei (h)                                     (7.21)
                                                                   i

                      We could also control some gross measure of the error, such as the root-mean-square
                      error defined by

                                                                        n−1
                                                                   1
                                                       E¯ (h) =                 Ei2 (h)                        (7.22)
                                                                   n
                                                                        i=0

                      where n is the number of first-order equations. Then we would use

                                                            e(h) = E¯ (h)                                      (7.23)

                      for the error measure. Because the root-mean-square error is easier to handle, we
                      adopt it for our program.
                           Error control is achieved by adjusting the increment h so that the per-step error
                      e(h) is approximately equal to a prescribed tolerance ε. Noting that the truncation
                      error in the fourth-order formula is O(h5 ), we conclude that
                                                                                 5
                                                           e(h1 )        h1
                                                                  ≈                                               (a)
                                                           e(h2 )        h2
                      Let us suppose that we performed an integration step with h1 that resulted in the
                      error e(h1 ). The step size h2 that we should have used can now be obtained from Eq.
                      (a) by setting e(h2 ) = ε:
                                                                                 1/5
                                                                       e(h1 )
                                                         h2 = h1                                                 (b)
                                                                         ε
                      If h2 ≥ h1 , we could repeat the integration step with h2 , but since the error was below
                      the tolerance, that would be a waste of a perfectly good result. So we accept the cur-
                      rent step and try h2 in the next step. On the other hand, if h2 < h1 , we must scrap the
                      current step and repeat it with h2 . As Eq. (b) is only an approximation, it is prudent to
                      incorporate a small margin of safety. In our program, we use the formula
                                                                                     1/5
                                                                        e(h1 )
                                                       h2 = 0.9h1                                              (7.24)
                                                                          ε
                          Recall that e(h) applies to a single integration step, that is, it is a measure of the
                      local truncation error. The all-important global truncation error is due to the accu-
                      mulation of the local errors. What should ε be set at in order to achieve a global error
                      tolerance ε global ? Because e(h) is a conservative estimate of the actual error, setting
                      ε = ε global will usually be adequate. If the number integration steps is large, it is ad-
                      visable to decrease ε accordingly.
                          Is there any reason to use the nonadaptive methods at all? Usually the answer
                      is no; however, there are special cases where adaptive methods break down. For
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                December 16, 2009      15:4




           272      Initial Value Problems

                    example, adaptive methods generally do not work if F(x, y) contains discontinuities.
                    Because the error behaves erratically at the point of discontinuity, the program can
                    get stuck in an infinite loop trying to find the appropriate value of h. We would also
                    use a nonadaptive method if the output is to have evenly spaced values of x.



                      run kut5

                    This module is compatible with run kut4 listed in the previous article. Any program
                    that calls integrate can choose between the adaptive and the nonadaptive meth-
                    ods by importing either run kut5 or run kut4. The input argument h is the trial
                    value of the increment for the first integration step.

                    ## module run_kut5
                    ’’’ X,Y = integrate(F,x,y,xStop,h,tol=1.0e-6).
                          Adaptive Runge--Kutta method for solving the
                          initial value problem {y}’ = {F(x,{y})}, where
                          {y} = {y[0],y[1],...y[n-1]}.
                          x,y     = initial conditions
                          xStop = terminal value of x
                          h       = initial increment of x used in integration
                          tol     = per-step error tolerance
                          F       = user-supplied function that returns the
                                    array F(x,y) = {y’[0],y’[1],...,y’[n-1]}.
                    ’’’
                    from numpy import array,sum,zeros
                    from math import sqrt


                    def integrate(F,x,y,xStop,h,tol=1.0e-6):


                          def run_kut5(F,x,y,h):
                              # Runge--Kutta-Fehlberg formulas
                                C = array([37./378, 0., 250./621, 125./594,                        \
                                             0., 512./1771])
                                D = array([2825./27648, 0., 18575./48384,                          \
                                             13525./55296, 277./14336, 1./4])
                                n = len(y)
                                K = zeros((6,n))
                                K[0] = h*F(x,y)
                                K[1] = h*F(x + 1./5*h, y + 1./5*K[0])
                                K[2] = h*F(x + 3./10*h, y + 3./40*K[0] + 9./40*K[1])
                                K[3] = h*F(x + 3./5*h, y + 3./10*K[0]- 9./10*K[1]                  \
                                       + 6./5*K[2])
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                          December 16, 2009       15:4




               273     7.5 Adaptive Runge–Kutta Method

                                  K[4] = h*F(x + h, y - 11./54*K[0] + 5./2*K[1]                   \
                                          - 70./27*K[2] + 35./27*K[3])
                                  K[5] = h*F(x + 7./8*h, y + 1631./55296*K[0]                     \
                                          + 175./512*K[1] + 575./13824*K[2]                       \
                                          + 44275./110592*K[3] + 253./4096*K[4])
                             # Initialize arrays {dy} and {E}
                                  E = zeros(n)
                                  dy = zeros(n)
                             # Compute solution increment {dy} and per-step error {E}
                                  for i in range(6):
                                       dy = dy + C[i]*K[i]
                                       E = E + (C[i] - D[i])*K[i]
                             # Compute RMS error e
                                  e = sqrt(sum(E**2)/n)
                                  return dy,e


                           X = []
                           Y = []
                           X.append(x)
                           Y.append(y)
                           stopper = 0        # Integration stopper(0 = off, 1 = on)


                           for i in range(10000):
                                  dy,e = run_kut5(F,x,y,h)
                             # Accept integration step if error e is within tolerance
                                  if   e <= tol:
                                       y = y + dy
                                       x = x + h
                                       X.append(x)
                                       Y.append(y)
                                    # Stop if end of integration range is reached
                                       if stopper == 1: break
                             # Compute next step size from Eq. (7.24)
                                  if e != 0.0:
                                       hNext = 0.9*h*(tol/e)**0.2
                                  else: hNext = h
                             # Check if next step is the last one; is so, adjust h
                                  if (h > 0.0) == ((x + hNext) >= xStop):
                                       hNext = xStop - x
                                       stopper = 1
                                  h = hNext
                           return array(X),array(Y)
P1: PHB

CUUS884-Kiusalaas    CUUS884-07          978 0 521 19132 6                                      December 16, 2009   15:4




           274      Initial Value Problems

                    EXAMPLE 7.8
                    The aerodynamic drag force acting on a certain object in free fall can be approxi-
                    mated by

                                                               F D = av 2 e−by

                    where

                                                 v = velocity of the object in m/s

                                                 y = elevation of the object in meters

                                                 a = 7.45 kg/m

                                                 b = 10.53 × 10−5 m−1

                    The exponential term accounts for the change of air density with elevation. The dif-
                    ferential equation describing the fall is

                                                             my¨ = −mg + F D

                    where g = 9.80665 m/s2 and m = 114 kg is the mass of the object. If the object is
                    released at an elevation of 9 km, use the adaptive Runge–Kutta method to determine
                    its elevation and speed after a 10-s fall.

                    Solution The differential equation and the initial conditions are
                                                      a 2
                                            y¨ = −g +   y˙ exp(−by)
                                                      m
                                                            7.45 2
                                               = −9.80665 +      y˙ exp(−10.53 × 10−5 y)
                                                             114

                                                       y(0) = 9000 m        ˙
                                                                            y(0) =0

                    Letting y 0 = y and y1 = y,
                                             ˙ the equivalent first-order equations become

                                   y˙0                                      y1
                            y˙ =          =                                 −3
                                   y˙1         −9.80665 + 65.351 × 10            y12 exp(−10.53 × 10−5 y0 )


                                                                      9000 m
                                                             y(0) =
                                                                         0

                    The driver program for run kut5 is listed next. We specified a per-step error toler-
                    ance of 10−2 in integrate. Considering the magnitude of y, this should be enough
                    for five-decimal place accuracy in the solution.

                    #!/usr/bin/python
                    ## example7_8
                    from numpy import array,zeros
                    from run_kut5 import *
                    from printSoln import *
                    from math import exp
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                December 16, 2009    15:4




               275     7.5 Adaptive Runge–Kutta Method

                      def F(x,y):
                           F = zeros(2)
                           F[0] = y[1]
                           F[1] = -9.80665 + 65.351e-3 * y[1]**2 * exp(-10.53e-5*y[0])
                           return F


                      x = 0.0
                      xStop = 10.0
                      y = array([9000, 0.0])
                      h = 0.5
                      freq = 1
                      X,Y = integrate(F,x,y,xStop,h,1.0e-2)
                      printSoln(X,Y,freq)
                      raw_input("\nPress return to exit")


                          Running the program resulted in the following output:

                                x             y[ 0 ]               y[ 1 ]
                         0.0000e+000        9.0000e+003           0.0000e+000
                         5.0000e-001        8.9988e+003          -4.8043e+000
                         2.0584e+000        8.9821e+003          -1.5186e+001
                         3.4602e+000        8.9581e+003          -1.8439e+001
                         4.8756e+000        8.9312e+003          -1.9322e+001
                         6.5347e+000        8.8989e+003          -1.9533e+001
                         8.6276e+000        8.8580e+003          -1.9541e+001
                         1.0000e+001        8.8312e+003          -1.9519e+001


                          The first step was carried out with the prescribed trial value h = 0.5 s. Apparently
                      the error was well within the tolerance, so the step was accepted. Subsequent step
                      sizes, determined from Eq. (7.24), were considerably larger.
                          Inspecting the output, we see that at t = 10 s the object is moving with the speed
                      v = − y˙ = 19.52 m/s at an elevation of y = 8831 m.

                      EXAMPLE 7.9
                      Integrate the moderately stiff problem

                                                    19
                                           y =−        y − 10y       y(0) = −9   y (0) = 0
                                                     4

                      from x = 0 to 10 with the adaptive Runge–Kutta method and plot the results (this
                      problem also appeared in Example 7.7).

                      Solution Because we use an adaptive method, there is no need to worry about the
                      stable range of h, as we did in Example 7.7. As long as we specify a reasonable
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                 December 16, 2009       15:4




           276      Initial Value Problems

                    tolerance for the per-step error (in this case, the default value 10−6 is fine), the al-
                    gorithm will find the appropriate step size. Here is the program and its output:


                    #!/usr/bin/python
                    ## example7_9
                    from numpy import array,zeros
                    from run_kut5 import *
                    from printSoln import *


                    def F(x,y):
                         F = zeros(2)
                         F[0] = y[1]
                         F[1] = -4.75*y[0] - 10.0*y[1]
                         return F


                    x = 0.0
                    xStop = 10.0
                    y = array([-9.0, 0.0])
                    h = 0.1
                    freq = 4
                    X,Y = integrate(F,x,y,xStop,h)
                    printSoln(X,Y,freq)
                    raw_input("\nPress return to exit")


                              x              y[ 0 ]           y[ 1 ]
                      0.0000e+000      -9.0000e+000        0.0000e+000
                      9.8941e-002      -8.8461e+000        2.6651e+000
                      2.1932e-001      -8.4511e+000        3.6653e+000
                      3.7058e-001      -7.8784e+000        3.8061e+000
                      5.7229e-001      -7.1338e+000        3.5473e+000
                      8.6922e-001      -6.1513e+000        3.0745e+000
                      1.4009e+000      -4.7153e+000        2.3577e+000
                      2.8558e+000      -2.2783e+000        1.1391e+000
                      4.3990e+000      -1.0531e+000        5.2656e-001
                      5.9545e+000      -4.8385e-001        2.4193e-001
                      7.5596e+000      -2.1685e-001        1.0843e-001
                      9.1159e+000      -9.9591e-002        4.9794e-002
                      1.0000e+001      -6.4010e-002        3.2005e-002



                        The results are in agreement with the analytical solution.
                        The plots of y and y show every fourth integration step. Note the high density of
                    points near x = 0 where y changes rapidly. As the y -curve becomes smoother, the
                    distance between the points increases.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                            December 16, 2009     15:4




               277     7.6 Bulirsch–Stoer Method

                                    4.0

                                    2.0
                                                                     y'
                                    0.0

                                   -2.0
                                                         y
                                   -4.0

                                   -6.0

                                   -8.0

                                  -10.0
                                       0.0         2.0        4.0             6.0         8.0            10.0
                                                                          x

              7.6     Bulirsch–Stoer Method
                      Midpoint Method
                      The midpoint formula of numerical integration of y = F(x, y) is


                                                 y(x + h) = y(x − h) + 2hF x, y(x)                                  (7.25)

                      It is a second-order formula, like the modified Euler’s formula. We discuss it here
                      because it is the basis of the powerful Bulirsch–Stoer method, which is the technique
                      of choice in problems where high accuracy is required.
                           Figure 7.3 illustrates the midpoint formula for a single differential equation y =
                      f (x, y). The change in y over the two panels shown is
                                                                               x+h
                                                y(x + h) − y(x − h) =                y (x)dx
                                                                              x−h

                      which equals the area under the y (x) curve. The midpoint method approximates this
                      area by the area 2hf (x, y) of the cross-hatched rectangle.
                          Consider now advancing the solution of y (x) = F(x, y) from x = x0 to x0 + H
                      with the midpoint formula. We divide the interval of integration into n steps of length


                                       y'(x)



                                                                          f(x,y)
                                                                h                    h
                                                                                                     x
                                                       x-h                x               x+h
                                       Figure 7.3. Graphical repesentation of midpoint formula.
P1: PHB

CUUS884-Kiusalaas     CUUS884-07     978 0 521 19132 6                                       December 16, 2009      15:4




           278      Initial Value Problems


                                                               H
                                                   h
                                                                                               x
                                    x0       x1        x2           x3           xn - 1 xn
                                    Figure 7.4. Mesh used in the midpoint method.



                    h = H/n each, as shown in Fig. 7.4, and carry out the computations

                                                         y1 = y0 + hF0

                                                         y2 = y0 + 2hF1

                                                         y3 = y1 + 2hF2                                    (7.26)
                                                               ..
                                                                .

                                                         yn = yn−2 + 2hFn−1

                    Here we used the notation yi = y(xi ) and Fi = F(xi , yi ). The first of Eqs. (7.26) uses
                    the Euler formula to “seed” the midpoint method; the other equations are midpoint
                    formulas. The final result is obtained by averaging yn in Eq. (7.26) and the estimate
                    yn ≈ yn−1 + hFn available from the Euler formula:

                                                               1
                                               y(x0 + H) =       yn + yn−1 + hFn                           (7.27)
                                                               2


                    Richardson Extrapolation
                    It can be shown that the error in Eq. (7.27) is

                                                    E = c1 h2 + c2 h4 + c3 h6 + · · ·

                    Herein lies the great utility of the midpoint method: We can eliminate as many of the
                    leading error terms as we wish by Richardson extrapolation. For example, we could
                    compute y(x0 + H) with a certain value of h and then repeat process with h/2. De-
                    noting the corresponding results by g(h) and g(h/2), Richardson extrapolation – see
                    Eq. (5.9) – then yields the improved result

                                                                         4g(h/2) − g(h)
                                                  ybetter (x0 + H) =
                                                                               3

                    which is fourth-order accurate. Another round of integration with h/4 followed by
                    Richardson extrapolation get us sixth-order accuracy, and so forth. Rather than halv-
                    ing the interval in successive integrations, we use the sequence h/2, h/4, h/6, h/8,
                    h/10, . . ., which has been found to be more economical.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                December 16, 2009    15:4




               279     7.6 Bulirsch–Stoer Method

                          The ys in Eqs. (7.26) should be viewed as temporary variables, because unlike
                      y(x0 + H), they cannot be refined by Richardson extrapolation.

                         midpoint

                      The function integrate in this module combines the midpoint method with
                      Richardson extrapolation. The first application of the midpoint method uses two in-
                      tegration steps. The number of steps is increased by 2 in successive integrations, each
                      integration being followed by Richardson extrapolation. The procedure is stopped
                      when two successive solutions differ (in the root-mean-square sense) by less than a
                      prescribed tolerance.

                      ## module midpoint
                      ’’’ yStop = integrate (F,x,y,xStop,tol=1.0e-6)
                            Modified midpoint method for solving the
                            initial value problem y’ = F(x,y}.
                            x,y    = initial conditions
                            xStop = terminal value of x
                            yStop = y(xStop)
                            F       = user-supplied function that returns the
                                      array F(x,y) = {y’[0],y’[1],...,y’[n-1]}.
                      ’’’
                      from numpy import zeros,float,sum
                      from math import sqrt


                      def integrate(F,x,y,xStop,tol):


                            def midpoint(F,x,y,xStop,nSteps):
                         # Midpoint formulas
                                  h = (xStop - x)/nSteps
                                  y0 = y
                                  y1 = y0 + h*F(x,y0)
                                  for i in range(nSteps-1):
                                      x = x + h
                                      y2 = y0 + 2.0*h*F(x,y1)
                                      y0 = y1
                                      y1 = y2
                                  return 0.5*(y1 + y0 + h*F(x,y2))


                            def richardson(r,k):
                         # Richardson’s extrapolation
                                  for j in range(k-1,0,-1):
                                      const = (k/(k - 1.0))**(2.0*(k-j))
                                      r[j] = (const*r[j+1] - r[j])/(const - 1.0)
                                  return
P1: PHB

CUUS884-Kiusalaas       CUUS884-07        978 0 521 19132 6                                          December 16, 2009   15:4




           280      Initial Value Problems

                           kMax = 51
                           n = len(y)
                           r = zeros((kMax,n),dtype=float)
                        # Start with two integration steps
                           nSteps = 2
                           r[1] = midpoint(F,x,y,xStop,nSteps)
                           r_old = r[1].copy()
                        # Increase the number of integration points by 2
                        # and refine result by Richardson extrapolation
                           for k in range(2,kMax):
                                  nSteps = 2*k
                                  r[k] = midpoint(F,x,y,xStop,nSteps)
                                  richardson(r,k)
                               # Compute RMS change in solution
                                  e = sqrt(sum((r[1] - r_old)**2)/n)
                               # Check for convergence
                                  if e < tol: return r[1]
                                  r_old = r[1].copy()
                           print "Midpoint method did not converge"




                    Bulirsch–Stoer Algorithm
                    When used on its own, the module midpoint has a major shortcoming: the solution
                    at points between the initial and final values of x cannot be refined by Richardson ex-
                    trapolation, so that y is usable only at the last point. This deficiency is rectified in the
                    Bulirsch–Stoer method. The fundamental idea behind the method is simple: Apply
                    the midpoint method in a piecewise fashion. That is, advance the solution in stages
                    of length H, using the midpoint method with Richardson extrapolation to perform
                    the integration in each stage. The value of H can be quite large, because the preci-
                    sion of the result is determined by the step length h in the midpoint method, not by
                    H. However, if H is too large, the midpoint method may not converge. If this happens,
                    try a smaller value of H or a larger error tolerance.
                         The original Bulirsch and Stoer technique3 is a complex procedure that incorpo-
                    rates many refinements missing in our algorithm. However, the function bulStoer
                    given next retains the essential ideas of Bulirsch and Stoer.
                         What are the relative merits of adaptive Runge–Kutta and Bulirsch–Stoer meth-
                    ods? The Runge–Kutta method is more robust, having a higher tolerance for non-
                    smooth functions and stiff problems. The Bulirsch–Stoer algorithm (in its original
                    form) is used mainly in problems where high accuracy is of paramount importance.



                    3   J. Stoer and R. Bulirsch, Introduction to Numerical Analysis (Springer, 1980).
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                              December 16, 2009   15:4




               281     7.6 Bulirsch–Stoer Method

                      Our simplified version is no more accurate than the adaptive Runge–Kutta method,
                      but it is useful if the output is to appear at equally spaced values of x.



                         bulStoer

                      This function contains a simplified algorithm for the Bulirsch–Stoer method.

                      ## module bulStoer
                      ’’’ X,Y = bulStoer(F,x,y,xStop,H,tol=1.0e-6).
                            Simplified Bulirsch-Stoer method for solving the
                            initial value problem {y}’ = {F(x,{y})}, where
                            {y} = {y[0],y[1],...y[n-1]}.
                            x,y    = initial conditions
                            xStop = terminal value of x
                            H       = increment of x at which results are stored
                            F       = user-supplied function that returns the
                                      array F(x,y) = {y’[0],y’[1],...,y’[n-1]}.
                      ’’’
                      from numpy import array
                      from midpoint import *


                      def bulStoer(F,x,y,xStop,H,tol=1.0e-6):
                            X = []
                            Y = []
                            X.append(x)
                            Y.append(y)
                            while x < xStop:
                                  H = min(H,xStop - x)
                                  y = integrate(F,x,y,x + H,tol) # Midpoint method
                                  x = x + H
                                  X.append(x)
                                  Y.append(y)
                            return array(X),array(Y)


                      EXAMPLE 7.10
                      Compute the solution of the initial value problem

                                                       y = sin y   y(0) = 1

                      at x = 0.5 with the midpoint formulas using n = 2 and n = 4, followed by Richardson
                      extrapolation (this problem was solved with the second-order Runge–Kutta method
                      in Example 7.3).
P1: PHB

CUUS884-Kiusalaas    CUUS884-07        978 0 521 19132 6                                 December 16, 2009   15:4




           282      Initial Value Problems

                    Solution With n = 2, the step length is h = 0.25. The midpoint formulas, Eqs. (7.26)
                    and (7.27), yield


                                        y1 = y0 + hf0 = 1 + 0.25 sin 1.0 = 1.210 368

                                        y2 = y0 + 2hf1 = 1 + 2(0.25) sin 1.210 368 = 1.467 87 3
                                             1
                                  yh (0.5) =   (y1 + y0 + hf2 )
                                             2
                                             1
                                           = (1.210 368 + 1.467 87 3 + 0.25 sin 1.467 87 3)
                                             2
                                           = 1.463 459


                    Using n = 4, we have h = 0.125, and the midpoint formulas become


                                   y1 = y0 + hf0 = 1 + 0.125 sin 1.0 = 1.105 184

                                   y2 = y0 + 2hf1 = 1 + 2(0.125) sin 1.105 184 = 1.223 387

                                   y3 = y1 + 2hf2 = 1.105 184 + 2(0.125) sin 1.223 387 = 1.340 248

                                   y4 = y2 + 2hf3 = 1.223 387 + 2(0.125) sin 1.340 248 = 1.466 772
                                         1
                           yh/2 (0.5) =    (y4 + y3 + hf4 )
                                         2
                                         1
                                       = (1.466 772 + 1.340 248 + 0.125 sin 1.466 772)
                                         2
                                       = 1.465 672


                    Richardson extrapolation results in

                                       4yh/2 (0.5) − yh (0.5)   4(1.465 672) − 1.463 459
                            y(0.5) =                          =                          = 1.466 410
                                                 3                         3

                    which compares favorably with the “true” solution y(0.5) = 1.466 404.


                    EXAMPLE 7.11


                                                                L

                                                            i
                                                     E(t)                   R
                                                                      i


                                                                C
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                        December 16, 2009   15:4




               283     7.6 Bulirsch–Stoer Method

                          The differential equations governing the loop current i and the charge q on the
                      capacitor of the electric circuit shown are

                                                       di        q                 dq
                                                   L      + Ri +   = E (t )           =i
                                                       dt        C                 dt

                      If the applied voltage E is suddenly increased from zero to 9 V, plot the resulting loop
                      current during the first 10 s. Use R = 1.0 , L = 2 H, and C = 0.45 F.

                      Solution Letting

                                                                    y0         q
                                                              y=           =
                                                                    y1         i

                      and substituting the given data, the differential equations become

                                                        y˙0                 y1
                                               y˙ =           =
                                                        y˙1        (−Ry1 − y0 /C + E ) /L

                      The initial conditions are

                                                                           0
                                                                  y(0) =
                                                                           0

                           We solved the problem with the function bulstoer with the increment H =
                      0.5 s:

                      #!/usr/bin/python
                      ## example7_11
                      from bulStoer import *
                      from numpy import array,zeros
                      from printSoln import *


                      def F(x,y):
                           F = zeros(2)
                           F[0] = y[1]
                           F[1] =(-y[1] - y[0]/0.45 + 9.0)/2.0
                           return F


                      H = 0.5
                      xStop = 10.0
                      x = 0.0
                      y = array([0.0, 0.0])
                      X,Y = bulStoer(F,x,y,xStop,H)
                      printSoln(X,Y,1)
                      raw_input("\nPress return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-07            978 0 521 19132 6                                        December 16, 2009   15:4




           284      Initial Value Problems

                        Skipping the numerical output, the plot of the current is

                                      4

                                      3

                                      2
                              i (A)

                                      1

                                      0

                                      -1

                                      -2
                                        0.0           2.0          4.0           6.0          8.0        10.0
                                                                         t (s)

                        Recall that in each interval H (the spacing of open circles) the integration was
                    performed by the modified midpoint method and refined by Richardson extrapola-
                    tion.

                    PROBLEM SET 7.2
                     1. Derive the analytical solution of the problem

                                               y + y − 380y = 0           y(0) = 1         y (0) = −20

                        Would you expect difficulties in solving this problem numerically?
                     2. Consider the problem

                                                            y = x − 10y          y(0) = 10

                        (a) Verify that the analytical solution is y(x) = 0.1x − 0.001 + 10.01e−10x . (b) De-
                        termine the step size h that you would use in numerical solution with the (non-
                        adaptive) Runge–Kutta method.
                     3.   Integrate the initial value problem in Prob. 2 from x = 0 to 5 with the Runge–
                        Kutta method using (a) h = 0.1, (b) h = 0.25, and (c) h = 0.5. Comment on the
                        results.
                     4.   Integrate the initial value problem in Prob. 2 from x = 0 to 10 with the adaptive
                        Runge–Kutta method.
                     5.


                                                                                       y
                                                               k
                                                                         m
                                                               c
P1: PHB

CUUS884-Kiusalaas    CUUS884-07     978 0 521 19132 6                                        December 16, 2009   15:4




               285     7.6 Bulirsch–Stoer Method

                            The differential equation describing the motion of the mass–spring–dashpot sys-
                            tem is
                                                                     c     k
                                                              y¨ +     y˙ + y = 0
                                                                     m     m
                          where m = 2 kg, c = 460 N·s/m, and k = 450 N/m. The initial conditions are
                          y(0) = 0.01 m and y(0)
                                             ˙    = 0. (a) Show that this is a stiff problem and determine a
                          value of h that you would use in numerical integration with the nonadaptive
                          Runge–Kutta method. (c) Carry out the integration from t = 0 to 0.2 s with the
                          chosen h and plot y˙ versus t .
                       6.    Integrate the initial value problem specified in Prob. 5 with the adaptive
                          Runge–Kutta method from t = 0 to 0.2 s, and plot y˙ versus t .
                       7.   Compute the numerical solution of the differential equation

                                               y = 16.81y            y(0) = 1.0      y (0) = −4.1

                          from x = 0 to 2 with the adaptive Runge–Kutta method. Use the initial conditions
                          (a) y(0) = 1.0, y (0) = −4.1; and (b) y(0) = 1.0, y (0) = −4.11. Explain the large
                          difference in the two solutions. Hint: derive the analytical solutions.
                       8.   Integrate

                                                y + y − y2 = 0           y(0) = 1        y (0) = 0

                          from x = 0 to 3.5. Is the sudden increase in y near the upper limit real or an arti-
                          fact caused by instability?
                       9.   Solve the stiff problem – see Eq. (7.16)

                                           y + 1001y + 1000y = 0              y(0) = 1       y (0) = 0

                            from x = 0 to 0.2 with the adaptive Runge–Kutta method and plot y˙ versus x.
                      10.     Solve
                                                                                     √
                                              y + 2y + 3y = 0      y(0) = 0   y (0) = 2

                          with the adaptive Runge–Kutta method from x = 0 to 5 (the analytical solution is
                                     √
                          y = e−x sin 2x).
                      11.   Solve the differential equation

                                                                     y = 2yy

                          from x = 0 to 10 with the initial conditions y(0) = 1, y (0) = −1. Plot y versus x.
                      12.    Repeat Prob. 11 with the initial conditions y(0) = 0, y (0) = 1 and the integra-
                          tion range x = 0 to 1.5.
                      13.    Use the adaptive Runge–Kutta method to integrate
                                                                9
                                                        y =       −y x            y(0) = 5
                                                                y
                            from x = 0 to 4 and plot y versus x.
                      14.     Solve Prob. 13 with the Bulirsch–Stoer method using H = 0.5.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07         978 0 521 19132 6                                           December 16, 2009   15:4




           286      Initial Value Problems

                    15.     Integrate

                                             x 2 y + xy + y = 0            y(1) = 0        y (1) = −2

                          from x = 1 to 20, and plot y and y versus x. Use the Bulirsch–Stoer method.
                    16.

                                                                         x
                                                                                       m
                                                                         k

                          The magnetized iron block of mass m is attached to a spring of stiffness k and
                          free length L. The block is at rest at x = L when the electromagnet is turned on,
                          exerting a repulsive force F = c/x 2 on the block. The differential equation of the
                          resulting motion is
                                                                      c
                                                            mx¨ =        − k(x − L)
                                                                      x2
                          Determine the amplitude and the period of the motion by numerical integration
                          with the adaptive Runge–Kutta method. Use c = 5 N·m2 , k = 120 N/m, L = 0.2
                          m, and m = 1.0 kg.
                    17.




                                                                           φ
                                                                                   C

                                                                       B
                                                        A         θ
                          The bar A BC is attached to the vertical rod with a horizontal pin. The assembly
                          is free to rotate about the axis of the rod. Neglecting friction, the equations of
                          motion of the system are
                                                          2
                                                   θ¨ = φ˙ sin θ cos θ         φ¨ = −2θ˙ φ˙ cot θ

                        The system is set into motion with the initial conditions θ (0) = π/12 rad, θ˙ (0) =
                        0, φ(0) = 0 and φ(0)
                                          ˙    = 20 rad/s. Obtain a numerical solution with the adaptive
                        Runge–Kutta method from t = 0 to 1.5 s and plot φ˙ versus t .
                    18.    Solve the circuit problem in Example 7.11 if R = 0 and

                                                                  0 when t < 0
                                                       E (t ) =
                                                                  9 sin πt when t ≥ 0

                    19.     Solve Prob. 21 in Problem Set 7.1 if E = 240 V (constant).
P1: PHB

CUUS884-Kiusalaas    CUUS884-07      978 0 521 19132 6                                             December 16, 2009   15:4




               287     7.6 Bulirsch–Stoer Method

                      20.

                                                               R1                     L

                                                   i1                        i2
                                          E(t )                              R2                       C
                                                                       i1                     i2

                                                               L

                            Kirchoff’s equations for the circuit in the figure are
                                                             di1
                                                         L       + R1i1 + R2 (i1 − i2 ) = E (t )
                                                              dt
                                                               di2                   q2
                                                             L     + R2 (i2 − i1 ) +    =0
                                                               dt                    C
                            where
                                                                         dq2
                                                                             = i2
                                                                         dt
                            Using the data R1 = 4 , R2 = 10 , L = 0.032 H, C = 0.53 F, and

                                                                      20 V if 0 < t < 0.005 s
                                                         E (t ) =
                                                                      0 otherwise

                          plot the transient loop currents i1 and i2 from t = 0 to 0.05 s.
                      21.   Consider a closed biological system populated by M number of prey and N
                          number of predators. Volterra postulated that the two populations are related by
                          the differential equations
                                                                    ˙ = a M − b MN
                                                                    M
                                                                    ˙ = −c N + d MN
                                                                    N

                            where a, b, c, and d are constants. The steady-state solution is M0 = c/d, N0 =
                            a/b; if numbers other than these are introduced into the system, the populations
                            undergo periodic fluctuations. Introducing the notation

                                                              y0 = M/M0         y1 = N/N0

                            allows us to write the differential equations as

                                                                   y˙0 = a(y0 − y0 y1 )

                                                                   y˙1 = b(−y1 + y0 y1 )

                            Using a = 1.0/year, b = 0.2/year, y0 (0) = 0.1, and y1 (0) = 1.0, plot the two popu-
                            lations from t = 0 to 5 years.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07       978 0 521 19132 6                                       December 16, 2009   15:4




           288      Initial Value Problems

                    22.     The equations

                                                              u˙ = −au + av

                                                              v˙ = cu − v − uw

                                                            ˙ = −bw + uv
                                                            w

                          known as the Lorenz equations, are encountered in the theory of fluid dynam-
                          ics. Letting a = 5.0, b = 0.9, and c = 8.2, solve these equations from t = 0 to 10
                          with the initial conditions u(0) = 0, v(0) = 1.0, w(0) = 2.0 and plot u(t ). Repeat
                          the solution with c = 8.3. What conclusions can you draw from the results?
                    23.


                                2 m3/s                         4 m3/s                        3 m3/s
                                                    c1                         c2
                              c = 25 mg/m3
                                                                  2m3/s
                                           4 m3/s                                   4 m3/s
                                                                  3m3/s
                                                                                              1m3/s
                                                    c3                         c4
                                                                  1m3/s                  c = 50 mg/m3

                          Four mixing tanks are connected by pipes. The fluid in the system is pumped
                          through the pipes at the rates shown in the figure. The fluid entering the system
                          contains a chemical of concentration c as indicated. The rate at which the mass
                          of the chemical changes in tank i is
                                                          dci
                                                     Vi       =     (Qc)in −   (Qc)out
                                                          dt
                          where Vi is the volume of the tank and Q represents the flow rate in the pipes
                          connected to it. Applying this equation to each tank, we obtain
                                                        dci
                                                      V1      = −6c1 + 4c2 + 2(25)
                                                        dt
                                                        dc2
                                                     V2       = −7c2 + 3c3 + 4c4
                                                        dt
                                                        dc3
                                                     V3       = 4c1 − 4c3
                                                        dt
                                                        dc4
                                                     V4       = 2c1 + c3 − 4c4 + 50
                                                        dt
                          Plot the concentration of the chemical in tanks 1 and 2 versus time t from t = 0 to
                          100 s. Let V1 = V2 = V3 = V4 = 10 m3 and assume that the concentration in each
                          tank is zero at t = 0. The steady-state version of this problem was solved in Prob.
                          21, Problem Set 2.2.
P1: PHB

CUUS884-Kiusalaas    CUUS884-07    978 0 521 19132 6                                December 16, 2009   15:4




               289     7.7 Other Methods

              7.7     Other Methods

                      The methods described so far belong to a group known as single-step methods. The
                      name stems from the fact that the information at a single point on the solution curve
                      is sufficient to compute the next point. There are also multistep methods that utilize
                      several points on the curve to extrapolate the solution at the next step. Well-known
                      members of this group are the methods of Adams, Milne, Hamming, and Gere. These
                      methods were once popular but have lost some of their luster in the past few years.
                      Multistep methods have two shortcomings that complicate their implementation:

                        • The methods are not self-starting, but must be provided with the solution at the
                          first few points by a single-step method.
                        • The integration formulas assume equally spaced steps, which makes it difficult
                          to change the step size.

                          Both of these hurdles can be overcome, but the price is complexity of the algo-
                      rithm that increases with the sophistication of the method. The benefits of multistep
                      methods are minimal – the best of them can outperform their single-step counter-
                      parts in certain problems, but these occasions are rare.
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                    December 16, 2009    15:4




          8         Two-Point Boundary Value Problems




                                         Solve y = f (x, y, y ),   y(a) = α,   y(b) = β




          8.1       Introduction

                    In two-point boundary value problems, the auxiliary conditions associated with the
                    differential equation, called the boundary conditions, are specified at two different
                    values of x. This seemingly small departure from initial value problems has a ma-
                    jor repercussion – it makes boundary value problems considerably more difficult to
                    solve. In an initial value problem we were able to start at the point where the initial
                    values were given and march the solution forward as far as needed. This technique
                    does not work for boundary value problems, because there are not enough starting
                    conditions available at either endpoint to produce a unique solution.
                         One way to overcome the lack of starting conditions is to guess the missing val-
                    ues. The resulting solution is very unlikely to satisfy boundary conditions at the other
                    end, but by inspecting the discrepancy we can estimate what changes to make to the
                    initial conditions before integrating again. This iterative procedure is known as the
                    shooting method. The name is derived from analogy with target shooting – take a
                    shot and observe where it hits the target, then correct the aim and shoot again.
                         Another means of solving two-point boundary value problems is the finite differ-
                    ence method, where the differential equations are approximated by finite differences
                    at evenly spaced mesh points. As a consequence, a differential equation is trans-
                    formed into set of simultaneous algebraic equations.
                         The two methods have a common problem: They give rise to nonlinear sets of
                    equations if the differential equations are not linear. As we noted in Chapter 2, all
                    methods of solving nonlinear equations are iterative procedures that can consume
                    a lot of computational resources. Thus, solution of nonlinear boundary value prob-
                    lems is not cheap. Another complication is that iterative methods need reasonably
                    good starting values in order to converge. Because there is no set formula for deter-
                    mining these, an algorithm for solving nonlinear boundary value problems requires
                    informed input, it cannot be treated as a “black box.”

           290
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                    December 16, 2009    15:4




               291     8.2 Shooting Method

              8.2     Shooting Method
                      Second-Order Differential Equation
                      The simplest two-point boundary value problem is a second-order differential equa-
                      tion with one condition specified at x = a and another one at x = b. Here is an ex-
                      ample of such a problem:

                                               y = f (x, y, y ),   y(a) = α,    y(b) = β                    (8.1)

                          Let us now attempt to turn Eqs. (8.1) into the initial value problem

                                              y = f (x, y, y ),    y(a) = α,    y (a) = u                   (8.2)

                      The key to success is finding the correct value of u. This could be done by trial
                      and error: guess u and solve the initial value problem by marching from x = a to
                      y = b. If the solution agrees with the prescribed boundary condition y(b) = β, we are
                      done; otherwise, we have to adjust u and try again. Clearly, this procedure is very
                      tedious.
                          More systematic methods become available to us if we realize that the determi-
                      nation of u is a root-finding problem. Because the solution of the initial value prob-
                      lem depends on u, the computed value of y(b) is a function of u; that is,

                                                              y(b) = θ (u)

                      Hence, u is a root of

                                                         r (u) = θ(u) − β = 0                               (8.3)

                      where r (u) is the boundary residual (difference between the computed and specified
                      boundary value at x = b). Equation (8.3) can be solved by one of the root-finding
                      methods discussed in Chapter 4. We reject the method of bisection because it in-
                      volves too many evaluations of θ (u). In the Newton–Raphson method we run into the
                      problem of having to compute dθ /du, which can be done, but not easily. That leaves
                      Ridder’s algorithm as our method of choice.
                          Here is the procedure we use in solving nonlinear boundary value problems:

                      1. Specify the starting values u1 and u2 that must bracket the root u of Eq. (8.3).
                      2. Apply Ridder’s method to solve Eq. (8.3) for u. Note that each iteration requires
                         evaluation of θ(u) by solving the differential equation as an initial value problem.
                      3. Having determined the value of u, solve the differential equations once more and
                         record the results.

                            If the differential equation is linear, any root-finding method will need only one
                      interpolation to determine u. Because Ridder’s method uses three points (u1 , u2 , and
                      u3 ), it is wasteful compared with linear interpolation, which uses only two points (u1
                      and u2 ). Therefore, we replace Ridder’s method with linear interpolation whenever
                      the differential equation is linear.
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                     December 16, 2009       15:4




           292      Two-Point Boundary Value Problems

                      linInterp

                    Here is the algorithm we use for linear interpolation:

                    ## module linInterp
                    ’’’ root = linInterp(f,x1,x2).
                          Finds the zero of the linear function f(x) by straight
                          line interpolation based on x = x1 and x2.
                    ’’’
                    def linInterp(f,x1,x2):
                          f1 = f(x1)
                          f2 = f(x2)
                          return = x2 - f2*(x2 - x1)/(f2 - f1)

                    EXAMPLE 8.1
                    Solve the boundary value problem

                                            y + 3yy = 0           y(0) = 0      y(2) = 1

                    Solution The equivalent first-order equations are

                                                           y0           y1
                                                    y =           =
                                                           y1         −3y0 y1

                    with the boundary conditions

                                                     y0 (0) = 0       y0 (2) = 1

                         Now comes the daunting task of determining the trial values of y (0). We could
                    always pick two numbers at random and hope for the best. However, it is possible
                    to reduce the element of chance with a little detective work. We start by making the
                    reasonable assumption that y is smooth (does not wiggle) in the interval 0 ≤ x ≤ 2.
                    Next, we note that y has to increase from 0 to 1, which requires y > 0. Because both
                    y and y are positive, we conclude that y must be negative in order to satisfy the
                    differential equation. Now we are in a position to make a rough sketch of y:



                                                                                y


                                                                                                           1

                                                                                                               x
                                                                                   0                      2
                        Looking at the sketch it is clear that y (0) > 0.5, so that y (0) = 1 and 2 appear
                    to be reasonable values for the brackets of y (0); if they are not, Ridder’s method will
                    display an error message.
P1: PHB

CUUS884-Kiusalaas    CUUS884-08   978 0 521 19132 6                                December 16, 2009   15:4




               293     8.2 Shooting Method

                           In the program listed next we chose the fourth-order Runge–Kutta method for
                      integration. It can be replaced by the adaptive version by substituting run kut5
                      for run kut4 in the import statement. Note that three user-supplied functions are
                      needed to describe the problem at hand. Apart from the function F(x,y) that de-
                      fines the differential equations, we also need the functions initCond(u) to specify
                      the initial conditions for integration, and r(u) to provide Ridder’s method with the
                      boundary condition residual. By changing a few statements in these functions, the
                      program can be applied to any second-order boundary value problem. It also works
                      for third-order equations if integration is started at the end where two of the three
                      boundary conditions are specified.


                      #!/usr/bin/python
                      ## example8_1
                      from numpy import zeros,array
                      from run_kut4 import *
                      from ridder import *
                      from printSoln import *


                      def initCond(u):       # Init. values of [y,y’]; use ’u’ if unknown
                           return array([0.0, u])


                      def r(u):              # Boundary condition residual--see Eq. (8.3)
                           X,Y = integrate(F,xStart,initCond(u),xStop,h)
                           y = Y[len(Y) - 1]
                           r = y[0] - 1.0
                           return r


                      def F(x,y):            # First-order differential equations
                           F = zeros(2)
                           F[0] = y[1]
                           F[1] = -3.0*y[0]*y[1]
                           return F


                      xStart = 0.0              # Start of integration
                      xStop = 2.0               # End of integration
                      u1 = 1.0                  # 1st trial value of unknown init. cond.
                      u2 = 2.0                  # 2nd trial value of unknown init. cond.
                      h = 0.1                   # Step size
                      freq = 2                  # Printout frequency
                      u = ridder(r,u1,u2) # Compute the correct initial condition
                      X,Y = integrate(F,xStart,initCond(u),xStop,h)
                      printSoln(X,Y,freq)
                      raw_input("\nPress return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                       December 16, 2009   15:4




           294      Two-Point Boundary Value Problems

                        Here is the solution :

                              x              y[ 0 ]              y[ 1 ]
                      0.0000e+000          0.0000e+000        1.5145e+000
                      2.0000e-001          2.9404e-001        1.3848e+000
                      4.0000e-001          5.4170e-001        1.0743e+000
                      6.0000e-001          7.2187e-001        7.3287e-001
                      8.0000e-001          8.3944e-001        4.5752e-001
                      1.0000e+000          9.1082e-001        2.7013e-001
                      1.2000e+000          9.5227e-001        1.5429e-001
                      1.4000e+000          9.7572e-001        8.6471e-002
                      1.6000e+000          9.8880e-001        4.7948e-002
                      1.8000e+000          9.9602e-001        2.6430e-002
                      2.0000e+000          1.0000e+000        1.4522e-002

                        Note that y (0) = 1.5145, so our starting values of 1.0 and 2.0 were on the mark.

                    EXAMPLE 8.2
                    Numerical integration of the initial value problem

                                            y + 4y = 4x          y(0) = 0        y (0) = 0

                    yielded y (2) = 1.653 64. Use this information to determine the value of y (0) that
                    would result in y (2) = 0.

                    Solution We use linear interpolation
                                                                         u2 − u1
                                                  u = u2 − θ (u2 )
                                                                     θ (u2 ) − θ(u1 )
                    where in our case u = y (0) and θ(u) = y (2). So far we are given u1 = 0 and θ (u1 ) =
                    1.653 64. To obtain the second point, we need another solution of the initial value
                    problem. An obvious solution is y = x, which gives us y(0) = 0 and y (0) = y (2) = 1.
                    Thus, the second point is u2 = 1 and θ(u2 ) = 1. Linear interpolation now yields
                                                                    1−0
                                           y (0) = u = 1 − (1)                = 2.529 89
                                                                 1 − 1.653 64
                    EXAMPLE 8.3
                    Solve the third-order boundary value problem

                                      y = 2y + 6xy            y(0) = 2        y(5) = y (5) = 0

                    and plot y versus x.

                    Solution The first-order equations and the boundary conditions are
                                                    ⎡ ⎤ ⎡                 ⎤
                                                      y0           y1
                                                    ⎢ ⎥ ⎢                 ⎥
                                                y = ⎣ y1 ⎦ = ⎣     y2     ⎦
                                                      y2       2y2 + 6xy0

                                                 y0 (0) = 2      y0 (5) = y1 (5) = 0
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                 December 16, 2009    15:4




               295     8.2 Shooting Method

                           The program listed next is based on example8 1. Because two of the three
                      boundary conditions are specified at the right end, we start the integration at x = 5
                      and proceed with negative h toward x = 0. Two of the three initial conditions are pre-
                      scribed: y0 (5) = y1 (5) = 0, whereas the third condition y2 (5) is unknown. Because the
                      differential equation is linear, we replaced ridder with linInterp. In linear inter-
                      polation, the two guesses for y2 (5) (u1 and u2 ) are not important, so we left them as
                      they were in Example 8.1. The adaptive Runge–Kutta method (run kut5) was chosen
                      for the integration.


                      #!/usr/bin/python
                      ## example8_3
                      from numpy import zeros,array
                      from run_kut5 import *
                      from linInterp import *
                      from printSoln import *


                      def initCond(u):        # Initial values of [y,y’,y"];
                                              # use ’u’ if unknown
                           return array([0.0, 0.0, u])


                      def r(u):     # Boundary condition residual--see Eq. (8.3)
                           X,Y = integrate(F,xStart,initCond(u),xStop,h)
                           y = Y[len(Y) - 1]
                           r = y[0] - 2.0
                           return r


                      def F(x,y):      # First-order differential equations
                           F = zeros(3)
                           F[0] = y[1]
                           F[1] = y[2]
                           F[2] = 2.0*y[2] + 6.0*x*y[0]
                           return F


                      xStart = 5.0               # Start of integration
                      xStop = 0.0                # End of integration
                      u1 = 1.0                   # 1st trial value of unknown init. cond.
                      u2 = 2.0                   # 2nd trial value of unknown init. cond.
                      h = -0.1                   # initial step size
                      freq = 2                   # printout frequency
                      u = linInterp(r,u1,u2)
                      X,Y = integrate(F,xStart,initCond(u),xStop,h)
                      printSoln(X,Y,freq)
                      raw_input("\nPress return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                                 December 16, 2009      15:4




           296      Two-Point Boundary Value Problems

                        We forego the rather long printout of the solution and show just the plot:

                                       8

                                       6


                                       4
                                   y
                                       2


                                       0

                                       -2
                                            0           1               2         3                4         5
                                                                              x



                    Higher-Order Equations
                    Let us consider the fourth-order differential equation

                                                         y (4) = f (x, y, y , y , y )                                (8.4a)

                    with the boundary conditions

                                    y(a) = α 1          y (a) = α 2         y(b) = β 1         y (b) = β 2           (8.4b)

                    To solve Eqs. (8.4) with the shooting method, we need four initial conditions at x = a,
                    only two of which are specified. Denoting the unknown initial values by u1 and u2 , the
                    set of initial conditions is

                                    y(a) = α 1         y (a) = u1           y (a) = α 2        y (a) = u2             (8.5)

                    If Eq. (8.4a) is solved with the shooting method using the initial conditions in Eq.
                    (8.5), the computed boundary values at x = b depend on the choice of u1 and u2 . We
                    denote this dependence as

                                                y(b) = θ 1 (u1 , u2 )       y (b) = θ 2 (u1 , u2 )                    (8.6)

                    The correct values u1 and u2 satisfy the given boundary conditions at x = b:

                                                  θ 1 (u1 , u2 ) = β 1      θ 2 (u1 , u2 ) = β 2

                    or, using vector notation

                                                                   θ(u) = β                                           (8.7)

                    These are simultaneous, (generally nonlinear) equations that can be solved by the
                    Newton–Raphson method discussed in Section 4.6. It must be pointed out again that
                    intelligent estimates of u1 and u2 are needed if the differential equation is not linear.
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                      December 16, 2009   15:4




               297     8.2 Shooting Method

                      EXAMPLE 8.4


                                                                                          w0
                                                                                               x
                                                                  L
                                          v
                          The displacement v of the simply supported beam can be obtained by solving
                      the boundary value problem
                                         d 4v   w0 x              d 2v
                                              =              v=        = 0 at x = 0 and x = L
                                         dx 4   EI L              dx 2
                      where E I is the bending rigidity. Determine by numerical integration the slopes at
                      the two ends and the displacement at mid-span.

                      Solution Introducing the dimensionless variables
                                                             x            EI
                                                        ξ=          y=          v
                                                             L           w0 L 4
                      transforms the problem to
                                              d4y                 d2y
                                                   =ξ        y=        = 0 at ξ = 0 and 1
                                              dξ 4                dξ 2
                      The equivalent first-order equations and the boundary conditions are (the prime de-
                      notes d/dξ )
                                                           ⎡ ⎤ ⎡ ⎤
                                                             y0       y1
                                                           ⎢y ⎥ ⎢y ⎥
                                                           ⎢ ⎥ ⎢ 2⎥
                                                       y = ⎢ 1⎥ = ⎢ ⎥
                                                           ⎣ y2 ⎦ ⎣ y3 ⎦
                                                             y3      ξ

                                                  y0 (0) = y2 (0) = y0 (1) = y2 (1) = 0

                            The program listed next is similar to the one in Example 8.1. With appropri-
                      ate changes in functions F(x,y), initCond(u), and r(u) the program can solve
                      boundary value problems of any order greater than 2. For the problem at hand we
                      chose the Bulirsch–Stoer algorithm to do the integration because it gives us control
                      over the printout (we need y precisely at mid-span). The nonadaptive Runge–Kutta
                      method could also be used here, but we would have to guess a suitable step size h.
                            As the differential equation is linear, the solution requires only one iteration with
                      the Newton–Raphson method. In this case, the initial values u1 = dy/dξ |x=0 and u2 =
                      d 3 y/dξ 3 |x=0 are irrelevant; convergence always occurs in one iteration.

                      #!/usr/bin/python
                      ## example8_4
                      from numpy import zeros,array
                      from bulStoer import *
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                   December 16, 2009   15:4




           298      Two-Point Boundary Value Problems

                    from newtonRaphson2 import *
                    from printSoln import *


                    def initCond(u):        # Initial values of [y,y’,y",y"’];
                                             # use ’u’ if unknown
                        return array([0.0, u[0], 0.0, u[1]])


                    def r(u):     # Boundary condition residuals--see Eq. (8.7)
                        r = zeros(len(u))
                        X,Y = bulStoer(F,xStart,initCond(u),xStop,H)
                        y = Y[len(Y) - 1]
                        r[0] = y[0]
                        r[1] = y[2]
                        return r


                    def F(x,y):      # First-order differential equations
                        F = zeros(4)
                        F[0] = y[1]
                        F[1] = y[2]
                        F[2] = y[3]
                        F[3] = x
                        return F


                    xStart = 0.0                         # Start of integration
                    xStop = 1.0                          # End of integration
                    u = array([0.0, 1.0])                # Initial guess for {u}
                    H = 0.5                              # Printout increment
                    freq = 1                             # Printout frequency
                    u = newtonRaphson2(r,u,1.0e-4)
                    X,Y = bulStoer(F,xStart,initCond(u),xStop,H)
                    printSoln(X,Y,freq)
                    raw_input("\nPress return to exit")


                       Here is the output:

                          x            y[ 0 ]              y[ 1 ]          y[ 2 ]           y[ 3 ]
                    0.0000e+000      0.0000e+000         1.9444e-002     0.0000e+000 -1.6667e-001
                    5.0000e-001      6.5104e-003         1.2153e-003 -6.2500e-002 -4.1667e-002
                    1.0000e+000 -2.4670e-014 -2.2222e-002 -2.7190e-012                    3.3333e-001


                       Noting that

                                          dv   dv dξ         w0 L 4 dy   1   w0 L 3 dy
                                             =       =                     =
                                          dx   dξ dx          E I dξ     L    E I dξ
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                               December 16, 2009   15:4




               299     8.2 Shooting Method

                      we obtain
                                                    dv                                w0 L 3
                                                               = 19.444 × 10−3
                                                    dx   x=0                           EI
                                                    dv                                   w0 L 3
                                                               = −22.222 × 10−3
                                                    dx   x=L                              EI
                                                                                      w0 L 4
                                                   v|x=0.5L = 6.5104 × 10−3
                                                                                       EI
                      which agree with the analytical solution (easily obtained by direct integration of the
                      differential equation).

                      EXAMPLE 8.5
                      Solve
                                                                         4 3
                                                               y (4) +     y =0
                                                                         x
                      with the boundary conditions

                                             y(0) = y (0) = 0            y (1) = 0         y (1) = 1

                      and plot y versus x.

                      Solution Our first task is to handle the indeterminacy of the differential equation at
                      the origin, where x = y = 0. The problem is resolved by applying L’Hˆospital’s rule:
                      4y 3 /x → 12y 2 y as x → 0. Thus, the equivalent first-order equations and the bound-
                      ary conditions that we use in the solution are
                                                            ⎡                        ⎤
                                                   ⎡ ⎤                  y1
                                                     y0     ⎢                        ⎥
                                                   ⎢y ⎥ ⎢   ⎢
                                                                        y2           ⎥
                                                                                     ⎥
                                                   ⎢ 1⎥ ⎢                            ⎥
                                              y =⎢ ⎥=⎢                  y3           ⎥
                                                   ⎣ y2 ⎦ ⎢                          ⎥
                                                            ⎣ −12y0 y1 if x = 0
                                                                     2
                                                                                     ⎦
                                                     y3
                                                                −4y03 /x otherwise


                                             y0 (0) = y1 (0) = 0         y2 (1) = 0          y3 (1) = 1

                           Because the problem is nonlinear, we need reasonable estimates for y (0) and
                      y (0). Based on the boundary conditions y (1) = 0 and y (1) = 1, the plot of y is
                      likely to look something like this:


                                             y"
                                             0                                       1         1
                                                                                         1         x
P1: PHB

CUUS884-Kiusalaas    CUUS884-08     978 0 521 19132 6                                December 16, 2009      15:4




           300      Two-Point Boundary Value Problems

                        If we are right, then y (0) < 0 and y (0) > 0. Based on this rather scanty infor-
                    mation, we try y (0) = −1 and y (0) = 1.
                        The following program uses the adaptive Runge–Kutta method (run kut5) for
                    integration:


                    #!/usr/bin/python
                    ## example8_5
                    from numpy import zeros,array
                    from run_kut5 import *
                    from newtonRaphson2 import *
                    from printSoln import *


                    def initCond(u):       # Initial values of [y,y’,y",y"’];
                                           # use ’u’ if unknown
                         return array([0.0, 0.0, u[0], u[1]])


                    def r(u):     # Boundary condition residuals-- see Eq. (8.7)
                         r = zeros(len(u))
                         X,Y = integrate(F,x,initCond(u),xStop,h)
                         y = Y[len(Y) - 1]
                         r[0] = y[2]
                         r[1] = y[3] - 1.0
                         return r


                    def F(x,y):     # First-order differential equations
                         F = zeros(4)
                         F[0] = y[1]
                         F[1] = y[2]
                         F[2] = y[3]
                         if x == 0.0: F[3] = -12.0*y[1]*y[0]**2
                         else:           F[3] = -4.0*(y[0]**3)/x
                         return F


                    x = 0.0                             # Start of integration
                    xStop = 1.0                         # End of integration
                    u = array([-1.0, 1.0])              # Initial guess for u
                    h = 0.1                             # Initial step size
                    freq = 1                            # Printout frequency
                    u = newtonRaphson2(r,u,1.0e-5)
                    X,Y = integrate(F,x,initCond(u),xStop,h)
                    printSoln(X,Y,freq)
                    raw_input("\nPress return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-08   978 0 521 19132 6                                         December 16, 2009    15:4




               301     8.2 Shooting Method

                          The results are:

                            x           y[ 0 ]                y[ 1 ]            y[ 2 ]             y[ 3 ]
                      0.0000e+000      0.0000e+000        0.0000e+000 -9.7607e-001                 9.7131e-001
                      1.0000e-001 -4.7184e-003 -9.2750e-002 -8.7893e-001                           9.7131e-001
                      3.9576e-001 -6.6403e-002 -3.1022e-001 -5.9165e-001                           9.7152e-001
                      7.0683e-001 -1.8666e-001 -4.4722e-001 -2.8896e-001                           9.7627e-001
                      9.8885e-001 -3.2061e-001 -4.8968e-001 -1.1144e-002                           9.9848e-001
                      1.0000e+000 -3.2607e-001 -4.8975e-001 -6.7428e-011                           1.0000e+000


                                      0.000

                                      -0.050

                                      -0.100
                                  y




                                      -0.150

                                      -0.200

                                      -0.250

                                      -0.300

                                      -0.350
                                            0.00       0.20       0.40         0.60       0.80      1.00
                                                                           x

                      By good fortune, our initial estimates y (0) = −1 and y (0) = 1 were very close to the
                      final values.

                      PROBLEM SET 8.1
                       1. Numerical integration of the initial value problem

                                                   y +y −y =0            y(0) = 0      y (0) = 1

                          yielded y(1) = 0.741028. What is the value of y (0) that would result in y(1) = 1,
                          assuming that y(0) is unchanged?
                       2. The solution of the differential equation

                                                              y + y + 2y = 6

                          with the initial conditions y(0) = 2, y (0) = 0, and y (0) = 1 yielded y(1) =
                          3.03765. When the solution was repeated with y (0) = 0 (the other conditions
                          being unchanged), the result was y(1) = 2.72318. Determine the value of y (0) so
                          that y(1) = 0.
                       3. Roughly sketch the solution of the following boundary value problems. Use the
                          sketch to estimate y (0) for each problem.

                                               (a)    y = −e−y         y(0) = 1       y(1) = 0.5

                                               (b)    y = 4y 2      y(0) = 10         y (1) = 0

                                               (c)    y = cos(xy)         y(0) = 0      y(1) = 2
P1: PHB

CUUS884-Kiusalaas    CUUS884-08       978 0 521 19132 6                                            December 16, 2009   15:4




           302      Two-Point Boundary Value Problems

                     4. Using a rough sketch of the solution estimate of y(0) for the following boundary
                        value problems.

                                           (a)     y = y 2 + xy         y (0) = 0       y(1) = 2
                                                        2
                                          (b)      y = − y − y2            y (0) = 0       y(1) = 2
                                                        x
                                           (c)     y = −x(y )2          y (0) = 2       y(1) = 1

                     5. Obtain a rough estimate of y (0) for the boundary value problem

                                                               y + 5y y 2 = 0


                                                    y(0) = 0         y (0) = 1       y(1) = 0

                     6. Obtain rough estimates of y (0) and y (0) for the boundary value problem

                                                          y (4) + 2y + y sin y = 0


                                              y(0) = y (0) = 0          y(1) = 5        y (1) = 0

                                                  ˙
                     7. Obtain rough estimates of x(0)     ˙
                                                       and y(0) for the boundary value problem

                                              x¨ + 2x 2 − y = 0          x(0) = 1       x(1) = 0

                                                 y¨ + y 2 − 2x = 1       y(0) = 0       y(1) = 1

                     8.     Solve the boundary value problem

                                         y + (1 − 0.2x) y 2 = 0           y(0) = 0        y(π/2) = 1

                     9.     Solve the boundary value problem

                                           y + 2y + 3y 2 = 0             y(0) = 0       y(2) = −1

                    10.     Solve the boundary value problem

                                             y + sin y + 1 = 0            y(0) = 0       y(π ) = 0

                    11.     Solve the boundary value problem
                                                     1
                                             y +       y +y =0           y(0) = 1       y (2) = 0
                                                     x
                        and plot y versus x. Warning: y changes very rapidly near x = 0.
                    12.   Solve the boundary value problem

                                           y − 1 − e−x y = 0              y(0) = 1       y(∞) = 0

                          and plot y versus x. Hint: Replace the infinity by a finite value β. Check your
                          choice of β by repeating the solution with 1.5β. If the results change, you must
                          increase β.
P1: PHB

CUUS884-Kiusalaas    CUUS884-08     978 0 521 19132 6                                              December 16, 2009   15:4




               303     8.2 Shooting Method

                      13.     Solve the boundary value problem
                                                              1    1
                                                         y = − y + 2 y + 0.1(y )3
                                                              x   x

                                                    y(1) = 0          y (1) = 0            y(2) = 1

                      14.     Solve the boundary value problem

                                                              y + 4y + 6y = 10

                                                   y(0) = y (0) = 0             y(3) − y (3) = 5

                      15.     Solve the boundary value problem

                                                              y + 2y + sin y = 0

                                                 y(−1) = 0           y (−1) = −1             y (1) = 1

                      16.     Solve the differential equation in Prob. 15 with the boundary conditions

                                                    y(−1) = 0             y(0) = 0          y(1) = 1

                          (this is a three-point boundary value problem).
                      17.   Solve the boundary value problem

                                                                     y (4) = −xy 2

                                            y(0) = 5         y (0) = 0         y (1) = 0         y (1) = 2

                      18.     Solve the boundary value problem

                                                                    y (4) = −2yy

                                                 y(0) = y (0) = 0           y(4) = 0          y (4) = 1

                      19.

                                             y
                                                        v0

                                                        θ
                                            t =0                                                       x
                                                              8000 m                      t = 10 s
                            A projectile of mass m in free flight experiences the aerodynamic drag force Fd =
                            cv 2 , where v is the velocity. The resulting equations of motion are
                                                                 c                    c
                                                        x¨ = −     v x˙      y¨ = −     v y˙ − g
                                                                 m                    m

                                                                   v=       x˙ 2 + y˙ 2
P1: PHB

CUUS884-Kiusalaas    CUUS884-08       978 0 521 19132 6                                                  December 16, 2009   15:4




           304      Two-Point Boundary Value Problems

                          If the projectile hits a target 8 km away after a 10-s flight, determine the launch
                          velocity v0 and its angle of inclination θ. Use m = 20 kg, c = 3.2 × 10−4 kg/m, and
                          g = 9.80665 m/s2 .
                    20.

                                                                   w0
                                       N                                                         N
                                                                                                         x
                                                                    L
                                             v
                          The simply supported beam carries a uniform load of intensity w0 and the tensile
                          force N. The differential equation for the vertical displacement v can be shown
                          to be
                                                             d 4v   N d 2v     w0
                                                                4
                                                                  −          =
                                                             dx     E I dx 2   EI
                          where E I is the bending rigidity. The boundary conditions are v = d 2v/dx 2 = 0
                                                                        x           EI
                          at x = 0 and L. Changing the variables to ξ = and y =           v transforms the
                                                                        L          w0 L 4
                          problem to the dimensionless form
                                                      d4y    d2y                          NL 2
                                                         4
                                                           −β 2 =1                 β=
                                                      dξ     dξ                           EI

                                                            d2y                        d2y
                                                 y|ξ =0 =                 = y|ξ =0 =                =0
                                                            dξ 2   ξ =0                dξ 2   x=1

                        Determine the maximum displacement if (a) β = 1.65929 and (b) β = −1.65929
                        (N is compressive).
                    21.   Solve the boundary value problem

                                             y + yy = 0              y(0) = y (0) = 0, y (∞) = 2

                          and plot y(x) and y (x). This problem arises in determining the velocity profile of
                          the boundary layer in incompressible flow (Blasius solution).
                    22.



                                        w0                                                       2w0
                                                                                                   x
                                                                      L
                                           v
                          The differential equation that governs the displacement v of the beam shown is
                                                              d 4v   w0    x
                                                                   =    1+
                                                              dx 4   EI    L
P1: PHB

CUUS884-Kiusalaas    CUUS884-08     978 0 521 19132 6                                             December 16, 2009    15:4




               305     8.3 Finite Difference Method

                          The boundary conditions are

                                                      d 2v                           dv
                                               v=          = 0 at x = 0         v=      = 0 at x = L
                                                      dx 2                           dx
                          Integrate the differential equation numerically and plot the displacement. Follow
                          the steps used in solving a similar problem in Example 8.4.


              8.3     Finite Difference Method

                      In the finite difference method we divide the range of integration (a, b) into m equal
                      subintervals of length h each, as shown in Fig. 8.1. The values of the numerical so-
                      lution at the mesh points are denoted by yi , i = 0, 1, . . . , m; the purpose of the two
                      points outside (a, b) will be explained shortly. We now make two approximations:

                      1. The derivatives of y in the differential equation are replaced by the finite differ-
                         ence expressions. It is common practice to use the first central difference approx-
                         imations (see Chapter 5):
                                                    yi+1 − yi−1               yi−1 − 2yi + yi+1
                                             yi =                      yi =                           etc.            (8.8)
                                                        2h                           h2
                      2. The differential equation is enforced only at the mesh points.

                           As a result, the differential equations are replaced by m + 1 simultaneous alge-
                      braic equations, the unknowns being yi , i = 0, 1, . . . .m. If the differential equation is
                      nonlinear, the algebraic equations will also be nonlinear and must be solved by the
                      Newton–Raphson method.
                           Because the truncation error in a first central difference approximation is O(h2 ),
                      the finite difference method is not nearly as accurate as the shooting method – recall
                      that the Runge–Kutta method has a truncation error of O(h5 ). Therefore, the conver-
                      gence criterion specified in the Newton–Raphson method should not be too severe.

                         y




                                                                                      ym - 2 y
                                                                                              m-1y     m     ym + 1
                                                                  y2
                                                         y1
                                                y0
                                       y-1

                                                                                                                      x
                                      x-1      x0       x1     x2                    xm - 2 xm - 1 xm        xm + 1
                                               a                                                   b
                         Figure 8.1. Finite difference mesh.
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                                      December 16, 2009         15:4




           306      Two-Point Boundary Value Problems

                    Second-Order Differential Equation
                    Consider the second-order differential equation

                                                                 y = f (x, y, y )

                    with the boundary conditions

                                                      y(a) = α           or       y (a) = α

                                                      y(b) = β           or       y (b) = β

                        Approximating the derivatives at the mesh points by finite differences, the prob-
                    lem becomes
                                  yi−1 − 2yi + yi+1                          yi+1 − yi−1
                                                    =f           xi , yi ,                     , i = 0, 1, . . . , m         (8.9)
                                         h2                                      2h

                                                                         y1 − y−1
                                                    y0 = α        or              =α                                       (8.10a)
                                                                            2h
                                                                         ym+1 − ym−1
                                                ym = β            or                 =β                                    (8.10b)
                                                                              2h
                    Note the presence of y−1 and ym+1 , which are associated with points outside solution
                    domain (a, b). This “spillover” can be eliminated by using the boundary conditions.
                    But before we do that, let us rewrite Eqs. (8.9) as
                                                                                           y1 − y−1
                                         y−1 − 2y0 + y1 − h2 f                x0 , y 0 ,              =0                        (a)
                                                                                              2h

                                                                       yi+1 − yi−1
                            yi−1 − 2yi + yi+1 − h2 f      xi , yi ,                         = 0, i = 1, 2, . . . , m − 1        (b)
                                                                           2h

                                                                                           ym+1 − ym−1
                                     ym−1 − 2ym + ym+1 − h2 f                 xm , yi ,                    =0                   (c)
                                                                                               2h
                        The boundary conditions on y are easily dealt with: Eq. (a) is simply replaced
                    by y0 − α = 0 and Eq. (c) is replaced by ym − β = 0. If y are prescribed, we obtain
                    from Eqs. (8.10) y−1 = y1 − 2hα and ym+1 = ym−1 + 2hβ, which are then substituted
                    into Eqs. (a) and (c), respectively. Hence, we finish up with m + 1 equations in the
                    unknowns y0 , y1 , . . . , ym :

                                     y0 − α = 0                                if y(a) = α
                                                                                                                           (8.11a)
                                     −2y0 + 2y1 − h2 f (x0 , y0 , α) − 2hα = 0 if y (a) = α

                                                                 yi+1 − yi−1
                         yi−1 − 2yi + yi+1 − h2 f    xi , yi ,                        = 0 i = 1, 2, . . . , m − 1          (8.11b)
                                                                     2h


                                    ym − β = 0                                 if y(b) = β
                                                                                                                           (8.11c)
                                    2ym−1 − 2ym − h2 f (xm , ym , β) + 2hβ = 0 if y (b) = β
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                          December 16, 2009   15:4




               307     8.3 Finite Difference Method

                      EXAMPLE 8.6
                      Write out Eqs. (8.11) for the following linear boundary value problem using m = 10:

                                            y = −4y + 4x                 y(0) = 0     y (π/2) = 0

                      Solve these equations with a computer program.

                      Solution In this case α = y(0) = 0, β = y (π/2) = 0, and f (x, y, y ) = −4y + 4x.
                      Hence Eqs. (8.11) are

                                                                             y0 = 0

                                   yi−1 − 2yi + yi+1 − h (−4yi + 4xi ) = 0, i = 1, 2, . . . , m − 1
                                                            2


                                       2y9 − 2y10 − h2 (−4y10 + 4x10 ) = 0

                      or, using matrix notation,
                                 ⎡                                                    ⎤⎡    ⎤ ⎡         ⎤
                                   1      0                                             y0      0
                                 ⎢1 −2 + 4h2        1                               ⎥ ⎢ y ⎥ ⎢ 4h2 x ⎥
                                 ⎢                                                  ⎥⎢ 1 ⎥ ⎢         1 ⎥
                                 ⎢                                                  ⎥⎢. ⎥ ⎢.            ⎥
                                 ⎢       ..        ..           ..                  ⎥⎢. ⎥ = ⎢.          ⎥
                                 ⎢          .         .              .              ⎥⎢. ⎥ ⎢.            ⎥
                                 ⎢                                                  ⎥⎢      ⎥ ⎢ 2       ⎥
                                 ⎣                     1   −2 + 4h2           1     ⎦ ⎣ y9 ⎦ ⎣ 4h x9 ⎦
                                                              2            −2 + 4h2     y10     4h2 x10
                           Note that the coefficient matrix is tridiagonal, so the equations can be solved ef-
                      ficiently by the decomposition and back substitution routines in module LUdecomp3,
                      described in Section 2.4. Recalling that in LUdecomp3 the diagonals of the coefficient
                      matrix are stored in vectors c, d, and e, we arrive at the following program:

                      #!/usr/bin/python
                      ## example8_6
                      from numpy import zeros,ones,array,arange
                      from LUdecomp3 import *
                      from math import pi


                      def equations(x,h,m): # Set up finite difference eqs.
                           h2 = h*h
                           d = ones(m + 1)*(-2.0 + 4.0*h2)
                           c = ones(m)
                           e = ones(m)
                           b = ones(m+1)*4.0*h2*x
                           d[0] = 1.0
                           e[0] = 0.0
                           b[0] = 0.0
                           c[m-1] = 2.0
                           return c,d,e,b


                      xStart = 0.0               # x at left end
                      xStop = pi/2.0             # x at right end
P1: PHB

CUUS884-Kiusalaas    CUUS884-08     978 0 521 19132 6                                December 16, 2009    15:4




           308      Two-Point Boundary Value Problems

                    m = 10                     # Number of mesh spaces
                    h = (xStop - xStart)/m
                    x = arange(xStart,xStop + h,h)
                    c,d,e,b = equations(x,h,m)
                    c,d,e = LUdecomp3(c,d,e)
                    y = LUsolve3(c,d,e,b)
                    print "\n             x                    y"
                    for i in range(m + 1):
                         print "%14.5e %14.5e" %(x[i],y[i])
                    raw_input("\nPress return to exit")



                        The solution is

                             x                   y
                      0.00000e+000        0.00000e+000
                      1.57080e-001        3.14173e-001
                      3.14159e-001        6.12841e-001
                      4.71239e-001        8.82030e-001
                      6.28319e-001        1.11068e+000
                      7.85398e-001        1.29172e+000
                      9.42478e-001        1.42278e+000
                      1.09956e+000        1.50645e+000
                      1.25664e+000        1.54995e+000
                      1.41372e+000        1.56451e+000
                      1.57080e+000        1.56418e+000


                        The exact solution of the problem is

                                                         y = x − sin 2x

                    which yields y(π /2) = π/2 = 1. 57080. Thus, the error in the numerical solution is
                    about 0.4%. More accurate results can be achieved by increasing m. For example,
                    with m = 100, we would get y(π /2) = 1.57073, which is in error by only 0.0002%.

                    EXAMPLE 8.7
                    Solve the boundary value problem

                                              y = −3yy       y(0) = 0     y(2) = 1

                    with the finite difference method. Use m = 10 and compare the output with the re-
                    sults of the shooting method in Example 8.1.

                    Solution As the problem is nonlinear, Eqs. (8.11) must be solved by the Newton–
                    Raphson method. The program listed here can be used as a model for other second-
                    order boundary value problems. The function residual(y) returns the residuals
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                 December 16, 2009    15:4




               309     8.3 Finite Difference Method

                      of the finite difference equations, which are the left-hand sides of Eqs. (8.11). The
                      differential equation y = f (x, y, y ) is defined in the function F(x,y,yPrime). In
                      this problem, we chose for the initial solution yi = 0.5xi , which corresponds to the
                      dashed straight line shown in the rough plot of y in Example 8.1. The starting values
                      of y0 , y1 , . . . , ym are specified by function startSoln(x). Note that we relaxed the
                      convergence criterion in the Newton–Raphson method to 1.0 × 10−5 , which is more
                      in line with the truncation error in the finite difference method.



                      #!/usr/bin/python
                      ## example8_7
                      from numpy import zeros,array,arange
                      from newtonRaphson2 import *


                      def residual(y):        # Residuals of finite diff. Eqs. (8.11)
                           r = zeros(m + 1)
                           r[0] = y[0]
                           r[m] = y[m] - 1.0
                           for i in range(1,m):
                                  r[i] = y[i-1] - 2.0*y[i] + y[i+1]                                      \
                                       - h*h*F(x[i],y[i],(y[i+1] - y[i-1])/(2.0*h))
                           return r


                      def F(x,y,yPrime):         # Differential eqn. y" = F(x,y,y’)
                           F = -3.0*y*yPrime
                           return F


                      def startSoln(x): # Starting solution y(x)
                           y = zeros(m + 1)
                           for i in range(m + 1): y[i] = 0.5*x[i]
                           return y


                      xStart = 0.0                     # x at left end
                      xStop = 2.0                      # x at right end
                      m = 10                           # Number of mesh intevals
                      h = (xStop - xStart)/m
                      x = arange(xStart,xStop + h,h)
                      y = newtonRaphson2(residual,startSoln(x),1.0e-5)
                      print "\n              x                  y"
                      for i in range(m + 1):
                           print "%14.5e %14.5e" %(x[i],y[i])
                      raw_input("\nPress return to exit")
P1: PHB

CUUS884-Kiusalaas     CUUS884-08      978 0 521 19132 6                                            December 16, 2009      15:4




           310      Two-Point Boundary Value Problems

                       Here is the output from our program together with the solution obtained in
                    Example 8.1.

                             x                     y            y from Ex. 8.1
                      0.00000e+000         0.00000e+000           0.00000e+000
                      2.00000e-001         3.02404e-001           2.94050e-001
                      4.00000e-001         5.54503e-001           5.41710e-001
                      6.00000e-001         7.34691e-001           7.21875e-001
                      8.00000e-001         8.49794e-001           8.39446e-001
                      1.00000e+000         9.18132e-001           9.10824e-001
                      1.20000e+000         9.56953e-001           9.52274e-001
                      1.40000e+000         9.78457e-001           9.75724e-001
                      1.60000e+000         9.90201e-001           9.88796e-001
                      1.80000e+000         9.96566e-001           9.96023e-001
                      2.00000e+000         1.00000e+000           1.00000e+000

                         The maximum discrepancy between the solutions is 1.8% occurring at x = 0.6.
                    As the shooting method used in Example 8.1 is considerably more accurate than the
                    finite difference method, the discrepancy can be attributed to truncation error in the
                    finite difference solution. This error would be acceptable in many engineering prob-
                    lems. Again, accuracy can be increased by using a finer mesh. With m = 100 we can
                    reduce the error to 0.07%, but we must question whether the 10-fold increase in com-
                    putation time is really worth the extra precision.


                    Fourth-Order Differential Equation
                    For the sake of brevity we limit our discussion to the special case where y and y do
                    not appear explicitly in the differential equation; that is, we consider


                                                          y (4) = f (x, y, y )

                    We assume that two boundary conditions are prescribed at each end of the solution
                    domain (a, b). Problems of this form are commonly encountered in beam theory.
                        Again, we divide the solution domain into m intervals of length h each. Replacing
                    the derivatives of y by finite differences at the mesh points, we get the finite difference
                    equations
                             yi−2 − 4yi−1 + 6yi − 4yi+1 + yi+2                        yi−1 − 2yi + yi+1
                                                               =f         xi , yi ,                              (8.12)
                                            h4                                               h2
                    where i = 0, 1, . . . , m. It is more revealing to write these equations as
                                                                                  y−1 − 2y0 + y1
                            y−2 − 4y−1 + 6y0 − 4y1 + y2 − h4 f       x0 , y 0 ,                     =0          (8.13a)
                                                                                        h2

                                                                                  y0 − 2y1 + y2
                             y−1 − 4y0 + 6y1 − 4y2 + y3 − h4 f       x1 , y 1 ,                    =0           (8.13b)
                                                                                       h2
P1: PHB

CUUS884-Kiusalaas    CUUS884-08      978 0 521 19132 6                                             December 16, 2009      15:4




               311     8.3 Finite Difference Method

                                                                                       y1 − 2y2 + y3
                                  y0 − 4y1 + 6y2 − 4y3 + y4 − h4 f        x2 , y 2 ,                   =0              (8.13c)
                                                                                            h2

                                                                     ..
                                                                      .

                                                                                                ym−2 − 2ym−1 + ym
                        ym−3 − 4ym−2 + 6ym−1 − 4ym + ym+1 − h4 f               xm−1 , ym−1 ,                            =0
                                                                                                        h2
                                                                                                                       (8.13d)
                                                                                               ym−1 − 2ym + ym+1
                          ym−2 − 4ym−1 + 6ym − 4ym+1 + ym+2 − h4 f                 xm , ym ,                        =0
                                                                                                       h2
                                                                                                       (8.13e)
                      We now see that there are four unknowns, y−2 , y−1 , ym+1 , and ym+2 , that lie outside
                      the solution domain and must be eliminated by applying the boundary conditions, a
                      task that is facilitated by Table 8.1.

                                       Bound. cond.      Equivalent finite difference expression
                                            y(a) = α     y0 = α
                                           y (a) = α     y−1 = y1 − 2hα
                                          y (a) = α      y−1 = 2y0 − y1 + h2 α
                                          y (a) = α      y−2 = 2y−1 − 2y1 + y2 − 2h3 α
                                            y(b) = β     ym = β
                                           y (b) = β     ym+1 = ym−1 + 2hβ
                                          y (b) = β      ym+1 = 2ym − ym−1 + h2 β
                                          y (b) = β      ym+2 = 2ym+1 − 2ym−1 + ym−2 + 2h3 β

                                      Table 8.1

                           The astute observer may notice that some combinations of boundary conditions
                      will not work in eliminating the “spillover.” One such combination is clearly y(a) = α 1
                      and y (a) = α 2 . The other one is y (a) = α 1 and y (a) = α 2 . In the context of beam
                      theory, this makes sense: we can impose either a displacement y or a shear force
                      E I y at a point, but it is impossible to enforce both of them simultaneously. Similarly,
                      it makes no physical sense to prescribe both the slope y and the bending moment
                      E I y at the same point.

                      EXAMPLE 8.8

                                                                  P
                                                                                                       x
                                                                L
                                           v
                          The uniform beam of length L and bending rigidity E I is attached to rigid sup-
                      ports at both ends. The beam carries a concentrated load P at its mid-span. If we
P1: PHB

CUUS884-Kiusalaas    CUUS884-08        978 0 521 19132 6                                                    December 16, 2009         15:4




           312      Two-Point Boundary Value Problems

                    utilize symmetry and model only the left half of the beam, the displacement v can be
                    obtained by solving the boundary value problem

                                                                       d 4v
                                                                  EI        =0
                                                                       dx 4


                                            dv                        dv                           d 3v
                           v|x=0 = 0                   =0                          =0         EI                   = −P/2
                                            dx   x=0                  dx   x=L/2                   dx 3   x=L/2


                    Use the finite difference method to determine the displacement and the bending mo-
                    ment M = −E I d 2v/dx 2 at the mid-span (the exact values are v = P L 3 /(192E I ) and
                    M = P L/8).

                    Solution By introducing the dimensionless variables

                                                                  x                 EI
                                                             ξ=             y=           v
                                                                  L                 P L3

                    the problem becomes

                                                                      d4y
                                                                           =0
                                                                      dξ 4


                                                 dy                        dy                      d3y                  1
                              y|ξ =0 = 0                     =0                          =0                        =−
                                                 dξ   ξ =0                 dξ   ξ =1/2             dξ 3   ξ =1/2        2

                        We now proceed to writing Eqs. (8.13) taking into account the boundary condi-
                    tions. Referring to Table 8.1, the finite difference expressions of the boundary condi-
                    tions at the left end are y0 = 0 and y−1 = y1 . Hence, Eqs. (8.13a) and (8.13b) become

                                                                                         y0 = 0                                 (a)

                                                       −4y0 + 7y1 − 4y2 + y3 = 0                                                (b)

                    Equation (8.13c) is

                                                      y0 − 4y1 + 6y2 − 4y3 + y4 = 0                                             (c)

                    At the right end the boundary conditions are equivalent to ym+1 = ym−1 and

                                   ym+2 = 2ym+1 + ym−2 − 2ym−1 + 2h3 (−1/2) = ym−2 − h3

                    Substitution into Eqs. (8.13d) and (8.13e) yields

                                                 ym−3 − 4ym−2 + 7ym−1 − 4ym = 0                                                 (d)

                                                             2ym−2 − 8ym−1 + 6ym = h               3
                                                                                                                                (e)
P1: PHB

CUUS884-Kiusalaas    CUUS884-08   978 0 521 19132 6                                December 16, 2009   15:4




               313     8.3 Finite Difference Method

                          The coefficient matrix of Eqs. (a)–(e) can be made symmetric by dividing Eq. (e)
                      by 2. The result is
                                    ⎡                                   ⎤⎡      ⎤ ⎡         ⎤
                                       1    0    0                           y0         0
                                    ⎢0      7 −4                        ⎥⎢y     ⎥ ⎢         ⎥
                                    ⎢                 1                 ⎥⎢ 1 ⎥ ⎢ 0 ⎥
                                    ⎢                                   ⎥⎢      ⎥ ⎢         ⎥
                                    ⎢0 −4        6 −4       1           ⎥ ⎢ y2 ⎥ ⎢ 0 ⎥
                                    ⎢                                   ⎥⎢      ⎥ ⎢         ⎥
                                    ⎢     ..   ..   ..    ..   ..       ⎥ ⎢ ..  ⎥ ⎢ .. ⎥
                                    ⎢        .    .    .     .    .     ⎥ ⎢     ⎥ = ⎢       ⎥
                                    ⎢                                   ⎥⎢.     ⎥ ⎢ . ⎥
                                    ⎢                                   ⎥⎢y     ⎥ ⎢         ⎥
                                    ⎢            1 −4       6 −4      1⎥ ⎢ m−2 ⎥ ⎢ 0 ⎥
                                    ⎢                                   ⎥⎢      ⎥ ⎢         ⎥
                                    ⎣                 1 −4       7 −4⎦ ⎣ ym−1 ⎦ ⎣ 0 ⎦
                                                            1 −4      3      ym       0.5h3

                          The foregoing system of equations can be solved with the decomposition and
                      back substitution routines in module LUdecomp5 – see Section 2.4. Recall that LUde-
                      comp5 works with the vectors d, e, and f that form the diagonals of the upper half of
                      the matrix. The constant vector is denoted by b. The program that sets up and solves
                      the equations is as follows:

                      #!/usr/bin/python
                      ## example8_8
                      from numpy import zeros,ones,array,arange
                      from LUdecomp5 import *


                      def equations(x,h,m): # Set up finite difference eqs.
                           h4 = h**4
                           d = ones(m + 1)*6.0
                           e = ones(m)*(-4.0)
                           f = ones(m-1)
                           b = zeros(m+1)
                           d[0] = 1.0
                           d[1] = 7.0
                           e[0] = 0.0
                           f[0] = 0.0
                           d[m-1] = 7.0
                           d[m] = 3.0
                           b[m] = 0.5*h**3
                           return d,e,f,b


                      xStart = 0.0              # x at left end
                      xStop = 0.5               # x at right end
                      m = 20                    # Number of mesh spaces
                      h = (xStop - xStart)/m
                      x = arange(xStart,xStop + h,h)
                      d,e,f,b = equations(x,h,m)
                      d,e,f = LUdecomp5(d,e,f)
                      y = LUsolve5(d,e,f,b)
P1: PHB

CUUS884-Kiusalaas    CUUS884-08          978 0 521 19132 6                                             December 16, 2009   15:4




           314      Two-Point Boundary Value Problems

                    print "\n                 x                           y"
                    for i in range(m + 1):
                          print "%14.5e %14.5e" %(x[i],y[i])
                    raw_input("\nPress return to exit")

                          When we ran the program with m = 20, the last two lines of the output were

                              x                         y
                      4.75000e-001            5.19531e-003
                      5.00000e-001            5.23438e-003

                          Thus at the mid-span we have
                                                             P L3                            P L3
                                            v|x=0.5L =            y|ξ =0.5 = 5.234 38 × 10−3
                                                             EI                              EI

                                  d 2v                P L3     1 d2y                     P L ym−1 − 2ym + ym+1
                                                  =                                  ≈
                                  dx 2   x=0.5L       EI       L 2 dξ 2   ξ =0.5         EI          h2

                                                    P L (5.19531 − 2(5.23438) + 5.19531) × 10−3
                                                  =
                                                    EI                   0.0252
                                                               PL
                                                  = −0.125 024
                                                               EI

                                                                     d 2v
                                              M|x=0.5L = −E I                        = 0.125 024 P L
                                                                     dx 2   ξ =0.5

                    In comparison, the exact solution yields
                                                                                            P L3
                                                       v|x=0.5L = 5.208 33 × 10−3
                                                                                            EI
                                                      M|x=0.5L = = 0.125 000 P L

                    PROBLEM SET 8.2

                    Problems 1–5 Use first central difference approximations to transform the boundary
                    value problem shown into simultaneous equations Ay = b.

                    Problems 6–10 Solve the given boundary value problem with the finite difference
                    method using m = 20.

                     1.   y = (2 + x)y, y(0) = 0, y (1) = 5.
                     2.   y = y + x 2 , y(0) = 0, y(1) = 1.
                     3.   y = e−x y , y(0) = 1, y(1) = 0.
                     4.   y (4) = y − y, y(0) = 0, y (0) = 1, y(1) = 0, y (1) = −1.
                     5.   y (4) = −9y + x, y(0) = y (0) = 0, y (1) = y (1) = 0.
                     6.      y = xy, y(1) = 1.5 y(2) = 3.
                     7.      y + 2y + y = 0, y(0) = 0, y(1) = 1. Exact solution is y = xe1−x .
                     8.        x 2 y + xy + y = 0, y(1) = 0, y(2) = 0.638961. Exact solution is y = sin
                          (ln x).
P1: PHB

CUUS884-Kiusalaas    CUUS884-08       978 0 521 19132 6                                                        December 16, 2009   15:4




               315     8.3 Finite Difference Method

                       9.     y = y 2 sin y, y (0) = 0, y(π ) = 1.
                      10.     y + 2y(2xy + y) = 0, y(0) = 1/2, y (1) = −2/9. Exact solution is y = (2 +
                             2 −1
                            x ) .
                      11.

                                                                          w0
                                            I0                                                               I0
                                                                                                                            x
                                           L /4                       L /2                      I1          L /4
                                  v
                            The simply supported beam consists of three segments with the moments of in-
                            ertia I0 and I1 as shown. A uniformly distributed load of intensity w0 acts over the
                            middle segment. Modeling only the left half of the beam, the differential equa-
                            tion
                                                                          d 2v    M
                                                                               =−
                                                                          dx 2    EI
                            for the displacement v is
                                                               ⎧x                                                       L
                                                               ⎪
                                                               ⎪                                          in 0 < x <
                                                               ⎪
                                                               ⎪
                                         d 2v     w0 L 2       ⎨L                                                       4
                                              = −        ×
                                         dx 2     4E I0 ⎪  ⎪                                         2
                                                           ⎪
                                                           ⎪ I0            x          x   1                    L     L
                                                           ⎩                 −2         −                 in     <x<
                                                             I1            L          L   4                    4     2

                            Introducing the dimensionless variables
                                                               x                E I0                      I1
                                                          ξ=              y=          v          γ =
                                                               L               w0 L 4                     I0
                            the differential equation becomes
                                                     ⎧ 1                                                            1
                                                     ⎪
                                                     ⎪ − ξ                                       in 0 < ξ <
                                                     ⎪
                                                     ⎪
                                               2
                                              d y    ⎨ 4                                                            4
                                                   =
                                              dξ 2   ⎪ 1
                                                     ⎪                                      2
                                                     ⎪
                                                     ⎪               1                                   1      1
                                                     ⎩−     ξ −2 ξ −                             in        <ξ <
                                                         4γ          4                                   4      2

                            with the boundary conditions

                                                           d2y                dy                 d3y
                                                y|ξ =0 =                  =                 =                      =0
                                                           dξ 2    ξ =0       dξ   ξ =1/2        dξ 3     ξ =1/2

                            Use the finite difference method to determine the maximum displacement of the
                            beam using m = 20 and γ = 1.5 and compare it with the exact solution

                                                                                61 w0 L 4
                                                                   vmax =
                                                                               9216 E I0
P1: PHB

CUUS884-Kiusalaas    CUUS884-08          978 0 521 19132 6                                                    December 16, 2009   15:4




           316      Two-Point Boundary Value Problems

                    12.
                                                 M0
                                     d0                                     d                                 d1
                                                             x
                                               v                                L
                          The simply supported, tapered beam has a circular cross section. A couple of
                          magnitude M0 is applied to the left end of the beam. The differential equation
                          for the displacement v is
                                                        d 2v    M     M0 (1 − x/L)
                                                             =−    =−
                                                        dx 2    EI    E I0 (d/d0 )4
                          where
                                                                     d1             x                πd04
                                                   d = d0 1 +           −1                   I0 =
                                                                     d0             L                 64
                          Substituting
                                                              x                  E I0              d1
                                                      ξ=             y=                v    δ=
                                                              L                 M0 L 2             d0
                          the differential equation becomes
                                                              d2y          1−ξ
                                                                   =−
                                                              dξ 2
                                                                      [1 + (δ − 1)ξ ]4
                          with the boundary conditions
                                                              d2y                          d2y
                                                   y|ξ =0 =                 = y|ξ =1 =                   =0
                                                              dx 2   ξ =0                  dx 2   ξ =1

                          Solve the problem with the finite difference method with δ = 1.5 and m = 20;
                          plot y versus ξ . The exact solution is
                                                                  (3 + 2δξ − 3ξ )ξ 2   1
                                                         y =−                        +
                                                                   6(1 + δξ − ξ )
                                                                                2      3δ
                    13.   Solve Example 8.4 by the finite difference method with m = 20. Hint: Compute
                        end slopes from second noncentral differences in Tables 5.3a and 5.3b.
                    14.   Solve Prob. 20 in Problem Set 8.1 with the finite difference method. Use m = 20.
                    15.

                                                                      w0
                                                                                                              x
                                                                      L
                                          v
                          The simply supported beam of length L is resting on an elastic foundation
                          of stiffness k N/m2 . The displacement v of the beam due to the uniformly
P1: PHB

CUUS884-Kiusalaas    CUUS884-08     978 0 521 19132 6                                                              December 16, 2009   15:4




               317     8.3 Finite Difference Method

                            distributed load of intensity w0 N/m is given by the solution of the boundary
                            value problem

                                          d 4v                                      d2y                              d 2v
                                     EI        + kv = w0 ,              v|x=0 =                     = v|x=L =                     =0
                                          dx 4                                      dx 2    x=0                      dx 2   x=L

                            The nondimensional form of the problem is

                                       d2y                                      d2y                                d2y
                                            + γ y = 1,              y|ξ =0 =                    = y|ξ =1 =                       =0
                                       dξ 4                                     dx 2     ξ −0                      dx 2   ξ =1

                            where
                                                                x                EI                          k L4
                                                      ξ=                  y=           v             γ =
                                                                L               w0 L 4                        EI
                          Solve this problem by the finite difference method with γ = 105 and plot y
                          versus ξ .
                      16.   Solve Prob. 15 if the ends of the beam are free and the load is confined to the
                          middle half of the beam. Consider only the left half of the beam, in which case
                          the nondimensional form of the problem is

                                                        d4y                     0 in 0 < ξ < 1/4
                                                             +γy =
                                                        dξ 4                    1 in 1/4 < ξ < 1/2

                                              d2y               d3y                 dy                  d3y
                                                            =                   =                   =                     =0
                                              dξ 2   ξ =0       dξ 3     ξ =0       dξ     ξ =1/2       dξ 3   ξ =1/2

                      17.     The general form of a linear, second-order boundary value problem is

                                                                y = r (x) + s(x)y + t (x)y

                                                                    y(a) = α or y (a) = α

                                                                    y(b) = β or y (b) = β

                            Write a program that solves this problem with the finite difference method for
                            any user-specified r (x), s(x) and t (x). Test the program by solving Prob. 8.
                      18.

                                                                                                               o
                                                                                                     200 C
                                                            a
                                                                           a /2
                                                                    r
                                                                                                        0o
P1: PHB

CUUS884-Kiusalaas    CUUS884-08    978 0 521 19132 6                                     December 16, 2009    15:4




           318      Two-Point Boundary Value Problems

                       The thick cylinder conveys a fluid with a temperature of 0◦ C. At the same time
                       the cylinder is immersed in a bath that is kept at 200◦ C. The differential equation
                       and the boundary conditions that govern steady-state heat conduction in the
                       cylinder are
                                      d2T     1 dT
                                         2
                                           =−              T |r =a/2 = 0      T |r =a = 200◦ C
                                      dr      r dr
                       where T is the temperature. Determine the temperature profile through the
                       thickness of the cylinder with the finite difference method and compare it with
                       the analytical solution
                                                                     ln r/a
                                                       T = 200 1 −
                                                                     ln 0.5
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                          December 16, 2009    15:4




              9       Symmetric Matrix Eigenvalue Problems




                                       Find λ for which nontrivial solutions of Ax = λx exist.




              9.1     Introduction

                      The standard form of the matrix eigenvalue problem is

                                                                   Ax = λx                                         (9.1)

                      where A is a given n × n matrix. The problem is to find the scalar λ and the vector x.
                      Rewriting Eq. (9.1) in the form

                                                               (A − λI) x = 0                                      (9.2)

                      it becomes apparent that we are dealing with a system of n homogeneous equations.
                      An obvious solution is the trivial one x = 0. A nontrivial solution can exist only if the
                      determinant of the coefficient matrix vanishes, that is, if

                                                                |A − λI| = 0                                       (9.3)

                      Expansion of the determinant leads to the polynomial equation, also known as the
                      characteristic equation

                                                   a 0 + a 1 λ + a 2 λ2 + · · · + a n λn = 0

                      which has the roots λi , i = 1, 2, . . . , n, called the eigenvalues of the matrix A. The so-
                      lutions xi of (A − λi I) x = 0 are known as the eigenvectors..
                           As an example, consider the matrix
                                                               ⎡                  ⎤
                                                                1       −1      0
                                                             ⎢                    ⎥
                                                         A = ⎣ −1        2     −1 ⎦                                 (a)
                                                                0       −1      1

               319
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                     December 16, 2009         15:4




           320      Symmetric Matrix Eigenvalue Problems

                    The characteristic equation is

                                                1 − λ −1               0
                                   |A − λI| =    −1 2 − λ             −1 = −3λ + 4λ2 − λ3 = 0                 (b)
                                                  0   −1             1−λ

                    The roots of this equation are λ1 = 0, λ2 = 1, λ3 = 3. To compute the eigenvector cor-
                    responding the λ3 , we substitute λ = λ3 into Eq. (9.2), obtaining
                                                ⎡                  ⎤⎡ ⎤ ⎡ ⎤
                                                  −2     −1      0     x1       0
                                                ⎢                  ⎥⎢ ⎥ ⎢ ⎥
                                                ⎣ −1     −1     −1 ⎦ ⎣ x2 ⎦ = ⎣ 0 ⎦                           (c)
                                                   0     −1     −2     x3       0

                    We know that the determinant of the coefficient matrix is zero, so that the equations
                    are not linearly independent. Therefore, we can assign an arbitrary value to any one
                    component of x and use two of the equations to compute the other two components.
                    Choosing x1 = 1, the first equation of Eq. (c) yields x2 = −2 and from the third equa-
                    tion we get x3 = 1. Thus, the eigenvector associated with λ3 is
                                                                     ⎡  ⎤
                                                                      1
                                                                   ⎢    ⎥
                                                              x3 = ⎣ −2 ⎦
                                                                      1

                    The other two eigenvectors
                                                         ⎡    ⎤               ⎡ ⎤
                                                            1                   1
                                                         ⎢    ⎥               ⎢ ⎥
                                                    x2 = ⎣ 0 ⎦           x1 = ⎣ 1 ⎦
                                                           −1                   1

                    can be obtained in the same manner.
                        It is sometimes convenient to display the eigenvectors as columns of a matrix X.
                    For the problem at hand, this matrix is
                                                                         ⎡            ⎤
                                                                          1    1    1
                                                                        ⎢             ⎥
                                            X = x1       x2     x3    = ⎣1     0   −2 ⎦
                                                                          1   −1    1

                        It is clear from the foregoing example that the magnitude of an eigenvector is
                    indeterminate; only its direction can be computed from Eq. (9.2). It is customary to
                    normalize the eigenvectors by assigning a unit magnitude to each vector. Thus, the
                    normalized eigenvectors in our example are
                                                     ⎡  √           √      √ ⎤
                                                      1/ 3        1/ 2   1/ 6
                                                    ⎢ √                    √ ⎥
                                                X = ⎣ 1/ 3            0 −2/ 6 ⎦
                                                        √           √      √
                                                      1/ 3       −1/ 2   1/ 6

                    Throughout this chapter, we assume that the eigenvectors are normalized.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                 December 16, 2009    15:4




               321     9.2 Jacobi Method

                          Here are some useful properties of eigenvalues and eigenvectors, given without
                      proof:

                        •   All the eigenvalues of a symmetric matrix are real.
                        •   All the eigenvalues of a symmetric, positive-definite matrix are real and positive.
                        •   The eigenvectors of a symmetric matrix are orthonormal, that is, XT X = I.
                        •   If the eigenvalues of A are λi , then the eigenvalues of A−1 are λi−1 .

                           Eigenvalue problems that originate from physical problems often end up with a
                      symmetric A. This is fortunate, because symmetric eigenvalue problems are easier to
                      solve than their nonsymmetric counterparts (which may have complex eigenvalues).
                      In this chapter, we largely restrict our discussion to eigenvalues and eigenvectors of
                      symmetric matrices.
                           Common sources of eigenvalue problems are the analysis of vibrations and sta-
                      bility. These problems often have the following characteristics:

                        • The matrices are large and sparse (e.g., have a banded structure).
                        • We need to know only the eigenvalues; if eigenvectors are required, only a few of
                          them are of interest.

                         A useful eigenvalue solver must be able to utilize these characteristics to mini-
                      mize the computations. In particular, it should be flexible enough to compute only
                      what we need and no more.



              9.2     Jacobi Method

                      The Jacobi method is a relatively simple iterative procedure that extracts all the
                      eigenvalues and eigenvectors of a symmetric matrix. Its utility is limited to small
                      matrices (less than 20 × 20), because the computational effort increases very rapidly
                      with the size of the matrix. The main strength of the method is its robustness – it
                      seldom fails to deliver.


                      Similarity Transformation and Diagonalization
                      Consider the standard matrix eigenvalue problem

                                                               Ax = λx                                    (9.4)

                      where A is symmetric. Let us now apply the transformation

                                                               x = Px∗                                    (9.5)

                      where P is a nonsingular matrix. Substituting Eq. (9.5) into Eq. (9.4) and premultiply-
                      ing each side by P−1 , we get

                                                         P−1 APx∗ = λP−1 Px∗
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                               December 16, 2009      15:4




           322      Symmetric Matrix Eigenvalue Problems

                    or

                                                               A∗ x∗ = λx∗                                          (9.6)

                    where A∗ = P−1 AP. Because λ was untouched by the transformation, the eigenval-
                    ues of A are also the eigenvalues of A∗ . Matrices that have the same eigenvalues are
                    deemed to be similar, and the transformation between them is called a similarity
                    transformation.
                         Similarity transformations are frequently used to change an eigenvalue problem
                    to a form that is easier to solve. Suppose that we managed by some means to find a P
                    that diagonalizes A∗ . Equations (9.6) then are
                                     ⎡                                           ⎤⎡ ∗⎤ ⎡ ⎤
                                       A ∗11 − λ        0        ···        0        x1     0
                                     ⎢             A ∗22 − λ     ···             ⎥ ⎢ x∗ ⎥ ⎢ 0 ⎥
                                     ⎢      0                               0    ⎥⎢ 2 ⎥ ⎢ ⎥
                                     ⎢                                           ⎥⎢. ⎥ = ⎢. ⎥
                                     ⎢      ..          ..       ..         ..   ⎥⎢. ⎥ ⎢. ⎥
                                     ⎣       .           .          .        .   ⎦⎣. ⎦ ⎣. ⎦
                                                                          ∗
                                            0           0        ···    A nn − λ     xn∗    0

                    which have the solutions

                                            λ1 = A ∗11         λ2 = A ∗22     ···           ∗
                                                                                     λn = A nn                      (9.7)


                                               ⎡ ⎤               ⎡ ⎤                        ⎡ ⎤
                                                  1                 0                          0
                                               ⎢0⎥               ⎢1⎥                        ⎢0⎥
                                               ⎢    ⎥            ⎢    ⎥                     ⎢    ⎥
                                         x∗1 = ⎢    ⎥
                                               ⎢ .. ⎥      x∗2 = ⎢    ⎥
                                                                 ⎢ .. ⎥        ···    xn∗ = ⎢    ⎥
                                                                                            ⎢ .. ⎥
                                               ⎣. ⎦              ⎣. ⎦                       ⎣. ⎦
                                                  0                 0                          1

                    or

                                                   X∗ = x∗1       x∗2   ···    xn∗ = I

                    According to Eq. (9.5), the eigenvectors of A are

                                                         X = PX∗ = PI = P                                           (9.8)

                    Hence, the transformation matrix P contains the eigenvectors of A, and the eigenval-
                    ues of A are the diagonal terms of A∗ .



                    Jacobi Rotation
                    A special similarity transformation is the plane rotation

                                                                 x = Rx∗                                            (9.9)
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                      December 16, 2009     15:4




               323     9.2 Jacobi Method

                      where


                                                                k
                                                       ⎡                             ⎤
                                                   1 0         0       0   0   0 0 0
                                                 ⎢                                   ⎥
                                                 ⎢0 1          0       0   0   0 0 0⎥
                                                 ⎢                                   ⎥
                                                 ⎢0 0          c       0   0   s 0 0⎥
                                                 ⎢                                   ⎥k
                                                 ⎢0 0                          0 0 0⎥
                                                 ⎢             0       1   0         ⎥
                                               R=⎢                                   ⎥                        (9.10)
                                                 ⎢0 0          0       0   1   0 0 0⎥
                                                 ⎢                                   ⎥
                                                 ⎢0 0          −s      0   0   c 0 0⎥
                                                 ⎢                                   ⎥
                                                 ⎢                                   ⎥
                                                 ⎣0 0          0       0   0   0 1 0⎦
                                                   0 0         0       0   0   0 0 1

                      is called the Jacobi rotation matrix. Note that R is an identity matrix modified by the
                      terms c = cos θ and s = sin θ appearing at the intersections of columns/rows k and
                       , where θ is the rotation angle. The rotation matrix has the useful property of being
                      orthogonal, meaning that

                                                                R−1 = RT                                      (9.11)

                      One consequence of orthogonality is that the transformation in Eq. (9.5) has the es-
                      sential characteristic of a rotation: It preserves the magnitude of the vector, that is,
                      |x| = |x∗ |.
                           The similarity transformation corresponding to the plane rotation in Eq. (9.9) is

                                                           A∗ = R−1 AR = RT AR                                (9.12)

                      The matrix A∗ not only has the same eigenvalues as the original matrix A, but thanks
                      to orthogonality of R, it is also symmetric. The transformation in Eq. (9.12) changes
                      only the rows/columns k and of A. The formulas for these changes are
                                               ∗
                                             A kk = c 2 A kk + s 2 A   − 2csA k

                                              A ∗ = c2 A     + s 2 A kk + 2csA k

                                             A k∗ = A ∗k = (c 2 − s 2 )A k + cs(A kk − A )                    (9.13)
                                                ∗      ∗
                                              A ki = A ik = cA ki − sA i , i = k, i =

                                              A ∗i = A i∗ = cA i + sA ki , i = k, i =


                      Jacobi Diagonalization
                      The angle θ in the Jacobi rotation matrix can be chosen so that A k∗ = A ∗k = 0. This
                      suggests the following idea: Why not diagonalize A by looping through all the off-
                      diagonal terms and zero them one by one? This is exactly what Jacobi diagonalization
                      does. However, there is a major snag – the transformation that annihilates an off-
                      diagonal term also undoes some of the previously created zeroes. Fortunately, it turns
                      out that the off-diagonal terms that reappear will be smaller than before. Thus, the
P1: PHB

CUUS884-Kiusalaas       CUUS884-09       978 0 521 19132 6                                        December 16, 2009          15:4




           324      Symmetric Matrix Eigenvalue Problems

                    Jacobi method is an iterative procedure that repeatedly applies Jacobi rotations until
                    the off-diagonal terms have virtually vanished. The final transformation matrix P is
                    the accumulation of individual rotations Ri :

                                                               P = R1 ·R2 ·R3 · ··                                 (9.14)

                    The columns of P finish up being the eigenvectors of A, and the diagonal elements of
                    A∗ = PT AP become the eigenvectors.
                        Let us now look at details of a Jacobi rotation. From Eq. (9.13), we see that A k∗ = 0
                    if

                                                     (c 2 − s 2 )A k + cs(A kk − A ) = 0                              (a)

                    Using the trigonometric identities c 2 − s 2 = cos2 θ − sin2 θ = cos 2θ and cs =
                    cos θ sin θ = (1/2) sin 2θ , Eq. (a) yields
                                                                              2A k
                                                             tan 2θ = −                                               (b)
                                                                           A kk − A
                    which could be solved for θ, followed by computation of c = cos θ and s = sin θ. How-
                    ever, the procedure described next leads to better algorithm.1
                        Introducing the notation
                                                                             A kk − A
                                                        φ = cot 2θ = −                                             (9.15)
                                                                                2A k
                    and utilizing the trigonometric identity
                                                                              2t
                                                              tan 2θ =
                                                                           (1 − t 2 )
                    where t = tan θ , Eq. (b) can be written as

                                                              t 2 + 2φt − 1 = 0

                    which has the roots

                                                              t = −φ ±       φ2 + 1

                    It has been found that the root |t | ≤ 1, which corresponds to |θ| ≤ 45◦ , leads to the
                    more stable transformation. Therefore, we choose the plus sign if φ > 0 and the mi-
                    nus sign if φ ≤ 0, which is equivalent to using

                                                      t = sgn(φ) − |φ| +           φ2 + 1

                    To forestall excessive roundoff error if φ is large, we multiply both sides of the equa-
                    tion by |φ| + φ 2 + 1, which yields
                                                                      sgn(φ)
                                                              t=                                                 (9.16a)
                                                                   |φ| +     φ2 + 1

                    1   The procedure is adapted from W. H. Press et al., Numerical Recipes in Fortran, 2nd ed. (Cambridge
                        University Press, 1992).
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                     December 16, 2009      15:4




               325     9.2 Jacobi Method

                      In the case of very large φ, we should replace Eq. (9.16a) by the approximation
                                                                          1
                                                                 t=                                          (9.16b)
                                                                         2φ

                      to prevent overflow in the computation of φ 2 . Having computed t , we can use the
                                                                        √
                      trigonometric relationship tan θ = sin θ / cos θ = 1 − cos2 θ / cos θ to obtain
                                                                 1
                                                        c= √                  s = tc                          (9.17)
                                                                1 + t2
                          We now improve the transformation formulas in Eqs. (9.13). Solving Eq. (a) for
                      A , we obtain
                                                                              c2 − s 2
                                                        A   = A kk + A k                                         (c)
                                                                                 cs
                      Replacing all occurrences of A by Eq. (c) and simplifying, the transformation for-
                      mulas in Eqs.(9.13) can be written as
                                            ∗
                                          A kk = A kk − t A k

                                          A∗ = A       + tAk

                                          A k∗ = A ∗k = 0                                                     (9.18)
                                            ∗      ∗
                                          A ki = A ik = A ki − s(A i + τ A ki ), i = k, i =

                                           A ∗i = A i∗ = A i + s(A ki − τ A i ), i = k, i =

                      where
                                                                      s
                                                                τ=                                            (9.19)
                                                                     1+c
                      The introduction of τ allowed us to express each formula in the form (original value)
                      + (change), which is helpful in reducing the roundoff error.
                           At the start of Jacobi’s diagonalization process, the transformation matrix P is
                      initialized to the identity matrix. Each Jacobi rotation changes this matrix from P to
                      P∗ = PR. The corresponding changes in the elements of P can be shown to be (only
                      the columns k and are affected)

                                                        Pik∗ = Pik − s(Pi + τ Pik )                           (9.20)

                                                        Pi∗ = Pi + s(Pik − τ Pi )

                           We still have to decide the order in which the off-diagonal elements of A are to
                      be eliminated. Jacobi’s original idea was to attack the largest element because this
                      results in fewest number of rotations. The problem here is that A has to be searched
                      for the largest element after every rotation, which is a time-consuming process. If the
                      matrix is large, it is faster to sweep through it by rows or columns and annihilate ev-
                      ery element above some threshold value. In the next sweep, the threshold is lowered
                      and the process repeated. We adopt Jacobi’s original scheme because of its simpler
                      implementation.
P1: PHB

CUUS884-Kiusalaas        CUUS884-09      978 0 521 19132 6                               December 16, 2009     15:4




           326      Symmetric Matrix Eigenvalue Problems

                         In summary, the Jacobi diagonalization procedure, which uses only the upper
                    half of the matrix, is:

                    1.    Find the largest (absolute value) off-diagonal element A k in the upper half of A.
                    2.    Compute φ, t , c, and s from Eqs. (9.15)–(9.17).
                    3.    Compute τ from Eq. (9.19)
                    4.    Modify the elements in the upper half of A according to Eqs. (9.18).
                    5.    Update the transformation matrix P using Eqs. (9.20).
                    6.    Repeat steps 1–5 until |A k | < ε, where ε is the error tolerance.


                         jacobi

                    This function computes all eigenvalues λi and eigenvectors xi of a symmetric, n × n
                    matrix A by the Jacobi method. The algorithm works exclusively with the upper tri-
                    angular part of A, which is destroyed in the process. The principal diagonal of A is re-
                    placed by the eigenvalues, and the columns of the transformation matrix P become
                    the normalized eigenvectors.


                    ## module jacobi
                    ’’’ lam,x = jacobi(a,tol = 1.0e-9).
                            Solution of std. eigenvalue problem [a]{x} = lambda{x}
                            by Jacobi’s method. Returns eigenvalues in vector {lam}
                            and the eigenvectors as columns of matrix [x].
                    ’’’
                    from numpy import array,identity,diagonal
                    from math import sqrt


                    def jacobi(a,tol = 1.0e-9):


                            def maxElem(a): # Find largest off-diag. element a[k,l]
                                 n = len(a)
                                 aMax = 0.0
                                 for i in range(n-1):
                                      for j in range(i+1,n):
                                             if abs(a[i,j]) >= aMax:
                                                   aMax = abs(a[i,j])
                                                   k = i; l = j
                                 return aMax,k,l


                            def rotate(a,p,k,l): # Rotate to make a[k,l] = 0
                                 n = len(a)
                                 aDiff = a[l,l] - a[k,k]
                                 if abs(a[k,l]) < abs(aDiff)*1.0e-36: t = a[k,l]/aDiff
                                 else:
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                              December 16, 2009   15:4




               327     9.2 Jacobi Method

                                      phi = aDiff/(2.0*a[k,l])
                                      t = 1.0/(abs(phi) + sqrt(phi**2 + 1.0))
                                      if phi < 0.0: t = -t
                                  c = 1.0/sqrt(t**2 + 1.0); s = t*c
                                  tau = s/(1.0 + c)
                                  temp = a[k,l]
                                  a[k,l] = 0.0
                                  a[k,k] = a[k,k] - t*temp
                                  a[l,l] = a[l,l] + t*temp
                                  for i in range(k):          # Case of i < k
                                      temp = a[i,k]
                                      a[i,k] = temp - s*(a[i,l] + tau*temp)
                                      a[i,l] = a[i,l] + s*(temp - tau*a[i,l])
                                  for i in range(k+1,l):      # Case of k < i < l
                                      temp = a[k,i]
                                      a[k,i] = temp - s*(a[i,l] + tau*a[k,i])
                                      a[i,l] = a[i,l] + s*(temp - tau*a[i,l])
                                  for i in range(l+1,n):      # Case of i > l
                                      temp = a[k,i]
                                      a[k,i] = temp - s*(a[l,i] + tau*temp)
                                      a[l,i] = a[l,i] + s*(temp - tau*a[l,i])
                                  for i in range(n):          # Update transformation matrix
                                      temp = p[i,k]
                                      p[i,k] = temp - s*(p[i,l] + tau*p[i,k])
                                      p[i,l] = p[i,l] + s*(temp - tau*p[i,l])


                           n = len(a)
                           maxRot = 5*(n**2)             # Set limit on number of rotations
                           p = identity(n)*1.0           # Initialize transformation matrix
                           for i in range(maxRot): # Jacobi rotation loop
                                  aMax,k,l = maxElem(a)
                                  if aMax < tol: return diagonal(a),p
                                  rotate(a,p,k,l)
                           print ’Jacobi method did not converge’



                         sortJacobi

                      The eigenvalues/eigenvectors returned by jacobi are not ordered. The function
                      listed here can be used to sort the eigenvalues and eigenvectors into ascending or-
                      der of eigenvalues.

                      ## module sortJacobi
                      ’’’ sortJacobi(lam,x).
                           Sorts the eigenvalues {lam} and eigenvectors [x]
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                 December 16, 2009      15:4




           328      Symmetric Matrix Eigenvalue Problems

                          in order of ascending eigenvalues.
                    ’’’
                    import swap


                    def sortJacobi(lam,x):
                          n = len(lam)
                          for i in range(n-1):
                              index = i
                              val = lam[i]
                              for j in range(i+1,n):
                                    if lam[j] < val:
                                         index = j
                                         val = lam[j]
                              if index != i:
                                    swap.swapRows(lam,i,index)
                                    swap.swapCols(x,i,index)



                    Transformation to Standard Form
                    Physical problems often give rise to eigenvalue problems of the form

                                                            Ax = λBx                                 (9.21)

                    where A and B are symmetric n × n matrices. We assume that B is also positive defi-
                    nite. Such problems must be transformed into the standard form before they can be
                    solved by Jacobi diagonalization.
                         As B is symmetric and positive definite, we can apply Choleski decomposition
                    B = LLT , where L is a lower-triangular matrix (see Section 3.3). Then we introduce
                    the transformation

                                                           x = (L−1 )T z                             (9.22)

                    Substituting into Eq. (9.21), we get

                                                     A(L−1 )T z =λLLT (L−1 )T z

                    Premultiplying both sides by L−1 results in

                                                L−1 A(L−1 )T z = λL−1 LLT (L−1 )T z

                    If we note that L−1 L = LT (L−1 )T = I, the last equation reduces to the standard form

                                                             Hz = λz                                 (9.23)

                    where

                                                         H = L−1 A(L−1 )T                            (9.24)
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                               December 16, 2009      15:4




               329     9.2 Jacobi Method

                      An important property of this transformation is that it does not destroy the symmetry
                      of the matrix, that is, symmetric A results in symmetric H.
                           Here is the general procedure for solving eigenvalue problems of the form Ax =
                      λBx:

                      1. Use Choleski decomposition B = LLT to compute L.
                      2. Compute L−1 (a triangular matrix can be inverted with relatively small computa-
                         tional effort).
                      3. Compute H from Eq. (9.24).
                      4. Solve the standard eigenvalue problem Hz = λz (e.g., using the Jacobi method).
                      5. Recover the eigenvectors of the original problem from Eq. (9.22): X = (L−1 )T Z.
                         Note that the eigenvalues were untouched by the transformation.

                            An important special case is where B is a diagonal matrix:
                                                          ⎡                    ⎤
                                                            β1 0      ··· 0
                                                          ⎢0     β2 · · · 0 ⎥
                                                          ⎢                    ⎥
                                                    B=⎢   ⎢ ..  ..    ..    .. ⎥
                                                                               ⎥                                          (9.25)
                                                          ⎣.     .        . . ⎦
                                                             0   0    · · · βn

                      Here
                               ⎡   1/2                          ⎤             ⎡       −1/2                          ⎤
                                β1        0     ···       0                    β1             0      ···     0
                              ⎢ 0         1/2
                                         β2     ···             ⎥            ⎢ 0              −1/2
                                                                                             β2      ···            ⎥
                              ⎢                           0     ⎥            ⎢                               0      ⎥
                            L=⎢
                              ⎢ ..        ..    ..        ..    ⎥
                                                                ⎥     L−1
                                                                            =⎢
                                                                             ⎢ ..             ..     ..      ..     ⎥
                                                                                                                    ⎥    (9.26a)
                              ⎣ .          .       .       .    ⎦            ⎣ .               .        .     .     ⎦
                                                          1/2                                                −1/2
                                 0        0     ···      βn                      0            0      ···    βn

                      and
                                                                             A ij
                                                                    Hij =                                                (9.26b)
                                                                             βi β j


                         stdForm

                      Given the matrices A and B, the function stdForm returns H and the transformation
                      matrix T = (L−1 )T . The inversion of L is carried out by invert (the triangular shape
                      of L allows this to be done by back substitution). Note that the original A, B, and L are
                      destroyed.

                      ## module stdForm
                      ’’’ h,t = stdForm(a,b).
                             Transforms the eigenvalue problem [a]{x} = lam[b]{x}
                             to the standard form [h]{z} = lam{z}. The eigenvectors
                             are related by {x} = [t]{z}.
                      ’’’
                      from numpy import dot,inner,transpose
                      from choleski import *
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                       December 16, 2009   15:4




           330      Symmetric Matrix Eigenvalue Problems

                     def stdForm(a,b):


                         def invert(L): # Inverts lower triangular matrix L
                              n = len(L)
                              for j in range(n-1):
                                    L[j,j] = 1.0/L[j,j]
                                    for i in range(j+1,n):
                                         L[i,j] = -dot(L[i,j:i],L[j:i,j])/L[i,i]
                              L[n-1,n-1] = 1.0/L[n-1,n-1]


                         n = len(a)
                         L = choleski(b)
                         invert(L)
                         h = dot(b,inner(a,L))
                         return h,transpose(L)

                    EXAMPLE 9.1

                                             40 MPa

                                                   30 MPa              30 MPa
                                                                           80 MPa


                                                 60 MPa

                        The stress matrix (tensor) corresponding to the state of stress shown is
                                                       ⎡            ⎤
                                                         80 30    0
                                                       ⎢            ⎥
                                                  S = ⎣ 30 40     0 ⎦ MPa
                                                          0  0 60

                    (each row of the matrix consists of the three stress components acting on a coordi-
                    nate plane). It can be shown that the eigenvalues of S are the principal stresses and the
                    eigenvectors are normal to the principal planes. (1) Determine the principal stresses
                    by diagonalizing S with one Jacobi rotation and (2) compute the eigenvectors.

                    Solution of Part(1) To eliminate S12 we must apply a rotation in the 1–2 plane. With
                    k = 1 and = 2, Eq. (9.15) is
                                                         S11 − S22    80 − 40    2
                                              φ=−                  =−         =−
                                                           2S12        2(30)     3
                    Equation (9.16a) then yields
                                             sgn(φ)                    −1
                                    t=                     =                         = −0.535 18
                                         |φ| +   φ +1
                                                   2           2/3 +    (2/3)2 + 1
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                        December 16, 2009   15:4




               331     9.2 Jacobi Method

                      According to Eqs. (9.18), the changes in S due to the rotation are

                                       ∗
                                      S11 = S11 − t S12 = 80 − (−0.535 18) (30) = 96.055 MPa
                                       ∗
                                      S22 = S22 + t S12 = 40 + (−0.535 18) (30) = 23.945 MPa
                                       ∗     ∗
                                      S12 = S21 =0

                      Hence, the diagonalized stress matrix is
                                                           ⎡                     ⎤
                                                              96.055        0  0
                                                            ⎢                    ⎥
                                                       S∗ = ⎣      0   23.945  0⎦
                                                                   0        0 60

                      where the diagonal terms are the principal stresses.

                      Solution of Part (2) To compute the eigenvectors, we start with Eqs. (9.17) and (9.19),
                      which yield

                                                1                      1
                                           c= √       =                             = 0.88168
                                               1 + t2           1 + (−0.535 18)2
                                           s = tc = (−0.535 18) (0.881 68) = −0.471 86
                                                  s     −0.47186
                                           τ =       =              = −0.250 77
                                                 1+c   1 + 0.881 68

                      We obtain the changes in the transformation matrix P from Eqs. (9.20). Recalling that
                      P is initialized to the identity matrix, the first equation gives us

                                       ∗
                                      P11 = P11 − s(P12 + τ P11 )

                                           = 1 − (−0.471 86) (0 + (−0.250 77) (1)) = 0.881 67


                                       ∗
                                      P21 = P21 − s(P22 + τ P21 )

                                           = 0 − (−0.471 86) [1 + (−0.250 77) (0)] = 0.471 86

                      Similarly, the second equation of Eqs. (9.20) yields

                                                  ∗                         ∗
                                                 P12 = −0.471 86           P22 = 0.881 67

                      The third row and column of P are not affected by the transformation. Thus,
                                                          ⎡                       ⎤
                                                         0.88167       −0.47186 0
                                                       ⎢                          ⎥
                                                  P∗ = ⎣ 0.47186        0.88167 0 ⎦
                                                               0              0 1

                      The columns of P∗ are the eigenvectors of S.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                             December 16, 2009   15:4




           332      Symmetric Matrix Eigenvalue Problems

                    EXAMPLE 9.2


                                               L                    L                  2L

                                          i1        3C      i2             C      i3              C
                                                    i1                   i2                  i3


                        (1) Show that the analysis of the electric circuit shown leads to a matrix eigen-
                    value problem. (2) Determine the circular frequencies and the relative amplitudes of
                    the currents.

                    Solution of Part(1) Kirchoff’s equations for the three loops are

                                                              di1 q1 − q2
                                                                L +       =0
                                                               dt    3C
                                                     di2 q2 − q1 q2 − q3
                                                   L    +         +       =0
                                                     dt      3C      C
                                                          di3 q3 − q2 q3
                                                       2L    +       +    =0
                                                          dt      C     C

                    Differentiating and substituting dqk /dt = ik , we get

                                                          1     1        d 2i1
                                                            i1 − i2 = −LC 2
                                                          3     3        dt
                                                    1    4             d 2i2
                                                   − i1 + i2 − i3 = −LC 2
                                                    3    3             dt
                                                                                   d 2i3
                                                          −i2 + 2i3 = −2LC
                                                                                   dt 2

                    These equations admit the solution

                                                          ik (t ) = uk sin ωt

                    where ω is the circular frequency of oscillation (measured in rad/s) and uk are the
                    relative amplitudes of the currents. Substitution into Kirchoff’s equations yields Au =
                    λBu (sin ωt cancels out), where
                                    ⎡                       ⎤               ⎡            ⎤
                                      1/3 −1/3            0                   1   0    0
                                   ⎢                        ⎥               ⎢            ⎥
                               A = ⎣ −1/3  4/3           −1 ⎦           B = ⎣0    1    0⎦         λ = LCω2
                                        0  −1             2                   0   0    2

                    which represents an eigenvalue problem of the nonstandard form.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09       978 0 521 19132 6                                           December 16, 2009   15:4




               333     9.2 Jacobi Method

                      Solution of Part (2) Because B is a diagonal matrix, we can readily transform the
                      problem into the standard form Hz = λz. From Eq. (9.26a) we get
                                                          ⎡             ⎤
                                                            1 0     0
                                                          ⎢             ⎥
                                                    L−1 = ⎣ 0 1     0 ⎦
                                                                    √
                                                            0 0 1/ 2

                      and Eq. (9.26b) yields
                                                              ⎡                         ⎤
                                                             1/3            −1/3      0
                                                          ⎢                          √ ⎥
                                                      H = ⎣ −1/3              4/3 −1/ 2 ⎦
                                                                              √
                                                               0           −1/ 2      1

                      The eigenvalues and eigenvectors of H can now be obtained with the Jacobi method.
                      Skipping the details, the results are

                                            λ1 = 0.147 79          λ2 = 0.582 35       λ3 = 1.936 53
                                        ⎡         ⎤                   ⎡        ⎤              ⎡            ⎤
                                         0.810 27                     0.562 74                    0.163 70
                                       ⎢          ⎥                ⎢           ⎥               ⎢           ⎥
                                  z1 = ⎣ 0.451 02 ⎦           z2 = ⎣ −0.420 40 ⎦          z3 = ⎣ −0.787 30 ⎦
                                         0.374 23                    −0.711 76                    0.594 44

                      The eigenvectors of the original problem are recovered from Eq. (9.22): yi = (L−1 )T zi ,
                      which yields
                                    ⎡          ⎤           ⎡           ⎤          ⎡             ⎤
                                      0.810 27                0.562 74                0.163 70
                                    ⎢          ⎥           ⎢           ⎥          ⎢             ⎥
                               u1 = ⎣ 0.451 02 ⎦      u2 = ⎣ −0.420 40 ⎦     u3 = ⎣ −0.787 30 ⎦
                                      0.264 62               −0.503 29                0.420 33

                      These vectors should now be normalized (each zi was normalized, but the transfor-
                      mation to ui does not preserve the magnitudes of vectors). The circular frequencies
                      are ωi = λi / (LC), so that
                                                  0.3844                0.7631              1.3916
                                             ω1 = √                ω2 = √              ω3 = √
                                                     LC                    LC                  LC
                      EXAMPLE 9.3

                                                                                             n +1
                                       -1       0         1       2             n -1 n                n+2
                                  P                                                                           x
                                                                           L
                          The propped cantilever beam carries a compressive axial load P. The lateral dis-
                      placement u(x) of the beam can be shown to satisfy the differential equation
                                                                           P
                                                                  u(4) +      u =0                                    (a)
                                                                           EI
                      where E I is the bending rigidity. The boundary conditions are

                                                    u(0) = u (0) = 0           u(L) = u (L) = 0                       (b)
P1: PHB

CUUS884-Kiusalaas     CUUS884-09     978 0 521 19132 6                                      December 16, 2009         15:4




           334      Symmetric Matrix Eigenvalue Problems

                    (1) Show that buckling analysis of the beam results in a matrix eigenvalue problem if
                    the derivatives are approximated by finite differences. (2) Use the Jacobi method to
                    compute the lowest three buckling loads and the corresponding eigenvectors.


                    Solution of Part (1) We divide the beam into n + 1 segments of length L/(n + 1) each
                    as shown. Replacing the derivatives of u in Eq. (a) by central finite differences of O(h2 )
                    at the interior nodes (nodes 1 to n), we obtain

                                              ui−2 − 4ui−1 + 6ui − 4ui+1 + ui+2
                                                             h4
                                               P −ui−1 + 2ui − ui−1
                                            =                       , i = 1, 2, . . . , n
                                              EI          h2

                    After multiplication by h4 , the equations become


                                         u−1 − 4u0 + 6u1 − 4u2 + u3 = λ(−u0 + 2u1 − u2 )

                                          u0 − 4u1 + 6u2 − 4u3 + u4 = λ(−u1 + 2u2 − u3 )
                                                                          ..
                                                                           .                                    (c)

                                 un−3 − 4un−2 + 6un−1 − 4un + un+1 = λ(−un−2 + 2un−1 − un )

                                 un−2 − 4un−1 + 6un − 4un+1 + un+2 = λ(−un−1 + 2un − un+1 )


                    where

                                                          Ph2      P L2
                                                     λ=       =
                                                          EI    (n + 1)2 E I

                    The displacements u−1 , u0 , un+1 , and un+2 can be eliminated by using the prescribed
                    boundary conditions. Referring to Table 8.1, the finite difference approximations to
                    the boundary conditions are


                                       u0 = 0       u−1 = −u1      un+1 = 0       un+2 = un


                    Substitution into Eqs. (c) yields the matrix eigenvalue problem Ax = λBx, where

                                                ⎡                                    ⎤
                                                 5 −4       1     0     0 ···    0
                                              ⎢ −4    6 −4              0 ···    0⎥
                                              ⎢                   1                  ⎥
                                              ⎢                                      ⎥
                                              ⎢ 1 −4        6 −4        1 ···    0⎥
                                              ⎢                                      ⎥
                                              ⎢ .                                 .. ⎥
                                          A = ⎢ .. . . . . . . . . . . . . . . .   . ⎥
                                              ⎢                                      ⎥
                                              ⎢                                      ⎥
                                              ⎢ 0 ···       1 −4        6 −4     1⎥
                                              ⎢                                      ⎥
                                              ⎣ 0 ···       0     1 −4        6 −4 ⎦
                                                 0 ···      0     0     1 −4     7
P1: PHB

CUUS884-Kiusalaas    CUUS884-09   978 0 521 19132 6                                  December 16, 2009   15:4




               335     9.2 Jacobi Method

                                                ⎡                                      ⎤
                                                  2     −1       0    0    0 ···   0
                                               ⎢ −1        2 −1            0 ···   0⎥
                                               ⎢                      0                ⎥
                                               ⎢                                       ⎥
                                               ⎢ 0      −1       2 −1      0 ···   0⎥
                                               ⎢                                       ⎥
                                               ⎢ .     . . .. . .   ..   ..   ..    .. ⎥
                                           B = ⎢ ..       ..      .    .    .    .   .⎥
                                               ⎢                                       ⎥
                                               ⎢                                       ⎥
                                               ⎢ 0      ···      0 −1      2 −1    0⎥
                                               ⎢                                       ⎥
                                               ⎣ 0      ···      0    0 −1      2 −1 ⎦
                                                  0     ···      0    0    0 −1    2

                      Solution of Part (2) The problem with the Jacobi method is that it insists on finding
                      all the eigenvalues and eigenvectors. It is also incapable of exploiting banded struc-
                      tures of matrices. Thus, the program listed below does much more work than neces-
                      sary for the problem at hand. More efficient methods of solution will be introduced
                      later in this chapter.

                      #!/usr/bin/python
                      ## example9_3
                      from numpy import array,zeros,dot
                      from stdForm import *
                      from jacobi import *
                      from sortJacobi import *


                      n = 10
                      a = zeros((n,n))
                      b = zeros((n,n))
                      for i in range(n):
                           a[i,i] = 6.0
                           b[i,i] = 2.0
                      a[0,0] = 5.0
                      a[n-1,n-1] = 7.0
                      for i in range(n-1):
                           a[i,i+1] = -4.0
                           a[i+1,i] = -4.0
                           b[i,i+1] = -1.0
                           b[i+1,i] = -1.0
                      for i in range(n-2):
                           a[i,i+2] = 1.0
                           a[i+2,i] = 1.0


                      h,t = stdForm(a,b)              # Convert to std. form
                      lam,z = jacobi(h)               # Solve by Jacobi mthd.
                      x = dot(t,z)                    # Eigenvectors of orig. prob. [x] = [t][z]
                      for i in range(n):              # Normalize eigenvectors
                           xMag = sqrt(dot(x[:,i],x[:,i]))
                           x[:,i] = x[:,i]/xMag
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                              December 16, 2009     15:4




           336      Symmetric Matrix Eigenvalue Problems

                    sortJacobi(lam,x)                 # Arrange in ascending order
                    print "Eigenvalues:\n",lam[0:3]
                    print "\nEigenvectors:\n",x[:,0:3]
                    raw_input("\n Press return to exit")



                        Running the program with n = 10 resulted in the following output:


                    Eigenvalues:
                    [ 0.16410379        0.47195675      0.90220118]


                    Eigenvectors:
                    [[ 0.16410119 -0.18476623            0.30699491]
                     [ 0.30618978 -0.26819121            0.36404289]
                     [ 0.40786549 -0.19676237            0.14669942]
                     [ 0.45735999        0.00994855 -0.12192373]
                     [ 0.45146805        0.26852252 -0.1724502 ]
                     [ 0.39607358        0.4710634       0.06772929]
                     [ 0.30518404        0.53612023      0.40894875]
                     [ 0.19863178        0.44712859      0.57038382]
                     [ 0.09881943        0.26022826      0.43341183]
                     [ 0.0270436         0.07776771      0.1486333 ]]



                        The first three mode shapes, which represent the relative displacements of the
                    bucked beam, are plotted here (we appended the zero end displacements to the
                    eigenvectors before plotting the points).



                           0.6
                                                            1
                           0.4


                           0.2
                      u




                           0.0

                                                                  3
                          -0.2
                                                  2
                          -0.4
                                  0.0         2.0           4.0         6.0       8.0         10.0
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                              December 16, 2009     15:4




               337     9.3 Power and Inverse Power Methods

                      The buckling loads are given by Pi = (n + 1)2 λi E I /L 2 . Thus,

                                                       (11)2 (0.164 103 7) E I         EI
                                             P1 =                              = 19.857 2
                                                                 L2                    L
                                                       (11)2 (0.471 956 75) E I         EI
                                             P2 =                   2
                                                                                = 57.107 2
                                                                  L                     L
                                                       (11)2 (0.902 201 18) E I         EI
                                             P3 =                   2
                                                                                = 109.17 2
                                                                  L                     L

                      The analytical values are P1 = 20.19E I /L 2 , P2 = 59.68E I /L 2 , and P3 = 118.9E I /L 2 .
                      It can be seen that the error introduced by the finite difference approximation in-
                      creases with the mode number (the error in Pi+1 is larger than in Pi ). Of course, the
                      accuracy of the finite difference model can be improved by using larger n, but beyond
                      n = 20 the cost of computation with the Jacobi method becomes rather high.



              9.3     Power and Inverse Power Methods
                      Inverse Power Method
                      The inverse power method is a simple and efficient algorithm that finds the smallest
                      eigenvalue λ1 and the corresponding eigenvector x1 of

                                                                     Ax = λx                                          (9.27)

                      The method works like this:

                      1. Let v be an approximation to x1 (a random vector of unit magnitude will do).
                      2. Solve

                                                                           Az = v                                     (9.28)

                         for the vector z.
                      3. Compute |z|.
                      4. Let v = z/|z| and repeat steps 2–4 until the change in v is negligible.

                          At the conclusion of the procedure, |z| = ±1/λ1 and v = x1 . The sign of λ1 is de-
                      termined as follows: If z changes sign between successive iterations, λ1 is negative;
                      otherwise, use the plus sign.
                          Let us now investigate why the method works. Because the eigenvectors xi of Eq.
                      (9.27) are orthonormal (linearly independent), they can be used as the basis for any
                      n-dimensional vector. Thus, v and z admit the unique representations

                                                             n                      n
                                                        v=         vi xi      z=          zi xi                          (a)
                                                             i=1                    i=1
P1: PHB

CUUS884-Kiusalaas     CUUS884-09       978 0 521 19132 6                                         December 16, 2009      15:4




           338      Symmetric Matrix Eigenvalue Problems

                    where vi and zi are the components of v and z with respect to the eigenvectors xi .
                    Substitution into Eq. (9.28) yields
                                                                n               n
                                                           A         zi xi −         vi xi = 0
                                                               i=1             i=1

                    But Axi = λi xi , so that
                                                               n
                                                                     (zi λi − vi ) xi = 0
                                                               i=1

                    Hence,
                                                                                vi
                                                                        zi =
                                                                                λi
                    It follows from Eq. (a) that
                                                     n                     n
                                                           vi      1                 λ1
                                                z=            xi =              vi      xi                     (9.29)
                                                           λi      λ1                λi
                                                     i=1                  i=1

                                                     1              λ1       λ1
                                                 =      v 1 x1 + v 2 x2 + v 3 x3 + · · ·
                                                     λ1             λ2       λ3
                    Because λ1 /λi < 1 (i = 1), we observe that the coefficient of x1 has become more
                    prominent in z than it was in v; hence, z is a better approximation to x1 . This com-
                    pletes the first iterative cycle.
                        In subsequent cycles, we set v = z/|z| and repeat the process. Each iteration will
                    increase the dominance of the first term in Eq. (9.29) so that the process converges to
                                                                      1           1
                                                               z=        v 1 x1 =    x1
                                                                      λ1          λ1
                    (at this stage v1 = 1 because v = x1 , so that v1 = 1, v2 = v3 = · · · = 0).
                         The inverse power method also works with the nonstandard eigenvalue problem

                                                                      Ax = λBx                                 (9.30)

                    provided that Eq. (9.28) is replaced by

                                                                       Az = Bv                                 (9.31)

                    The alternative is, of course, to transform the problem to standard form before ap-
                    plying the power method.


                    Eigenvalue Shifting
                    By inspection of Eq. (9.29) we see that the speed of convergence is determined by
                    the strength of the inequality|λ1 /λ2 | < 1 (the second term in the equation). If |λ2 |
                    is well separated from |λ1 |, the inequality is strong and the convergence is rapid.
                    On the other hand, close proximity of these two eigenvalues results in very slow
                    convergence.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                  December 16, 2009     15:4




               339     9.3 Power and Inverse Power Methods

                           The rate of convergence can be improved by a technique called eigenvalue shift-
                      ing. Letting
                                                             λ = λ∗ + s                                   (9.32)
                      where s is a predetermined “shift,” the eigenvalue problem in Eq. (9.27) is trans-
                      formed to
                                                           Ax = (λ∗ + s)x
                      or
                                                             A∗ x = λ ∗ x                                 (9.33)
                      where
                                                            A∗ = A − sI                                   (9.34)
                      Solving the transformed problem in Eq. (9.33) by the inverse power method yields λ∗1
                      and x1 , where λ∗1 is the smallest eigenvalue of A∗ . The corresponding eigenvalue of
                      the original problem, λ = λ∗1 + s, is thus the eigenvalue closest to s.
                           Eigenvalue shifting has two applications. An obvious one is the determination
                      of the eigenvalue closest to a certain value s. For example, if the working speed of a
                      shaft is s rpm, it is imperative to ensure that there are no natural frequencies (which
                      are related to the eigenvalues) close to that speed.
                           Eigenvalue shifting is also used to speed up convergence. Suppose that we are
                      computing the smallest eigenvalue λ1 of the matrix A. The idea is to introduce a shift
                      s that makes λ∗1 /λ∗2 as small as possible. Because λ∗1 = λ1 − s, we should choose s ≈ λ1
                      (s = λ1 should be avoided to prevent division by zero). Of course, this method works
                      only if we have a prior estimate of λ1 .
                           The inverse power method with eigenvalue shifting is a particularly powerful tool
                      for finding eigenvectors if the eigenvalues are known. By shifting very close to an
                      eigenvalue, the corresponding eigenvector can be computed in one or two iterations.


                      Power Method
                      The power method converges to the eigenvalue furthest from zero and the associated
                      eigenvector. It is very similar to the inverse power method; the only difference be-
                      tween the two methods is the interchange of v and z in Eq. (9.28). The outline of the
                      procedure is:

                      1. Let v be an approximation to x1 (a random vector of unit magnitude will do).
                      2. Compute the vector
                                                                z = Av                                    (9.35)
                      3. Compute |z|.
                      4. Let v = z/|z| and repeat steps 2–4 until the change in v is negligible.

                         At the conclusion of the procedure, |z| = ±λn and v = xn (the sign of λn is deter-
                      mined in the same way as in the inverse power method).
P1: PHB

CUUS884-Kiusalaas    CUUS884-09       978 0 521 19132 6                                 December 16, 2009       15:4




           340      Symmetric Matrix Eigenvalue Problems

                      inversePower

                    Given the matrix A and the shift s, the function inversePower returns the eigenvalue
                    of A closest to s and the corresponding eigenvector. The matrix A∗ = A − sI is decom-
                    posed as soon as it is formed, so that only the solution phase (forward and back sub-
                    stitution) is needed in the iterative loop. If A is banded, the efficiency of the program
                    could be improved by replacing LUdecomp and LUsolve by functions that specialize
                    in banded matrices (e.g., LUdecomp5 and LUsolve5) – see Example 9.6. The program
                    line that forms A∗ must also be modified to be compatible with the storage scheme
                    used for A.



                    ## module inversePower
                    ’’’ lam,x = inversePower(a,s,tol=1.0e-6).
                          Inverse power method for solving the eigenvalue problem
                          [a]{x} = lam{x}. Returns ’lam’ closest to ’s’ and the
                          corresponding eigenvector {x}.
                    ’’’
                    from numpy import zeros,dot,identity
                    from LUdecomp import *
                    from math import sqrt
                    from random import random


                    def inversePower(a,s,tol=1.0e-6):
                          n = len(a)
                          aStar = a - identity(n)*s           # Form [a*] = [a] - s[I]
                          aStar = LUdecomp(aStar)             # Decompose [a*]
                          x = zeros(n)
                          for i in range(n):                  # Seed [x] with random numbers
                              x[i] = random()
                          xMag = sqrt(dot(x,x))               # Normalize [x]
                          x =x/xMag
                          for i in range(50):                 # Begin iterations
                              xOld = x.copy()                 # Save current [x]
                              x = LUsolve(aStar,x)            # Solve [a*][x] = [xOld]
                              xMag = sqrt(dot(x,x))           # Normalize [x]
                              x = x/xMag
                              if dot(xOld,x) < 0.0:           # Detect change in sign of [x]
                                    sign = -1.0
                                    x = -x
                              else: sign = 1.0
                              if sqrt(dot(xOld - x,xOld - x)) < tol:
                                    return s + sign/xMag,x
                          print ’Inverse power method did not converge’
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                     December 16, 2009   15:4




               341     9.3 Power and Inverse Power Methods

                      EXAMPLE 9.4
                      The stress matrix describing the state of stress at a point is
                                                      ⎡                     ⎤
                                                        −30      10     20
                                                      ⎢                     ⎥
                                                 S = ⎣ 10        40 −50 ⎦ MPa
                                                          20 −50 −10
                      Determine the largest principal stress (the eigenvalue of S furthest from zero) by the
                      power method.

                      Solution

                      First iteration:
                                          T
                      Let v = 1 0 0         be the initial guess for the eigenvector. Then,
                                                   ⎡                     ⎤⎡ ⎤ ⎡             ⎤
                                                     −30      10      20     1      −30.0
                                                   ⎢                     ⎥⎢ ⎥ ⎢             ⎥
                                          z = Sv = ⎣ 10       40 −50 ⎦ ⎣ 0 ⎦ = ⎣ 10.0 ⎦
                                                        20 −50 −10           0         20.0

                                                      |z| =   302 + 102 + 202 = 37.417
                                                          ⎡    ⎤          ⎡           ⎤
                                                         −30.0              −0.801 77
                                                  z    ⎢       ⎥ 1        ⎢           ⎥
                                              v=     = ⎣ 10.0 ⎦         = ⎣ 0.267 26 ⎦
                                                 |z|             37.417
                                                          20.0               0.534 52

                      Second iteration:
                                                ⎡                 ⎤⎡           ⎤ ⎡           ⎤
                                              −30          10  20    −0.801 77        37.416
                                            ⎢                     ⎥⎢           ⎥ ⎢           ⎥
                                   z = Sv = ⎣ 10           40 −50 ⎦ ⎣ 0.267 26 ⎦ = ⎣ −24.053 ⎦
                                               20         −50 −10     0.534 52       −34.744

                                              |z| =   37.4162 + 24.0532 + 34.7442 = 56. 442

                                                         ⎡    ⎤           ⎡          ⎤
                                                       37.416                0.66291
                                               z    ⎢         ⎥    1      ⎢          ⎥
                                           v=     = ⎣ −24.053 ⎦         = ⎣ −0.42615 ⎦
                                              |z|               56. 442
                                                      −34.744               −0.61557

                      Third iteration:
                                                ⎡                    ⎤⎡           ⎤ ⎡          ⎤
                                              −30          10     20      0.66291      −36.460
                                            ⎢                        ⎥⎢           ⎥ ⎢          ⎥
                                   z = Sv = ⎣ 10           40    −50 ⎦ ⎣ −0.42615 ⎦ = ⎣ 20.362 ⎦
                                               20         −50    −10     −0.61557       40.721


                                              |z| =    36.4602 + 20.3622 + 40.7212 = 58.328
                                                    ⎡         ⎤          ⎡          ⎤
                                                      −36.460              −0.62509
                                               z    ⎢         ⎥    1     ⎢          ⎥
                                           v=     = ⎣ 20.362 ⎦         = ⎣ 0.34909 ⎦
                                              |z|               58.328
                                                       40.721               0.69814
P1: PHB

CUUS884-Kiusalaas     CUUS884-09     978 0 521 19132 6                                   December 16, 2009       15:4




           342      Symmetric Matrix Eigenvalue Problems

                    At this point the approximation of the eigenvalue we seek is λ = −58.328 MPa (the
                    negative sign is determined by the sign reversal of z between iterations). This is actu-
                    ally close to the second-largest eigenvalue λ2 = −58.39 MPa. By continuing the itera-
                    tive process we would eventually end up with the largest eigenvalue λ3 = 70.94 MPa.
                    But since |λ2 | and |λ3 | are rather close, the convergence is too slow from this point on
                    for manual labor. Here is a program that does the calculations for us:

                    #!/usr/bin/python
                    ## example9_4
                    from numpy import array,dot
                    from math import sqrt


                    s = array([[-30.0,         10.0,     20.0], \
                                   [ 10.0,     40.0, -50.0], \
                                   [ 20.0, -50.0, -10.0]])
                    v = array([1.0, 0.0, 0.0])
                    for i in range(100):
                         vOld = v.copy()
                         z = dot(s,v)
                         zMag = sqrt(dot(z,z))
                         v = z/zMag
                         if dot(vOld,v) < 0.0:
                              sign = -1.0
                              v = -v
                         else: sign = 1.0
                         if sqrt(dot(vOld - v,vOld - v)) < 1.0e-6: break
                    lam = sign*zMag
                    print "Number of iterations =",i
                    print "Eigenvalue =",lam
                    raw_input("Press return to exit")

                        The results are:

                    Number of iterations = 92
                    Eigenvalue = 70.9434833068

                        Note that it took 92 iterations to reach convergence.

                    EXAMPLE 9.5
                    Determine the smallest eigenvalue λ1 and the corresponding eigenvector of
                                                   ⎡                     ⎤
                                                     11 2     3    1   4
                                                   ⎢ 2 9      3    5   2⎥
                                                   ⎢                     ⎥
                                                   ⎢                     ⎥
                                               A = ⎢ 3 3 15        4   3⎥
                                                   ⎢                     ⎥
                                                   ⎣ 1 5      4 12     4⎦
                                                          4   2   3    4   17
                    Use the inverse power method with eigenvalue shifting, knowing that λ1 ≈ 5.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09   978 0 521 19132 6                                 December 16, 2009   15:4




               343     9.3 Power and Inverse Power Methods

                      Solution

                      #!/usr/bin/python
                      ## example9_5
                      from numpy import array
                      from inversePower import *


                      s = 5.0
                      a = array([[ 11.0, 2.0,         3.0,    1.0,   4.0],     \
                                    [   2.0, 9.0,     3.0,    5.0,   2.0],    \
                                    [   3.0, 3.0, 15.0,       4.0,   3.0],    \
                                    [   1.0, 5.0,     4.0, 12.0,     4.0],    \
                                    [   4.0, 2.0,     3.0,    4.0, 17.0]])
                      lam,x = inversePower(a,s)
                      print "Eigenvalue =",lam
                      print "\nEigenvector:\n",x
                      raw_input("\nPrint press return to exit")

                          Here is the output:

                      Eigenvalue = 4.87394637865


                      Eigenvector:
                      [-0.26726603      0.74142854     0.05017271 -0.59491453         0.14970633]

                           Convergence was achieved with four iterations. Without the eigenvalue shift, 26
                      iterations would be required.

                      EXAMPLE 9.6
                      Unlike Jacobi diagonalization, the inverse power method lends itself to eigenvalue
                      problems of banded matrices. Write a program that computes the smallest buckling
                      load of the beam described in Example 9.3, making full use of the banded forms. Run
                      the program with 100 interior nodes (n = 100).

                      Solution The function inversePower5 listed here returns the smallest eigenvalue
                      and the corresponding eigenvector of Ax = λBx, where A is a pentadiagonal ma-
                      trix and B is a sparse matrix (in this problem it is tridiagonal). The matrix A is in-
                      put by its diagonals d, e, and f as was done in Section 2.4 in conjunction with the
                      LU decomposition. The algorithm for inversePower5 does not use B directly, but
                      calls the function Bv(v) that supplies the product Bv. Eigenvalue shifting is not
                      used.

                      ## module inversePower5
                      ’’’ lam,x = inversePower5(Bv,d,e,f,tol=1.0e-6).
                           Inverse power method for solving the eigenvalue problem
                           [A]{x} = lam[B]{x}, where [A] = [f\e\d\e\f] is
                           pentadiagonal and [B] is sparse.. User must supply the
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                            December 16, 2009   15:4




           344      Symmetric Matrix Eigenvalue Problems

                          function Bv(v) that returns the vector [B]{v}.
                    ’’’
                    from numpy import zeros,dot
                    from LUdecomp5 import *
                    from math import sqrt
                    from random import random


                    def inversePower5(Bv,d,e,f,tol=1.0e-6):
                          n = len(d)
                          d,e,f = LUdecomp5(d,e,f)
                          x = zeros(n)
                          for i in range(n):                 # Seed {v} with random numbers
                               x[i] = random()
                          xMag = sqrt(dot(x,x))              # Normalize {v}
                          x = x/xMag
                          for i in range(30):                # Begin iterations
                               xOld = x.copy()               # Save current {v}
                               x = Bv(xOld)                  # Compute [B]{v}
                               x = LUsolve5(d,e,f,x)         # Solve [A]{z} = [B]{v}
                               xMag = sqrt(dot(x,x))         # Normalize {z}
                               x = x/xMag
                               if dot(xOld,x) < 0.0:        # Detect change in sign of {x}
                                    sign = -1.0
                                    x = -x
                               else: sign = 1.0
                               if sqrt(dot(xOld - x,xOld - x)) < tol:
                                    return sign/xMag,x
                          print ’Inverse power method did not converge’

                          The program that utilizes inversePower5 is

                    #!/usr/bin/python
                    ## example9_6
                    from numpy import ones,zeros
                    from inversePower5 import *


                    def Bv(v):                       # Compute {z} = [B]{v}
                             n = len(v)
                             z = zeros(n)
                             z[0] = 2.0*v[0] - v[1]
                             for i in range(1,n-1):
                                 z[i] = -v[i-1] + 2.0*v[i] - v[i+1]
                             z[n-1] = -v[n-2] + 2.0*v[n-1]
                             return z
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                         December 16, 2009   15:4




               345     9.3 Power and Inverse Power Methods

                      n = 100                             # Number of interior nodes
                      d = ones(n*6.0                # Specify diagonals of [A] = [f\e\d\e\f]
                      d[0] = 5.0
                      d[n-1] = 7.0
                      e = ones(n-1)*(-4.0)
                      f = ones(n-2)*1.0
                      lam,x = inversePower5(Bv,d,e,f)
                      print "PLˆ2/EI =",lam*(n+1)**2
                      raw_input("\nPress return to exit")

                            The output is in excellent agreement with the analytical value:

                      PLˆ2/EI = 20.1867355603

                      PROBLEM SET 9.1
                       1. Given
                                                         ⎡               ⎤          ⎡          ⎤
                                                        7      3       1            4    0   0
                                                      ⎢                  ⎥        ⎢            ⎥
                                                  A = ⎣3       9       6⎦     B = ⎣0     9   0⎦
                                                        1      6       8            0    0   4

                          convert the eigenvalue problem Ax = λBx to the standard form Hz = λz. What is
                          the relationship between x and z?
                       2. Convert the eigenvalue problem Ax = λBx, where
                                              ⎡               ⎤        ⎡               ⎤
                                                  4 −1      0              2 −1      0
                                              ⎢               ⎥        ⎢               ⎥
                                          A = ⎣ −1     4 −1 ⎦     B = ⎣ −1     2 −1 ⎦
                                                  0 −1      4              0 −1      1

                          to the standard form.
                       3. An eigenvalue of the problem in Prob. 2 is roughly 2.5. Use the inverse power
                          method with eigenvalue shifting to compute this eigenvalue to four decimal
                                                                   T
                          places. Start with x = 1 0 0 . Hint: two iterations should be sufficient.
                       4. The stress matrix at a point is
                                                          ⎡            ⎤
                                                            150 −60  0
                                                          ⎢            ⎥
                                                     S = ⎣ −60 120   0 ⎦ MPa
                                                              0   0 80

                            Compute the principal stresses (eigenvalues of S).
                       5.




                                                        θ1 L                 θ2 L
                                                                         k
                                                        m                               2m
P1: PHB

CUUS884-Kiusalaas    CUUS884-09       978 0 521 19132 6                                   December 16, 2009   15:4




           346      Symmetric Matrix Eigenvalue Problems

                         The two pendulums are connected by a spring that is undeformed when the pen-
                         dulums are vertical. The equations of motion of the system can be shown to be
                                                      k L(θ 2 − θ 1 ) − mgθ 1 = mL θ¨ 1

                                                   −k L(θ 2 − θ 1 ) − 2mgθ 2 = 2mL θ¨ 2

                         where θ 1 and θ 2 are the angular displacements and k is the spring stiffness.
                         Determine the circular frequencies of vibration and the relative amplitudes of
                         the angular displacements. Use m = 0.25 kg, k = 20 N/m, L = 0.75 m, and g =
                         9.80665 m/s2 .
                    6.
                                                               L                 L


                                                          i1                i2       i2
                                                                        C
                                              C

                                                                   i1       i3       i3
                                                                        C

                                                                                 L
                         Kirchoff’s laws for the electric circuit are
                                                                                 d 2i1
                                                          3i1 − i2 − i3 = −LC
                                                                                 dt 2
                                                                                 d 2i2
                                                               −i1 + i2 = −LC
                                                                                 dt 2
                                                                                 d 2i3
                                                               −i1 + i3 = −LC
                                                                                 dt 2
                       Compute the circular frequencies of the circuit and the relative amplitudes of the
                       loop currents.
                    7. Compute the matrix A∗ that results from annihilation A 14 and A 41 in the matrix
                                                      ⎡                    ⎤
                                                          4 −1        0 1
                                                      ⎢ −1      6 −2 0 ⎥
                                                      ⎢                    ⎥
                                                  A=⎢                      ⎥
                                                      ⎣ 0 −2          3 2⎦
                                                          1     0     2 4
                         by a Jacobi rotation.
                    8.     Use the Jacobi method to determine the eigenvalues and eigenvectors of
                                                          ⎡            ⎤
                                                             4 −1 2
                                                          ⎢            ⎥
                                                      A = ⎣ −1     3 3⎦
                                                            −2     3 1
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                   December 16, 2009   15:4




               347     9.3 Power and Inverse Power Methods

                       9.     Find the eigenvalues and eigenvectors of
                                                          ⎡                 ⎤
                                                             4 −2      1 −1
                                                          ⎢ −2    4 −2    1⎥
                                                          ⎢                 ⎥
                                                      A=⎢                   ⎥
                                                          ⎣ 1 −2       4 −2 ⎦
                                                            −1    1 −2    4

                          with the Jacobi method.
                      10.   Use the power method to compute the largest eigenvalue and the correspond-
                          ing eigenvector of the matrix A given in Prob. 9.
                      11.   Find the smallest eigenvalue and the corresponding eigenvector of the matrix
                          A in Prob. 9. Use the inverse power method.
                      12.   Let
                                             ⎡              ⎤          ⎡                  ⎤
                                               1.4 0.8 0.4                  0.4 −0.1  0.0
                                             ⎢              ⎥          ⎢                  ⎥
                                        A = ⎣ 0.8 6.6 0.8 ⎦        B = ⎣ −0.1    0.4 −0.1 ⎦
                                               0.4 0.8 5.0                  0.0 −0.1  0.4

                            Find the eigenvalues and eigenvectors of Ax = λBx by the Jacobi method.
                      13.     Use the inverse power method to compute the smallest eigenvalue in Prob. 12.
                      14.     Use the Jacobi method to compute the eigenvalues and eigenvectors of the
                            matrix
                                                        ⎡                        ⎤
                                                          11 2     3    1   4 2
                                                        ⎢                        ⎥
                                                        ⎢ 2 9      3    5   2 1⎥
                                                        ⎢                        ⎥
                                                        ⎢ 3 3 15        4   3 2⎥
                                                    A=⎢ ⎢ 1 5
                                                                                 ⎥
                                                        ⎢          4 12     4 3⎥ ⎥
                                                        ⎢                        ⎥
                                                        ⎣ 4 2      3    4 17 5 ⎦
                                                           2 1     2    3   5 8

                      15.     Find the eigenvalues of Ax = λBx by the Jacobi method, where
                                           ⎡                   ⎤          ⎡                  ⎤
                                              6 −4       1   0                1 −2     3 −1
                                           ⎢ −4     6 −4     1⎥           ⎢ −2    6 −2     3⎥
                                           ⎢                   ⎥          ⎢                  ⎥
                                       A=⎢                     ⎥      B=⎢                    ⎥
                                           ⎣ 1 −4        6 −4 ⎦           ⎣ 3 −2       6 −2 ⎦
                                              0     1 −4     7              −1    3 −2     9

                            Warning: B is not positive definite.
                      16.
                                             u
                                                                   L
                                                                                          x
                                                  1 2                                n

                            The figure shows a cantilever beam with a superimposed finite difference mesh.
                            If u(x, t ) is the lateral displacement of the beam, the differential equation of mo-
                            tion governing bending vibrations is
                                                                          γ
                                                               u(4) = −      u¨
                                                                          EI
P1: PHB

CUUS884-Kiusalaas    CUUS884-09        978 0 521 19132 6                                       December 16, 2009      15:4




           348      Symmetric Matrix Eigenvalue Problems

                          where γ is the mass per unit length and E I is the bending rigidity. The bound-
                          ary conditions are u(0, t ) = u (0, t ) = u (L, t ) = u (L, t ) = 0. With u(x, t ) = y(x)
                          sin ωt the problem becomes

                                                     ω2 γ
                                           y (4) =        y    y(0) = y (0) = y (L) = y (L) = 0
                                                     EI

                          The corresponding finite difference equations are

                                       ⎡                                      ⎤⎡     ⎤    ⎡       ⎤
                                          7 −4       1     0     0 ···    0       y1         y1
                                       ⎢ −4    6 −4              0 ···    0⎥    ⎢    ⎥    ⎢ y ⎥
                                       ⎢                   1                  ⎥ ⎢ y2 ⎥    ⎢ 2 ⎥
                                       ⎢                                      ⎥⎢     ⎥    ⎢       ⎥
                                       ⎢ 1 −4        6 −4        1 ···    0 ⎥ ⎢ y3 ⎥      ⎢ y3 ⎥
                                       ⎢                                      ⎥⎢     ⎥    ⎢       ⎥
                                       ⎢ .                                 .. ⎥ ⎢ .. ⎥    ⎢ . ⎥
                                   A = ⎢ .. . . . . . . . . . . . . . . .   . ⎥ ⎢ . ⎥ = λ ⎢ .. ⎥
                                       ⎢                                      ⎥⎢     ⎥    ⎢       ⎥
                                       ⎢                                      ⎥⎢     ⎥    ⎢       ⎥
                                       ⎢ 0 ···       1 −4        6 −4     1 ⎥ ⎢ yn−2 ⎥    ⎢ yn−2 ⎥
                                       ⎢                                      ⎥⎢     ⎥    ⎢       ⎥
                                       ⎣ 0 ···       0     1 −4        5 −2 ⎦ ⎣ yn−1 ⎦    ⎣ yn−1 ⎦
                                          0 ···      0     0     1 −2     1       yn        yn /2

                          where

                                                                                    4
                                                                      ω2 γ    L
                                                               λ=
                                                                      EI      n

                          (a) Write down the matrix H of the standard form Hz = λz and the transforma-
                          tion matrix P as in y = Pz. (b) Write a program that computes the lowest two
                          circular frequencies of the beam and the corresponding mode shapes (eigenvec-
                          tors) using the Jacobi method. Run the program with n = 10. Note: the analytical
                                                                                       √
                          solution for the lowest circular frequency is ω1 = 3.515/L 2   E I /γ .
                    17.

                                       P             L /4            L /2               L /4    P
                                                     EI 0            2EI 0              EI 0
                                                                      (a)
                                                              L /4           L /4

                                                         0 1 2 3 4 5 6 7 8 9 10
                                                                     (b)

                          The simply supported column in Fig. (a) consists of three segments with the
                          bending rigidities shown. If only the first buckling mode is of interest, it is suf-
                          ficient to model half of the beam as shown in Fig. (b). The differential equation
                          for the lateral displacement u(x) is

                                                                             P
                                                                 u =−           u
                                                                             EI
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                            December 16, 2009   15:4




               349     9.3 Power and Inverse Power Methods

                            with the boundary conditions u(0) = u (0) = 0. The corresponding finite differ-
                            ence equations are
                                 ⎡                                              ⎤⎡      ⎤  ⎡         ⎤
                                    2 −1       0   0   0      0    0 ···    0       u1         u1
                                 ⎢                                              ⎥⎢      ⎥  ⎢         ⎥
                                 ⎢ −1    2 −1      0   0      0    0 ···    0 ⎥ ⎢ u2 ⎥     ⎢ u2 ⎥
                                 ⎢                                              ⎥⎢      ⎥  ⎢         ⎥
                                 ⎢ 0 −1        2 −1    0      0    0 ···    0⎥    ⎢     ⎥  ⎢ u3 ⎥
                                 ⎢                                              ⎥ ⎢ u3 ⎥   ⎢         ⎥
                                 ⎢ 0     0 −1      2 −1            0 ···    0⎥    ⎢     ⎥  ⎢ u ⎥
                                 ⎢                            0                 ⎥ ⎢ u4 ⎥   ⎢      4  ⎥
                                 ⎢                                              ⎥⎢      ⎥  ⎢         ⎥
                                 ⎢ 0     0     0 −1    2 −1        0 ···    0 ⎥ ⎢ u5 ⎥ = λ ⎢ u5 /1.5 ⎥
                                 ⎢                                              ⎥⎢      ⎥  ⎢         ⎥
                                 ⎢ 0               0 −1       2 −1 · · ·    0⎥    ⎢     ⎥  ⎢ u6 /2 ⎥
                                 ⎢       0     0                                ⎥ ⎢ u6 ⎥   ⎢         ⎥
                                 ⎢ .      ..    ..  ..  .. . .   ..   ..     .. ⎥ ⎢ . ⎥    ⎢ . ⎥
                                 ⎢ ..                          .    .    .      ⎥ ⎢  .  ⎥  ⎢     .   ⎥
                                 ⎢         .     .   .   .                    .⎥⎢ . ⎥      ⎢ . ⎥
                                 ⎢                                              ⎥⎢      ⎥  ⎢         ⎥
                                 ⎣ 0 ···       0   0   0      0 −1      2 −1 ⎦ ⎣ u9 ⎦      ⎣ u9 /2 ⎦
                                    0 ···      0   0   0      0    0 −1     1       u10      u10 /4
                            where
                                                                                        2
                                                                         P         L
                                                               λ=
                                                                        E I0       20
                            Write a program that computes the lowest buckling load P of the column with
                            the inverse power method. Utilize the banded forms of the matrices.
                      18.
                                                                                    L             P
                                                                   L                θ3
                                                   L                                         k
                                                                   θ2
                                                        θ1 k                   k

                            The springs supporting the three-bar linkage are undeformed when the linkage
                            is horizontal. The equilibrium equations of the linkage in the presence of the
                            horizontal force P can be shown to be
                                              ⎡          ⎤⎡ ⎤         ⎡          ⎤⎡ ⎤
                                                6 5 3       θ1          1 1 1        θ1
                                              ⎢          ⎥⎢ ⎥       P ⎢          ⎥⎢ ⎥
                                              ⎣ 3   3  2    θ
                                                         ⎦⎣ 2⎦  =     ⎣ 0   1  1 ⎦ ⎣ θ2 ⎦
                                                                   kL
                                                1 1 1       θ3          0 0 1        θ3
                            where k is the spring stiffness. Determine the smallest buckling load P and the
                            corresponding mode shape. Hint: The equations can easily rewritten in the stan-
                            dard form Aθ = λθ, where A is symmetric.
                      19.
                                                              u1                    u2            u3
                                               k          k                        k             k
                                                    m              3m                       2m


                            The differential equations of motion for the mass-spring system are

                                                              k (−2u1 + u2 ) = m¨
                                                                                u1

                                                          k(u1 − 2u2 + u3 ) = 3m¨
                                                                                u2

                                                                k(u2 − 2u3 ) = 2m¨
                                                                                 u3
P1: PHB

CUUS884-Kiusalaas    CUUS884-09           978 0 521 19132 6                                                          December 16, 2009   15:4




           350      Symmetric Matrix Eigenvalue Problems

                          where ui (t ) is the displacement of mass i from its equilibrium position and k is
                          the spring stiffness. Determine the circular frequencies of vibration and the cor-
                          responding mode shapes.
                    20.
                                            L                       L                       L                    L

                                     i1               i2                          i3                      i4
                                                i1 C /2                  i2                     i3                   i4
                                 C                                             C /3                    C /4                C /5


                          Kirchoff’s equations for the circuit are

                                                                        d 2i1  1    2
                                                                    L       2
                                                                              + i1 + (i1 − i2 ) = 0
                                                                        dt     C    C
                                                              d 2i2  2            3
                                                          L       2
                                                                    + (i2 − i1 ) + (i2 − i3 ) = 0
                                                              dt     C            C
                                                              d 2i3  3            4
                                                          L       2
                                                                    + (i3 − i2 ) + (i3 − i4 ) = 0
                                                              dt     C            C
                                                                        d 2i4  4            5
                                                                    L         + (i4 − i3 ) + i4 = 0
                                                                        dt 2   C            C
                          Find the circular frequencies of the current.
                    21.
                                                C                       C/2                     C/3                  C/4

                                       i1                      i2                      i3                   i4
                                                i1                        i2                     i3                   i4
                                   L                       L                       L                    L                   L


                          Determine the circular frequencies of oscillation for the circuit shown, given the
                          Kirchoff equations

                                                                     d 2i1             d 2i1   d 2i2         1
                                                                 L         +L                −           +     i1 = 0
                                                                     dt 2              dt  2   dt 2          C
                                                         d 2i2   d 2i1                  d 2i2   d 2i3            2
                                                    L        2
                                                               −               +L             −             +      =0
                                                         dt      dt 2                   dt  2   dt 2             C
                                                        d 2i3   d 2i2                  d 2i3   d 2i4         3
                                                L             −               +L             −           +     i3 = 0
                                                        dt 2    dt 2                   dt 2    dt 2          C
                                                                        d 2i4   d 2i3                d 2i4  4
                                                                 L          2
                                                                              −                 +L         + i4 = 0
                                                                        dt      dt 2                 dt 2   C

                    22.     Several iterative methods exist for finding the eigenvalues of a matrix A. One of
                          these is the LR method, which requires the matrix to be symmetric and positive
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                   December 16, 2009     15:4




               351     9.4 Householder Reduction to Tridiagonal Form

                          definite. Its algorithm is very simple:

                                     Let A0 = A
                                     do with i = 0, 1, 2, . . .
                                          Use Choleski’s decomposition Ai = Li LiT to compute Li
                                          Form Ai+1 = LiT Li
                                     end do

                          It can be shown that the diagonal elements of Ai+1 converge to the eigenvalues
                          of A. Write a program that implements the LR method and test it with
                                                              ⎡        ⎤
                                                                4 3 1
                                                              ⎢        ⎥
                                                          A = ⎣3 4 2⎦
                                                                1 2 3



              9.4     Householder Reduction to Tridiagonal Form

                      It was mentioned before that similarity transformations can be used to transform an
                      eigenvalue problem to a form that is easier to solve. The most desirable of the “easy”
                      forms is, of course, the diagonal form that results from the Jacobi method. However,
                      the Jacobi method requires about 10n3 to 20n3 multiplications, so that the amount of
                      computation increases very rapidly with n. We are generally better off by reducing the
                      matrix to the tridiagonal form, which can be done in precisely n − 2 transformations
                      by the Householder method. Once the tridiagonal form is achieved, we still have to
                      extract the eigenvalues and the eigenvectors, but there are effective means of dealing
                      with that, as we see in the next section.


                      Householder Matrix
                      Each Householder transformation utilizes the Householder matrix
                                                                      uuT
                                                             Q=I−                                          (9.36)
                                                                       H
                      where u is a vector and
                                                              1 T    1
                                                        H=      u u = |u|2                                 (9.37)
                                                              2      2
                      Note that uuT in Eq. (9.36) is the outer product, that is, a matrix with the elements
                       uuT ij = ui u j . Because Q is obviously symmetric (QT = Q), we can write

                                                       uuT          uuT            uuT   u uT u uT
                                  QT Q = QQ = I −             I−            =I−2       +
                                                        H            H              H       H2
                                                uuT   u (2H) uT
                                      = I−2         +           =I
                                                 H        H2
                      which shows that Q is also orthogonal.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09       978 0 521 19132 6                                                      December 16, 2009      15:4




           352      Symmetric Matrix Eigenvalue Problems

                          Now let x be an arbitrary vector and consider the transformation Qx. Choosing

                                                                u = x + ke1                                                (9.38)

                    where
                                                                                                     T
                                            k = ± |x|           e1 = 1      0     0   ···       0

                    we get
                                                                                                T
                                                          uuT                   u x + ke1
                                          Qx = I −               x= I−                               x
                                                           H                         H

                                                       u xT x+ke1T x     u k 2 + kx1
                                              = x−                   =x−
                                                             H                 H
                    But
                                                   T
                                  2H = x + ke1         x + ke1 = |x|2 + k xT e1 +e1T x + k 2 e1T e1

                                      = k 2 + 2kx1 + k 2 = 2 k 2 + kx1

                    so that
                                                                                                         T
                                         Qx = x − u = −ke1 = −k                   0   0       ···    0                     (9.39)

                    Hence, the transformation eliminates all elements of x except the first one.


                    Householder Reduction of a Symmetric Matrix
                    Let us now apply the following transformation to a symmetric n × n matrix A:

                                                    1 0T          A 11   xT           A 11          xT
                                          P1 A =                                  =                                        (9.40)
                                                    0 Q            x     A            Qx            QA

                    Here x represents the first column of A with the first element omitted, and A is sim-
                    ply A with its first row and column removed. The matrix Q of dimensions (n − 1) ×
                    (n − 1) is constructed using Eqs. (9.36)–(9.38). Referring to Eq. (9.39), we see that the
                    transformation reduces the first column of A to
                                                                 ⎡      ⎤
                                                                   A 11
                                                                 ⎢ −k ⎥
                                                                 ⎢      ⎥
                                                         A 11    ⎢      ⎥
                                                               =⎢⎢   0  ⎥
                                                                        ⎥
                                                         Qx      ⎢ .. ⎥
                                                                 ⎣ . ⎦
                                                                              0

                    The transformation
                                                                                          T
                                                                         A 11     Qx
                                                   A ← P1 AP1 =                                                            (9.41)
                                                                         Qx       QA Q
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                                     December 16, 2009     15:4




               353     9.4 Householder Reduction to Tridiagonal Form

                      thus tridiagonalizes the first row as well as the first column of A. Here is a diagram of
                      the transformation for a 4 × 4 matrix:

                                      1     0    0     0         A 11       A 12      A 13   A 14     1     0   0    0
                                      0                          A 21                                 0
                                                               ·                                    ·
                                      0          Q               A 31                 A               0         Q
                                      0                          A 41                                 0

                                     A 11       −k         0          0
                                     −k
                                   =
                                      0                QA Q
                                      0

                      The second row and column of A are reduced next by applying the transformation to
                      the 3 × 3 lower right portion of the matrix. This transformation can be expressed as
                      A ← P2 AP2 , where now


                                                                                 I2   0T
                                                                      P2 =                                                    (9.42)
                                                                                 0    Q

                      In Eq. (9.42), I2 is a 2 × 2 identity matrix and Q is a (n − 2) × (n − 2) matrix con-
                      structed by choosing for x the bottom n − 2 elements of the second column of A.
                      It takes a total of n − 2 transformations with

                                                                Ii    0T
                                                     Pi =                    , i = 1, 2, . . . , n − 2
                                                                0     Q

                      to attain the tridiagonal form.
                           It is wasteful to form Pi and to carry out the matrix multiplication Pi APi . We note
                      that
                                                                      uuT                  Au T
                                            AQ=A               I−             =A −           u = A −vuT
                                                                       H                   H
                      where
                                                                                   Au
                                                                            v=                                                (9.43)
                                                                                   H
                      Therefore,
                                                        uuT                                         uuT
                                     QA Q = I −                       A −vuT = A −vuT −                 A −vuT
                                                         H                                           H
                                                                     u uT A   u uT v uT
                                            = A −vuT −                      +
                                                                       H         H
                                            = A −vuT −uvT + 2guuT

                      where
                                                                                   uT v
                                                                            g=                                                (9.44)
                                                                                   2H
P1: PHB

CUUS884-Kiusalaas     CUUS884-09      978 0 521 19132 6                                             December 16, 2009         15:4




           354      Symmetric Matrix Eigenvalue Problems

                    Letting

                                                              w = v − gu                                          (9.45)

                    it can be easily verified that the transformation can be written as

                                                      QA Q = A −wuT −uwT                                          (9.46)

                    which gives us the following computational procedure that is to be carried out with
                    i = 1, 2, . . . , n − 2:

                     1. Let A be the (n − i) × n − i lower right-hand portion of A.
                                                                   T
                     2. Let x = A i+1,i A i+2,i · · · A n,i   (the column of length n − i just left of A ).
                     3. Compute |x|. Let k = |x| if x1 > 0 and k = − |x| if x1 < 0 (this choice of sign mini-
                        mizes the roundoff error).
                                                                       T
                     4.   Let u = k+x1 x2 x3 · · · xn−i .
                     5.   Compute H = |u| /2.
                     6.   Compute v = A u/H.
                     7.   Compute g = uT v/(2H).
                     8.   Compute w = v − gu.
                     9.   Compute the transformation A ← A −wT u − uT w.
                    10.   Set A i,i+1 = A i+1,i = −k.


                    Accumulated Transformation Matrix
                    Because we used similarity transformations, the eigenvalues of the tridiagonal matrix
                    are the same as those of the original matrix. However, to determine the eigenvectors
                    X of original A, we must use the transformation

                                                             X = PXtridiag

                    where P is the accumulation of the individual transformations:

                                                           P = P1 P2 · · · Pn−2

                    We build up the accumulated transformation matrix by initializing P to an n × n iden-
                    tity matrix and then applying the transformation

                                                     P11     P12           Ii    0T       P11   P21 Q
                                      P ← PPi =                                       =                                 (b)
                                                     P21     P22           0     Q        P12   P22 Q

                    with i = 1, 2, . . . , n − 2. It can be seen that each multiplication affects only the right-
                    most n − i columns of P (because the first row of P12 contains only zeroes, it can also
                    be omitted in the multiplication). Using the notation

                                                                           P12
                                                              P=
                                                                           P22
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                  December 16, 2009     15:4




               355     9.4 Householder Reduction to Tridiagonal Form

                      we have
                                      P12 Q                      uuT           Pu T
                                               =PQ=P        I−          =P −     u = P −yuT               (9.47)
                                      P22 Q                       H            H

                      where
                                                                   Pu
                                                              y=                                          (9.48)
                                                                   H
                      The procedure for carrying out the matrix multiplication in Eq. (b) is:

                        • Retrieve u (in our triangularization procedure the u’s are stored in the columns
                          of the lower triangular portion of A).
                        • Compute H = |u| /2.
                        • Compute y = P u/H.
                        • Compute the transformation P ← P −yuT .


                         householder

                      The function householder in this module does the triangulization. It returns (d, c),
                      where d and c are vectors that contain the elements of the principal diagonal and the
                      subdiagonal, respectively. Only the upper triangular portion is reduced to the trian-
                      gular form. The part below the principal diagonal is used to store the vectors u. This is
                      done automatically by the statement u = a[k+1:n,k], which does not create a new
                      object u, but simply sets up a reference to a[k+1:n,k] (makes a deep copy). Thus,
                      any changes made to u are reflected in a[k+1:n,k].
                           The function computeP returns the accumulated transformation matrix P. There
                      is no need to call it if only the eigenvalues are to be computed.

                      ## module householder
                      ’’’ d,c = householder(a).
                            Householder similarity transformation of matrix [a] to
                            the tridiagonal form [c\d\c].


                            p = computeP(a).
                            Computes the acccumulated transformation matrix [p]
                            after calling householder(a).
                      ’’’
                      from numpy import dot,diagonal,outer,identity
                      from math import sqrt


                      def householder(a):
                            n = len(a)
                            for k in range(n-2):
                                  u = a[k+1:n,k]
                                  uMag = sqrt(dot(u,u))
                                  if u[0] < 0.0: uMag = -uMag
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                  December 16, 2009   15:4




           356      Symmetric Matrix Eigenvalue Problems

                              u[0] = u[0] + uMag
                              h = dot(u,u)/2.0
                              v = dot(a[k+1:n,k+1:n],u)/h
                              g = dot(u,v)/(2.0*h)
                              v = v - g*u
                              a[k+1:n,k+1:n] = a[k+1:n,k+1:n] - outer(v,u) \
                                                        - outer(u,v)
                              a[k,k+1] = -uMag
                         return diagonal(a),diagonal(a,1)


                    def computeP(a):
                         n = len(a)
                         p = identity(n)*1.0
                         for k in range(n-2):
                              u = a[k+1:n,k]
                              h = dot(u,u)/2.0
                              v = dot(p[1:n,k+1:n],u)/h
                              p[1:n,k+1:n] = p[1:n,k+1:n] - outer(v,u)
                         return p

                    EXAMPLE 9.7
                    Transform the matrix
                                                          ⎡            ⎤
                                                        7      2  3 −1
                                                     ⎢ 2             1⎥
                                                     ⎢         8  5    ⎥
                                                   A=⎢                 ⎥
                                                     ⎣ 3       5 12  9⎦
                                                       −1      1  9  7
                    into tridiagonal form.

                    Solution Reduce the first row and column:
                                      ⎡            ⎤         ⎡    ⎤
                                        8     5 1               2
                                      ⎢            ⎥         ⎢    ⎥
                                 A = ⎣ 5 12 9 ⎦          x = ⎣ 3⎦           k = |x| = 3. 7417
                                        1     9 7              −1
                                        ⎡        ⎤ ⎡        ⎤
                                          k + x1     5.7417
                                        ⎢        ⎥ ⎢        ⎥               1 2
                                    u = ⎣ x2 ⎦ = ⎣        3⎦           H=     |u| = 21. 484
                                                                            2
                                            x3           −1
                                                   ⎡                           ⎤
                                                      32.967    17 225 −5.7417
                                                   ⎢                           ⎥
                                             uuT = ⎣ 17.225          9      −3 ⎦
                                                     −5.7417        −3       1
                                                  ⎡                                     ⎤
                                                    −0.53450       −0.80176     0.26725
                                            uuT   ⎢                                     ⎥
                                     Q = I−     = ⎣ −0.80176        0.58108     0.13964 ⎦
                                             H
                                                     0.26725        0.13964     0.95345
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                          December 16, 2009   15:4




               357     9.4 Householder Reduction to Tridiagonal Form
                                                        ⎡                                  ⎤
                                                         10.642       −0.1388      −9.1294
                                                      ⎢                                    ⎥
                                               QA Q = ⎣ −0.1388        5.9087       4.8429 ⎦
                                                        −9.1294        4.8429      10.4480

                                                            ⎡                               ⎤
                                                                 7 −3.7417        0       0
                                                   T     ⎢ −3.7417  10.642 −0. 1388 −9.1294 ⎥
                                       A 11    Qx        ⎢                                  ⎥
                              A←                        =⎢                                  ⎥
                                       Qx      QA Q      ⎣       0 −0.1388  5.9087   4.8429 ⎦
                                                                 0 −9.1294  4.8429 10.4480
                                                                                               T
                      In the last step, we used the formula Qx = −k           0    ···     0       .
                           Reduce the second row and column:

                                     5.9087     4.8429                 −0.1388
                             A =                                 x=                       k = − |x| = −9.1305
                                     4.8429    10.4480                 −9.1294

                      where the negative sign of k was determined by the sign of x1 .

                                               k + x1           −9. 2693                 1 2
                                     u=                  =                    H=           |u| = 84.633
                                              −9.1294           −9.1294                  2


                                                                  85.920   84.623
                                                        uuT =
                                                                  84.623   83.346


                                                        uuT         0.01521       −0.99988
                                               Q = I−       =
                                                         H         −0.99988        0.01521


                                                                    10.594 4.772
                                                        QA Q =
                                                                     4.772 5.762

                                                                ⎡                            ⎤
                                       ⎡                      ⎤        7 −3.742      0     0
                                        A 11    A 12     0T     ⎢
                                      ⎢                     T ⎥ ⎢ −3.742 10.642  9.131     0⎥⎥
                                  A ← ⎣ A 21    A 22    Qx ⎦ ⎢                               ⎥
                                                                ⎣      0  9.131 10.594 4.772 ⎦
                                         0      Qx      QA Q
                                                                       0     −0  4.772 5.762

                      EXAMPLE 9.8
                      Use the function householder to tridiagonalize the matrix in Example 9.7; also de-
                      termine the transformation matrix P.

                      Solution

                      #!/usr/bin/python
                      ## example9_8
                      from numpy import array
                      from householder import *
P1: PHB

CUUS884-Kiusalaas       CUUS884-09     978 0 521 19132 6                                      December 16, 2009   15:4




           358      Symmetric Matrix Eigenvalue Problems

                    a = array([[ 7.0, 2.0,             3.0, -1.0],        \
                                     [ 2.0, 8.0,       5.0,      1.0],    \
                                     [ 3.0, 5.0, 12.0,           9.0],    \
                                     [-1.0, 1.0,       9.0,      7.0]])
                    d,c = householder(a)
                    print "Principal diagonal {d}:\n", d
                    print "\nSubdiagonal {c}:\n",c
                    print "\nTransformation matrix [P]:"
                    print computeP(a)
                    raw_input("\nPress return to exit")



                          The results of running the foregoing program are:


                    Principal diagonal {d}:
                    [    7.              10.64285714          10.59421525          5.76292761]


                    Subdiagonal {c}:
                    [-3.74165739        9.13085149         4.77158058]


                    Transformation matrix [P]:
                    [[ 1.                0.                 0.                0.          ]
                     [ 0.               -0.53452248 -0.25506831               0.80574554]
                     [ 0.               -0.80178373 -0.14844139 -0.57888514]
                     [ 0.                0.26726124 -0.95546079 -0.12516436]]




          9.5       Eigenvalues of Symmetric Tridiagonal Matrices
                    Sturm Sequence
                    In principle, the eigenvalues of a matrix A can be determined by finding the roots of
                    the characteristic equation |A − λI| = 0. This method is impractical for large matri-
                    ces, because the evaluation of the determinant involves n3 /3 multiplications. How-
                    ever, if the matrix is tridiagonal (we also assume it to be symmetric), its characteristic
                    polynomial


                                                 d1 − λ   c1      0      0    ···                   0
                                                   c1   d2 − λ   c2      0    ···                   0
                                                    0     c2   d3 − λ   c3    ···                   0
                               Pn (λ) = |A−λI| =    0      0     c3   d4 − λ · · ·                  0
                                                    ..     ..     ..     ..   ..                    ..
                                                     .      .      .      .      .                   .
                                                    0      0     ...     0   cn−1                dn − λ
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                        December 16, 2009     15:4




               359     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                      can be computed with only 3(n − 1) multiplications using the following sequence of
                      operations:

                                       P0 (λ) = 1

                                       P1 (λ) = d1 − λ                                                          (9.49)

                                       Pi (λ) = (di − λ)Pi−1 (λ) − ci−1
                                                                    2
                                                                        Pi−2 (λ), i = 2, 3, . . . , n

                          The polynomials P0 (λ), P1 (λ), . . . , Pn (λ) form a Sturm sequence that has the fol-
                      lowing property:

                        • The number of sign changes in the sequence P0 (a), P1 (a), . . . , Pn (a) is equal to
                          the number of roots of Pn (λ) that are smaller than a. If a member Pi (a) of the
                          sequence is zero, its sign is to be taken opposite to that of Pi−1 (a).

                          As we see later, the Sturm sequence property makes it possible to bracket the
                      eigenvalues of a tridiagonal matrix.


                         sturmSeq

                      Given d, c, and λ, the function sturmSeq returns the Sturm sequence

                                                         P0 (λ), P1 (λ), . . . Pn (λ)

                      The function numLambdas returns the number of sign changes in the sequence (as
                      noted before, this equals the number of eigenvalues that are smaller than λ).

                      ## module sturmSeq
                      ’’’ p = sturmSeq(c,d,lam).
                            Returns the Sturm sequence {p[0],p[1],...,p[n]}
                            associated with the characteristic polynomial
                            |[A] - lam[I]| = 0, where [A] = [c\d\c] is a n x n
                            tridiagonal matrix.


                            numLam = numLambdas(p).
                            Returns the number of eigenvalues of a tridiagonal
                            matrix [A] = [c\d\c] that are smaller than ’lam’.
                            Uses the Sturm sequence {p} obtained from ’sturmSeq’.
                      ’’’
                      from numpy import ones


                      def sturmSeq(d,c,lam):
                            n = len(d) + 1
                            p = ones(n)
                            p[1] = d[0] - lam
                            for i in range(2,n):
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                 December 16, 2009     15:4




           360      Symmetric Matrix Eigenvalue Problems

                    ##          if c[i-2] == 0.0: c[i-2] = 1.0e-12
                              p[i] = (d[i-1] - lam)*p[i-1] - (c[i-2]**2)*p[i-2]
                         return p


                    def numLambdas(p):
                         n = len(p)
                         signOld = 1
                         numLam = 0
                         for i in range(1,n):
                              if p[i] > 0.0: sign = 1
                              elif p[i] < 0.0: sign = -1
                              else: sign = -signOld
                              if sign*signOld < 0: numLam = numLam + 1
                              signOld = sign
                         return numLam

                    EXAMPLE 9.9
                    Use the Sturm sequence property to show that the smallest eigenvalue of A is in the
                    interval (0.25, 0.5), where
                                                  ⎡                     ⎤
                                                       2 −1     0     0
                                                  ⎢                     ⎥
                                                  ⎢ −1     2 −1       0⎥
                                                A=⎢                     ⎥
                                                  ⎣ 0 −1        2 −1 ⎦
                                                       0   0 −1       2

                    Solution Taking λ = 0.5, we have di − λ = 1.5 and ci−1
                                                                       2
                                                                           = 1 and the Sturm sequence
                    in Eqs. (9.49) becomes

                                           P0 (0.5) = 1
                                           P1 (0.5) = 1.5
                                           P2 (0.5) = 1.5(1.5) − 1 = 1.25
                                           P3 (0.5) = 1.5(1.25) − 1.5 = 0.375
                                           P4 (0.5) = 1.5(0.375) − 1.25 = −0.6875

                    Because the sequence contains one sign change, there exists one eigenvalue smaller
                    than 0.5.
                         Repeating the process with λ = 0.25, we get di − λ = 1.75 and ci2 = 1, which re-
                    sults in the Sturm sequence

                                         P0 (0.25) = 1
                                         P1 (0.25) = 1.75
                                         P2 (0.25) = 1.75(1.75) − 1 = 2.0625
                                         P3 (0.25) = 1.75(2.0625) − 1.75 = 1.8594
                                         P4 (0.25) = 1.75(1.8594) − 2.0625 = 1.1915
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                             December 16, 2009     15:4




               361     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                      There are no sign changes in the sequence, so that all the eigenvalues are greater than
                      0.25. We thus conclude that 0.25 < λ1 < 0.5.


                      Gerschgorin’s Theorem
                      Gerschgorin’s theorem is useful in determining the global bounds on the eigenval-
                      ues of an n × n matrix A. The term “global” means the bounds that enclose all the
                      eigenvalues. Here we give a simplified version for a symmetric matrix.

                        • If λ is an eigenvalue of A, then

                                                   ai − r i ≤ λ ≤ ai + r i ,     i = 1, 2, . . . , n

                            where
                                                                                  n
                                                          ai = A ii      ri =           A ij                          (9.50)
                                                                                 j =1
                                                                                 j =i

                            It follows that the limits on the smallest and the largest eigenvalues are given by

                                               λmin ≥ min(ai − ri )            λmax ≤ max(ai + ri )                   (9.51)
                                                          i                                i



                         gerschgorin

                      The function gerschgorin returns the lower and upper global bounds on the eigen-
                      values of a symmetric tridiagonal matrix A = [c\d\c].

                      ## module gerschgorin
                      ’’’ lamMin,lamMax = gerschgorin(d,c).
                            Applies Gerschgorin’s theorem to find the global bounds on
                            the eigenvalues of a tridiagomal matrix [A] = [c\d\c].
                      ’’’
                      def gerschgorin(d,c):
                            n = len(d)
                            lamMin = d[0] - abs(c[0])
                            lamMax = d[0] + abs(c[0])
                            for i in range(1,n-1):
                                  lam = d[i] - abs(c[i]) - abs(c[i-1])
                                  if lam < lamMin: lamMin = lam
                                  lam = d[i] + abs(c[i]) + abs(c[i-1])
                                  if lam > lamMax: lamMax = lam
                            lam = d[n-1] - abs(c[n-2])
                            if lam < lamMin: lamMin = lam
                            lam = d[n-1] + abs(c[n-2])
                            if lam > lamMax: lamMax = lam
                            return lamMin,lamMax
P1: PHB

CUUS884-Kiusalaas     CUUS884-09      978 0 521 19132 6                                      December 16, 2009         15:4




           362      Symmetric Matrix Eigenvalue Problems

                      EXAMPLE 9.10
                    Use Gerschgorin’s theorem to determine the bounds on the eigenvalues of the matrix
                                                      ⎡              ⎤
                                                         4 −2      0
                                                      ⎢              ⎥
                                                  A = ⎣ −2     4 −2 ⎦
                                                         0 −2      5
                    Solution Referring to Eqs. (9.50), we get

                                                   a1 = 4       a2 = 4      a3 = 5

                                                    r1 = 2      r2 = 4      r3 = 2

                    Hence,

                                                  λmin ≥ min(ai − ri ) = 4 − 4 = 0

                                                  λmax ≤ max(ai + ri ) = 4 + 4 = 8


                    Bracketing Eigenvalues
                    The Sturm sequence property together with Gerschgorin’s theorem provide us con-
                    venient tools for bracketing each eigenvalue of a symmetric tridiagonal matrix.


                      lamRange

                    The function lamRange brackets the N smallest eigenvalues of a symmetric tridi-
                    agonal matrix A = [c\d\c]. It returns the sequence r 0 , r 1 , . . . , r N , where each interval
                     ri−1 , ri contains exactly one eigenvalue. The algorithm first finds the bounds on all
                    the eigenvalues by Gerschgorin’s theorem. Then the method of bisection in conjunc-
                    tion with Sturm sequence property is used to determine r N , r N−1 , . . . , r 0 in that order.

                    ## module lamRange
                    ’’’ r = lamRange(d,c,N).
                          Returns the sequence {r[0],r[1],...,r[N]} that
                          separates the N lowest eigenvalues of the tridiagonal
                          matrix [A] = [c\d\c]; that is, r[i] < lam[i] < r[i+1].
                    ’’’
                    from numpy import ones
                    from sturmSeq import *
                    from gerschgorin import *


                    def lamRange(d,c,N):
                          lamMin,lamMax = gerschgorin(d,c)
                          r = ones(N+1)
                          r[0] = lamMin
                      # Search for eigenvalues in descending order
                          for k in range(N,0,-1):
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                   December 16, 2009   15:4




               363     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                             # First bisection of interval(lamMin,lamMax)
                                  lam = (lamMax + lamMin)/2.0
                                  h = (lamMax - lamMin)/2.0
                                  for i in range(1000):
                                   # Find number of eigenvalues less than lam
                                      p = sturmSeq(d,c,lam)
                                      numLam = numLambdas(p)
                                   # Bisect again & find the half containing lam
                                      h = h/2.0
                                      if numLam < k: lam = lam + h
                                      elif numLam > k: lam = lam - h
                                      else: break
                             # If eigenvalue located, change the upper limit
                             # of search and record it in [r]
                                  lamMax = lam
                                  r[k] = lam
                           return r

                      EXAMPLE 9.11
                      Bracket each eigenvalue of the matrix A in Example 9.10.

                      Solution In Example 9.10 we found that all the eigenvalues lie in (0, 8). We now bisect
                      this interval and use the Sturm sequence to determine the number of eigenvalues in
                      (0, 4). With λ = 4, the sequence is – see Eqs. (9.49).

                                                  P0 (4) = 1

                                                  P1 (4) = 4 − 4 = 0

                                                  P2 (4) = (4 − 4)(0) − 22 (1) = −4

                                                  P3 (4) = (5 − 4)(−4) − 22 (0) = −4

                      Because a zero value is assigned, the sign opposite to that of the preceding member,
                      the signs in this sequence are (+, −, −, −). The one sign change shows the presence
                      of one eigenvalue in (0, 4).
                          Next, we bisect the interval (4, 8) and compute the Sturm sequence with λ = 6:

                                                   P0 (6) = 1

                                                   P1 (6) = 4 − 6 = −2

                                                   P2 (6) = (4 − 6)(−2) − 22 (1) = 0

                                                   P3 (6) = (5 − 6)(0) − 22 (−2) = 8

                      In this sequence the signs are (+, −, +, +), indicating two eigenvalues in (0, 6).
                           Therefore,

                                              0 ≤ λ1 ≤ 4        4 ≤ λ2 ≤ 6   6 ≤ λ3 ≤ 8
P1: PHB

CUUS884-Kiusalaas    CUUS884-09       978 0 521 19132 6                            December 16, 2009     15:4




           364      Symmetric Matrix Eigenvalue Problems

                    Computation of Eigenvalues
                    Once the desired eigenvalues are bracketed, they can be found by determining the
                    roots of Pn (λ) = 0 with bisection or Ridder’s method.


                      eigenvals3

                    The function eigenvals3 computes N smallest eigenvalues of a symmetric tridiag-
                    onal matrix with the method of Ridder.

                    ## module eigenvals3
                    ’’’ lam = eigenvals3(d,c,N).
                          Returns the N smallest eigenvalues of a
                          tridiagonal matrix [A] = [c\d\c].
                    ’’’
                    from lamRange import *
                    from ridder import *
                    from sturmSeq import sturmSeq
                    from numpy import zeros,float


                    def eigenvals3(d,c,N):


                          def f(x):                       # f(x) = |[A] - x[I]|
                               p = sturmSeq(d,c,x)
                               return p[len(p)-1]


                          lam = zeros(N)
                          r = lamRange(d,c,N)             # Bracket eigenvalues
                          for i in range(N):              # Solve by Ridder’s method
                               lam[i] = ridder(f,r[i],r[i+1])
                          return lam

                    EXAMPLE 9.12
                    Use eigenvals3 to determine the three smallest eigenvalues of the 100 × 100 matrix
                                                          ⎡                  ⎤
                                                       2 −1     0 ··· 0
                                                    ⎢ −1  2 −1 · · · 0 ⎥
                                                    ⎢                        ⎥
                                                    ⎢                        ⎥
                                                    ⎢
                                                  A=⎢  0 −1     2  · · · 0   ⎥
                                                                             ⎥
                                                    ⎢ ..  .. . .    ..    .. ⎥
                                                    ⎣ .    .     .     . .⎦
                                                       0  0 · · · −1 2

                    Solution

                    #!/usr/bin/python
                    ## example9_12
                    from numpy import ones
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                             December 16, 2009   15:4




               365     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                      from eigenvals3 import *


                      N = 3
                      n = 100
                      d = ones(n)*2.0
                      c = ones(n-1)*(-1.0)
                      lambdas = eigenvals3(d,c,N)
                      print lambdas
                      raw_input("\nPress return to exit")

                            Here are the eigenvalues:

                      [ 0.00096744       0.00386881     0.0087013 ]



                      Computation of Eigenvectors
                      If the eigenvalues are known (approximate values will be good enough), the best
                      means of computing the corresponding eigenvectors is the inverse power method
                      with eigenvalue shifting. This method was discussed before, but the algorithm listed
                      did not take advantage of banding. Here we present a version of the method written
                      for symmetric tridiagonal matrices.


                         inversePower3

                      This function is very similar to inversePower listed in Section 9.3, but it executes
                      much faster because it exploits the tridiagonal structure of the matrix.

                      ## module inversePower3
                      ’’’ lam,x = inversePower3(d,c,s,tol=1.0e-6).
                            Inverse power method applied to a tridiagonal matrix
                            [A] = [c\d\c]. Returns the eigenvalue closest to ’s’
                            and the corresponding eigenvector.
                      ’’’
                      from numpy import dot,zeros
                      from LUdecomp3 import *
                      from math import sqrt
                      from random import random


                      def inversePower3(d,c,s,tol=1.0e-6):
                            n = len(d)
                            e = c.copy()
                            cc = c.copy()                     # Save original [c]
                            dStar = d - s                     # Form [A*] = [A] - s[I]
                            LUdecomp3(cc,dStar,e)             # Decompose [A*]
                            x = zeros(n)
P1: PHB

CUUS884-Kiusalaas    CUUS884-09         978 0 521 19132 6                          December 16, 2009   15:4




           366      Symmetric Matrix Eigenvalue Problems

                        for i in range(n):                   # Seed [x] with random numbers
                              x[i] = random()
                        xMag = sqrt(dot(x,x))                # Normalize [x]
                        x =x/xMag
                        flag = 0
                        for i in range(30):                  # Begin iterations
                              xOld = x.copy()                # Save current [x]
                              LUsolve3(cc,dStar,e,x)         # Solve [A*][x] = [xOld]
                              xMag = sqrt(dot(x,x))          # Normalize [x]
                              x = x/xMag
                              if dot(xOld,x) < 0.0:          # Detect change in sign of [x]
                                   sign = -1.0
                                   x = -x
                              else: sign = 1.0
                              if sqrt(dot(xOld - x,xOld - x)) < tol:
                                   return s + sign/xMag,x
                        print ’Inverse power method did not converge’

                    EXAMPLE 9.13
                    Compute the 10th smallest eigenvalue of the matrix A given in Example 9.12.

                    Solution The following program extracts the Nth eigenvalue of A by the inverse
                    power method with eigenvalue shifting:

                    #!/usr/bin/python
                    ## example9_13
                    from numpy import ones
                    from lamRange import *
                    from inversePower3 import *


                    N = 10
                    n = 100
                    d = ones(n)*2.0
                    c = ones(n-1)*(-1.0)
                    r = lamRange(d,c,N)                     # Bracket N smallest eigenvalues
                    s = (r[N-1] + r[N])/2.0                 # Shift to midpoint of Nth bracket
                    lam,x = inversePower3(d,c,s)            # Inverse power method
                    print "Eigenvalue No.",N," =",lam
                    raw_input("\nPress return to exit")

                        The result is

                    Eigenvalue No. 10           = 0.0959737849345

                    EXAMPLE 9.14
                    Compute the three smallest eigenvalues and the corresponding eigenvectors of the
                    matrix A in Example 9.5.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                December 16, 2009   15:4




               367     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                      Solution

                      #!/usr/bin/python
                      ## example9_14
                      from householder import *
                      from eigenvals3 import *
                      from inversePower3 import *
                      from numpy import array,zeros,dot


                      N = 3       # Number of eigenvalues requested
                      a = array([[ 11.0, 2.0,             3.0,   1.0,   4.0],   \
                                     [   2.0, 9.0,        3.0,   5.0,   2.0],   \
                                     [   3.0, 3.0, 15.0,         4.0,   3.0],   \
                                     [   1.0, 5.0,        4.0, 12.0,    4.0],   \
                                     [   4.0, 2.0,        3.0,   4.0, 17.0]])
                      xx = zeros((len(a),N))
                      d,c = householder(a)                         # Tridiagonalize [A]
                      p = computeP(a)                              # Compute transformation matrix
                      lambdas = eigenvals3(d,c,N)                  # Compute eigenvalues
                      for i in range(N):
                           s = lambdas[i]*1.0000001                # Shift very close to eigenvalue
                           lam,x = inversePower3(d,c,s) # Compute eigenvector [x]
                           xx[:,i] = x                             # Place [x] in array [xx]
                      xx = dot(p,xx)                   # Recover eigenvectors of [A]
                      print "Eigenvalues:\n",lambdas
                      print "\nEigenvectors:\n",xx
                      raw_input("Press return to exit")


                      Eigenvalues:
                      [   4.87394638       8.66356791        10.93677451]


                      Eigenvectors:
                      [[ 0.26726603       0.72910002        0.50579164]
                       [-0.74142854       0.41391448 -0.31882387]
                       [-0.05017271 -0.4298639              0.52077788]
                       [ 0.59491453       0.06955611 -0.60290543]
                       [-0.14970633 -0.32782151 -0.08843985]]


                      PROBLEM SET 9.2

                       1. Use Gerschgorin’s theorem to determine bounds on the eigenvalues of
                                               ⎡             ⎤            ⎡              ⎤
                                                  10 4 −1                     4 2 −2
                                               ⎢             ⎥            ⎢              ⎥
                                     (a) A = ⎣ 4 2         3⎦     (b) B = ⎣ 2 5        3⎦
                                                 −1 3      6                 −2 3      4
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                     December 16, 2009   15:4




           368      Symmetric Matrix Eigenvalue Problems

                    2. Use the Sturm sequence to show that
                                                         ⎡          ⎤
                                                         5 −2  0  0
                                                      ⎢ −2  4 −1  0⎥
                                                      ⎢             ⎥
                                                    A=⎢             ⎥
                                                      ⎣ 0 −1   4 −2 ⎦
                                                         0  0 −2  5

                       has one eigenvalue in the interval (2, 4).
                    3. Bracket each eigenvalue of
                                                         ⎡             ⎤
                                                              4 −1   0
                                                         ⎢             ⎥
                                                    A = ⎣ −1      4 −1 ⎦
                                                              0 −1   4

                    4. Bracket each eigenvalue of
                                                                 ⎡             ⎤
                                                                6        1   0
                                                              ⎢                ⎥
                                                          A = ⎣1         8   2⎦
                                                                0        2   9

                    5. Bracket every eigenvalue of
                                                         ⎡          ⎤
                                                         2 −1  0  0
                                                      ⎢ −1  2 −1  0⎥
                                                      ⎢             ⎥
                                                    A=⎢             ⎥
                                                      ⎣ 0 −1   2 −1 ⎦
                                                         0  0 −1  1

                    6. Tridiagonalize the matrix
                                                                 ⎡              ⎤
                                                             12 4             3
                                                           ⎢                    ⎥
                                                         A=⎣ 4 9              3⎦
                                                              3 3            15

                       with the Householder reduction.
                    7. Use the Householder reduction to transform the matrix
                                                    ⎡                    ⎤
                                                        4 −2      1 −1
                                                    ⎢ −2      4 −2     1⎥
                                                    ⎢                    ⎥
                                                A=⎢                      ⎥
                                                    ⎣ 1 −2        4 −2 ⎦
                                                       −1     1 −2     4

                       to tridiagonal form.
                    8.   Compute all the eigenvalues of
                                                             ⎡                       ⎤
                                                             6       2   0   0     0
                                                           ⎢2        5   2   0     0⎥
                                                           ⎢                         ⎥
                                                           ⎢                         ⎥
                                                       A = ⎢0        2   7   4     0⎥
                                                           ⎢                         ⎥
                                                           ⎣0        0   4   6     1⎦
                                                             0       0   0   1     3
P1: PHB

CUUS884-Kiusalaas    CUUS884-09      978 0 521 19132 6                                           December 16, 2009   15:4




               369     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                       9.     Find the smallest two eigenvalues of
                                                           ⎡              ⎤
                                                               4 −1   0 1
                                                           ⎢ −1    6 −2 0 ⎥
                                                           ⎢              ⎥
                                                       A=⎢                ⎥
                                                           ⎣ 0 −2     3 2⎦
                                                               1   0  2 4

                      10.     Compute the three smallest eigenvalues of
                                                   ⎡                            ⎤
                                                        7 −4      3 −2     1  0
                                                   ⎢                            ⎥
                                                   ⎢ −4      8 −4       3 −2  1⎥
                                                   ⎢                            ⎥
                                                   ⎢ 3 −4         9 −4     3 −2 ⎥
                                                A=⎢⎢ −2
                                                                                ⎥
                                                   ⎢         3 −4 10 −4       3⎥⎥
                                                   ⎢                            ⎥
                                                   ⎣ 1 −2         3 −4 11 −4 ⎦
                                                        0    1 −2       3 −4 12

                          and the corresponding eigenvectors.
                      11.   Find the two smallest eigenvalues of the 6 × 6 Hilbert matrix
                                                    ⎡                              ⎤
                                                       1   1/2 1/3 · · · 1/6
                                                    ⎢ 1/2 1/3 1/4 · · · 1/7 ⎥
                                                    ⎢                              ⎥
                                                    ⎢                              ⎥
                                               A=⎢  ⎢ 1/3  1/4    1/5    · · · 1/7 ⎥
                                                                                   ⎥
                                                    ⎢ ..    ..     ..     ..    .. ⎥
                                                    ⎣ .      .      .        .   . ⎦
                                                             1/8       1/9   1/10   ···   1/11

                          Recall that this matrix is ill conditioned.
                      12.    Rewrite the function lamRange(d,c,N) so that it will bracket the N largest
                          eigenvalues of a tridiagonal matrix. Use this function to compute the two largest
                          eigenvalues of the Hilbert matrix in Example 9.11.
                      13.
                                                                  u1             u2           u3
                                                 k            k                 k            k
                                                         m              3m            2m


                            The differential equations of motion of the mass–spring system are

                                                               k (−2u1 + u2 ) = m¨
                                                                                 u1

                                                             k(u1 − 2u2 + u3 ) = 3m¨
                                                                                   u2

                                                                   k(u2 − 2u3 ) = 2m¨
                                                                                    u3

                            where ui (t ) is the displacement of mass i from its equilibrium position and k is
                            the spring stiffness. Substituting ui (t ) = yi sin ωt , we obtain the matrix eigenvalue
                            problem
                                              ⎡              ⎤⎡ ⎤                 ⎡          ⎤⎡ ⎤
                                                  2 −1     0       y1           2    1 0 0       y1
                                              ⎢              ⎥⎢ ⎥           mω    ⎢          ⎥⎢ ⎥
                                              ⎣ −1     2 −1 ⎦ ⎣ y2 ⎦ =            ⎣ 0 3 0 ⎦ ⎣ y2 ⎦
                                                                              k
                                                  0 −1     2       y3                0 0 2       y3
P1: PHB

CUUS884-Kiusalaas    CUUS884-09        978 0 521 19132 6                                                        December 16, 2009   15:4




           370      Symmetric Matrix Eigenvalue Problems

                          Determine the circular frequencies ω and the corresponding relative amplitudes
                          yi of vibration.
                    14.
                                                              u1            u2                           un
                                               k1            k2            k3  kn
                                                    m              m                             m


                          The figure shows n identical masses connected by springs of different stiffnesses.
                          The equation governing free vibration of the system is Au = mω2 u, where ω is the
                          circular frequency and

                                         ⎡                                                                             ⎤
                                        k1 + k2         −k 2           0                   0             ···        0
                                      ⎢                                                                                ⎥
                                      ⎢ −k 2           k2 + k3      −k 3                   0             ···        0 ⎥
                                      ⎢                                                                                ⎥
                                      ⎢ 0               −k 3       k3 + k4                −k 4           ···        0 ⎥
                                      ⎢                                                                                ⎥
                                    A=⎢    ..             ..          ..                  ..             ..         .. ⎥
                                      ⎢                                  .                   .              .        . ⎥
                                      ⎢     .              .                                                           ⎥
                                      ⎢                                                                                ⎥
                                      ⎣ 0                ···               0          −kn−1          kn−1 + kn     −kn ⎦
                                           0             ···               0            0              −kn         kn

                                                                                                 T
                          Given the spring stiffnesses k = k 1             k2       ···     kn       , write a program that com-
                          putes the N lowest eigenvalues λ = mω and the corresponding eigenvectors.
                                                                                2

                          Run the program with N = 4 and

                                                                                                            T
                                        k = 400        400    400 0.2               400 400          200        kN/m


                          Note that the system is weakly coupled, k 4 being small. Do the results make
                          sense?
                    15.
                                                                       L
                                                                                                           x
                                                 1 2                                                 n


                          The differential equation of motion of the axially vibrating bar is

                                                                                ρ
                                                                   u =            u¨
                                                                                E

                          where u(x, t ) is the axial displacement, ρ represents the mass density, and E is the
                          modulus of elasticity. The boundary conditions are u(0, t ) = u (L, t ) = 0. Letting
                          u(x, t ) = y(x) sin ωt , we obtain

                                                                 ρ
                                                    y = −ω2        y            y(0) = y (L) = 0
                                                                 E
P1: PHB

CUUS884-Kiusalaas    CUUS884-09     978 0 521 19132 6                                       December 16, 2009      15:4




               371     9.5 Eigenvalues of Symmetric Tridiagonal Matrices

                            The corresponding finite difference equations are
                                    ⎡                                 ⎤⎡     ⎤                       ⎡         ⎤
                                       2 −1       0     0 ···     0       y1                             y1
                                    ⎢                                 ⎥⎢     ⎥                       ⎢       ⎥
                                    ⎢ −1    2 −1        0 ···     0 ⎥ ⎢ y2 ⎥                         ⎢   y2  ⎥
                                    ⎢                                 ⎥⎢     ⎥                       ⎢       ⎥
                                    ⎢ 0 −1        2 −1 · · ·      0⎥    ⎢    ⎥                       ⎢       ⎥
                                                                      ⎥ ⎢ y3 ⎥                           y3
                                                                                               2
                                    ⎢                                                     ωL       ρ ⎢       ⎥
                                    ⎢ .     .   .     .     .      .  ⎥⎢ . ⎥ =                       ⎢    .. ⎥
                                    ⎢ ..    ..    ..    ..    ..   .. ⎥ ⎢ .. ⎥            n        E⎢        ⎥
                                    ⎢                                 ⎥⎢     ⎥                       ⎢     . ⎥
                                    ⎢                                 ⎥⎢     ⎥                       ⎢       ⎥
                                    ⎣ 0     0 · · · −1        2 −1 ⎦ ⎣ yn−1 ⎦                        ⎣ yn−1 ⎦
                                       0    0 ···       0 −1      1       yn                           yn /2

                            (a) If the standard form of these equations is Hz = λz, write down H and the
                            transformation matrix P in y = Pz. (b) Compute the lowest circular frequency of
                            the bar with n = 10, 100, and 1000 utilizing the module inversePower3. Note:
                                                             √
                            The analytical solution is ω1 = π E /ρ/ (2L).
                      16.
                                               u
                                         P               1    2           n -1 n           P x
                                                                      k
                                                                      L

                            The simply supported column is resting on an elastic foundation of stiffness k
                            (N/m per meter length). An axial force P acts on the column. The differential
                            equation and the boundary conditions for the lateral displacement u are

                                                                      P      k
                                                             u(4) +      u +    u=0
                                                                      EI     EI

                                                        u(0) = u (0) = u(L) = u (L) = 0

                            Using the mesh shown, the finite difference approximation of these equations is

                                                        (5 + α)u1 − 4u2 + u3 = λ(2u1 − u2 )

                                             −4u1 + (6 + α)u2 − 4u3 + u4 = λ(−u1 + 2u2 + u3 )

                                        u1 − 4u2 + (6 + α)u3 − 4u4 + u5 = λ(−u2 + 2u3 − u4 )
                                                                                   ..
                                                                                    .

                                      un−3 − 4un−2 + (6 + α)un−1 − 4un = λ(−un−2 + 4un−1 − un )

                                                un−2 − 4un−1 + (5 + α)un = λ(−un−1 + 2un)

                            where
                                               kh4      1     k L4                 Ph2      1     P L2
                                         α=        =                         λ=        =
                                               EI    (n + 1)4 E I                  EI    (n + 1)2 E I
                            Write a program that computes the lowest three buckling loads P and the corre-
                            sponding mode shapes. Run the program with k L 4 /(E I ) = 1000 and n = 25.
P1: PHB

CUUS884-Kiusalaas    CUUS884-09       978 0 521 19132 6                                                  December 16, 2009   15:4




           372      Symmetric Matrix Eigenvalue Problems

                    17.     Find the five smallest eigenvalues of the 20 × 20 matrix
                                                          ⎡                                      ⎤
                                                       2        1 0        0        ···    0 1
                                                     ⎢1                             ···    0 0⎥
                                                     ⎢          2 1        0                     ⎥
                                                     ⎢                                           ⎥
                                                     ⎢0         1 2        1        ···    0 0⎥
                                                     ⎢                                           ⎥
                                                     ⎢.         .. . .     ..       ..     .. .. ⎥
                                                 A = ⎢ ..        .     .        .      .    . . ⎥
                                                     ⎢                                           ⎥
                                                     ⎢                                           ⎥
                                                     ⎢0          0 ···     1        2      1 0⎥
                                                     ⎢                                           ⎥
                                                     ⎣0          0 ···     0        1      2 1⎦
                                                        1        0 ···     0        0      1 2

                          Note: this is a difficult matrix that has many pairs of double eigenvalues.
                    18.
                                                                                               z
                                                                  L
                                                                                    x
                                                                                                     y


                                                θ


                                                    P

                          When the depth/width ratio of a beam is large, lateral buckling may occur.
                          The differential equation that governs lateral buckling of the cantilever beam
                          shown is

                                                          d 2θ          x           2
                                                               + γ2 1 −                 θ =0
                                                          dx 2          L

                          where θ is the angle of rotation of the cross section and

                                                            P2 L2
                                                γ2 =
                                                          (G J )(E Iz )
                                               G J = torsional rigidity

                                               E Iz = bending rigidity about the z-axis

                          The boundary conditions are θ |x=0 = 0 and dθ /dx|x=L = 0. Using the finite dif-
                          ference approximation of the differential equation, determine the buckling load
                          Pcr . The analytical solution is

                                                                           (G J )(E Iz )
                                                           Pcr = 4.013
                                                                              L2
P1: PHB

CUUS884-Kiusalaas    CUUS884-09    978 0 521 19132 6                                 December 16, 2009    15:4




               373     9.6 Other Methods

              9.6     Other Methods

                      On occasions when all the eigenvalues and eigenvectors of a matrix are required, the
                      OR algorithm is a worthy contender. It is based on the decomposition A = QR, where
                      Q and R are orthogonal and upper-triangular matrices, respectively. The decompo-
                      sition is carried out in conjunction with the Householder transformation. There is
                      also a QL algorithm A = QL that works in the same manner, but here L is a lower
                      triangular matrix.
                           Schur’s factorization is another solid technique for determining the eigenvalues
                      of A. Here the decomposition is A = QT UQ, where Q is orthogonal and U is an upper
                      triangular matrix. The diagonal terms of U are the eigenvalues of A.
                           The LR algorithm is probably the fastest means of computing the eigenvalues; it
                      is also very simple to implement – see Prob. 22 of Problem Set 9.1. But its stability is
                      inferior to that of the other methods.
P1: PHB

CUUS884-Kiusalaas     CUUS884-10     978 0 521 19132 6                                   December 16, 2009       15:4




          10 Introduction to Optimization




                                   Find x that minimizes F (x) subject to g(x) = 0, h(x) ≥ 0




          10.1 Introduction

                    Optimization is the term often used for minimizing or maximizing a function. It is
                    sufficient to consider the problem of minimization only; maximization of F (x) is
                    achieved by simply minimizing −F (x). In engineering, optimization is closely related
                    to design. The function F (x), called the merit function or objective function, is the
                    quantity that we wish to keep as small as possible, such as the cost or weight. The
                    components of x, known as the design variables, are the quantities that we are free
                    to adjust. Physical dimensions (lengths, areas, angles, etc.) are common examples of
                    design variables.
                         Optimization is a large topic with many books dedicated to it. The best we can do
                    in limited space is to introduce a few basic methods that are good enough for prob-
                    lems that are reasonably well behaved and do not involve too many design variables.
                    By omitting the more sophisticated methods, we may actually not miss all that much.
                    All optimization algorithms are unreliable to a degree – any one of them may work on
                    one problem and fail on another. As a rule of the thumb, by going up in sophistication
                    we gain computational efficiency, but not necessarily reliability.
                         The algorithms for minimization are iterative procedures that require starting
                    values of the design variables x. If F (x) has several local minima, the initial choice of
                    x determines which of these will be computed. There is no guaranteed way of finding
                    the global optimal point. One suggested procedure is to make several computer runs
                    using different starting points and pick the best result.
                         More often than not, the design variables are also subjected to restrictions, or
                    constraints, which may have the form of equalities or inequalities. As an example,
                    take the minimum weight design of a roof truss that has to carry a certain loading.
                    Assume that the layout of the members is given, so that the design variables are the
                    cross-sectional areas of the members. Here the design is dominated by inequality

           374
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                               December 16, 2009      15:4




               375     10.1 Introduction

                      constraints that consist of prescribed upper limits on the stresses and possibly the
                      displacements.
                           The majority of available methods are designed for unconstrained optimization,
                      where no restrictions are placed on the design variables. In these problems, the min-
                      ima, if they exit, are stationary points (points where the gradient vector of F (x) van-
                      ishes). In the more difficult problem of constrained optimization, the minima are
                      usually located where the F (x) surface meets the constraints. There are special al-
                      gorithms for constrained optimization, but they are not easily accessible because of
                      their complexity and specialization. One way to tackle a problem with constraints is
                      to use an unconstrained optimization algorithm, but modify the merit function so
                      that any violation of constrains is heavily penalized.
                           Consider the problem of minimizing F (x) where the design variables are subject
                      to the constraints

                                                       gi (x) = 0, i = 1, 2, . . . , M

                                                       h j (x) ≤ 0,    j = 1, 2, . . . , N

                      We choose the new merit function be

                                                            F ∗ (x) = F (x) + µP(x)                                    (10.1a)

                      where
                                                        M                   N
                                                                                                   2
                                             P(x) =          [gi (x)]2 +          max 0, h j (x)                       (10.1b)
                                                       i=1                 j =1

                      is the penalty function and µ is a multiplier. The function max(a, b) returns the larger
                      of a and b. It is evident that P(x) = 0 if no constraints are violated. Violation of a
                      constraint imposes a penalty proportional to the square of the violation. Hence, the
                      minimization algorithm tends to avoid the violations, the degree of avoidance being
                      dependent on the magnitude of µ. If µ is small, optimization will proceed faster be-
                      cause there is more “space” in which the procedure can operate, but there may be
                      significant violation of constraints. On the other hand, large µ can result in a poorly
                      conditioned procedure, but the constraints will be tightly enforced. It is advisable to
                      run the optimization program with a µ that is on the small side. If the results show
                      unacceptable constraint violation, increase µ and run the program again, starting
                      with the results of the previous run.
                           An optimization procedure may also become ill conditioned when the con-
                      straints have widely different magnitudes. This problem can be alleviated by scaling
                      the offending constraints, that is, multiplying the constraint equations by suitable
                      constants.
                           It is not always necessary (or even advisable) to employ an iterative minimization
                      algorithm. In problems where the derivatives of F (x) can be readily computed and
                      inequality constraints are absent, the optimal point can always be found directly by
                      calculus. For example, if there are no constraints, the coordinates of the point where
                      F (x) is minimized are given by the solution of the simultaneous (usually nonlinear)
P1: PHB

CUUS884-Kiusalaas     CUUS884-10      978 0 521 19132 6                                          December 16, 2009      15:4




           376      Introduction to Optimization

                    equations ∇ F (x) = 0. The direct method for finding the minimum of F (x) subject to
                    equality constraints gi (x) = 0, i = 1, 2, . . . , m is to form the function
                                                                           m
                                                    F ∗ (x, λ) = F (x) +         λi gi (x)                    (10.2a)
                                                                           i=1

                    and solve the equations

                                            ∇ F ∗ (x) = 0     gi (x) = 0, i = 1, 2, . . . , m                 (10.2b)

                    for x and λi . The parameters λi are known as the Lagrangian multipliers. The direct
                    method can also be extended to inequality constraints, but the solution of the result-
                    ing equations is not straightforward because of lack of uniqueness.



          10.2 Minimization along a Line

                    Consider the problem of minimizing a function f (x) of a single variable x with the
                    constraints c ≤ x ≤ d. A hypothetical plot of the function is shown in Fig. 10.1. There
                    are two minimum points: a stationary point characterized by f (x) = 0 that repre-
                    sents a local minimum, and a global minimum at the constraint boundary. It appears
                    that finding the global minimum is simple. All the stationary points could be located
                    by finding the roots of df/dx = 0, and each constraint boundary may be checked for
                    a global minimum by evaluating f (c) and f (d). Then why do we need an optimization
                    algorithm? We need it if f (x) is difficult or impossible to differentiate – for example, if
                    f represents a complex computer algorithm.


                    Bracketing
                    Before a minimization algorithm can be entered, the minimum point must be brack-
                    eted. The procedure of bracketing is simple: start with an initial value of x0 and move
                    downhill computing the function at x1 , x2 , x3 , . . . until we reach the point xn where
                    f (x) increases for the first time. The minimum point is now bracketed in the interval
                    (xn−2 , xn ). What should the step size hi = xi+1 − xi be? It is not a good idea to have a


                                    f (x)
                                                                  Local minimum


                                                             Global minimum
                                                    Constraint boundaries
                                                                                                   x
                                         c                                                   d
                                    Figure 10.1. Example of local and global minima.
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                        December 16, 2009   15:4




               377     10.2 Minimization along a Line


                                    f (x )
                                                                2Rh - h
                                                           f1            f2
                                                                               Rh
                                                        Rh
                                                                                                    x
                                       a                      x1           x2                  b
                                                                    h
                                                                   (a)

                                    f (x )


                                                                                      Rh'
                                                                     Rh'
                                                                                                    x
                                                          a                x1        x2         b
                                                                                h'
                                                                   (b)
                                    Figure 10.2. Golden section telescoping.



                      constant hi , because it often results in too many steps. A more efficient scheme is to
                      increase the size with every step, the goal being to reach the minimum quickly, even
                      if its resulting bracket is wide. In our algorithm we chose to increase the step size by
                      a constant factor, that is, we use hi+1 = chi , c > 1.



                      Golden Section Search
                      The golden section search is the counterpart of bisection used in finding roots of
                      equations. Suppose that the minimum of f (x) has been bracketed in the interval
                      (a, b) of length h. To telescope the interval, we evaluate the function at x1 = b − Rh
                      and x2 = a + Rh, as shown in Fig. 10.2(a). The constant R is to be determined shortly.
                      If f1 > f2 as indicated in the figure, the minimum lies in (x1 , b); otherwise, it is located
                      in (a, x2 ).
                            Assuming that f1 > f2 , we set a ← x1 and x1 ← x2 , which yields a new interval
                      (a, b) of length h = Rh, as illustrated in Fig. 10.2(b). To carry out the next telescoping
                      operation, we evaluate the function at x2 = a + Rh and repeat the process.
                            The procedure works only if Figs. 10.1(a) and (b) are similar, that is, if the same
                      constant R locates x1 and x2 in both figures. Referring to Fig. 10.2(a), we note that
                      x2 − x1 = 2Rh − h. The same distance in Fig. 10.2(b) is x1 − a = h − Rh . Equating
                      the two, we get

                                                          2Rh − h = h − Rh
P1: PHB

CUUS884-Kiusalaas       CUUS884-10        978 0 521 19132 6                                          December 16, 2009          15:4




           378      Introduction to Optimization

                    Substituting h = Rh and cancelling h yields

                                                              2R − 1 = R(1 − R)

                    the solution of which is the golden ratio.1
                                                         √
                                                    −1 + 5
                                               R=             = 0.618 033 989 . . .                                   (10.3)
                                                       2
                         Note that each telescoping decreases the interval containing the minimum by
                    the factor R, which is not as good as the factor is 0.5 in bisection. However, the golden
                    search method achieves this reduction with one function evaluation, whereas two
                    evaluations would be needed in bisection.
                         The number of telescopings required to reduce h from b − a to an error toler-
                    ance ε is given by

                                                                 b − a Rn = ε

                    which yields
                                                     ln(ε/ b − a )                   ε
                                               n=                  = −2.078 087 ln                                    (10.4)
                                                          ln R                     b −a


                        goldSearch

                    This module contains the bracketing and the golden section search algorithms. For
                    the factor that multiplies successive search intervals in bracket, we chose c = 1 + R.

                    ## module goldSearch
                    ’’’ a,b = bracket(f,xStart,h)
                           Finds the brackets (a,b) of a minimum point of the
                           user-supplied scalar function f(x).
                           The search starts downhill from xStart with a step
                           length h.


                           x,fMin = search(f,a,b,tol=1.0e-6)
                           Golden section method for determining x that minimizes
                           the user-supplied scalar function f(x).
                           The minimum must be bracketed in (a,b).
                    ’’’
                    from math import log


                    def bracket(f,x1,h):
                           c = 1.618033989
                           f1 = f(x1)
                           x2 = x1 + h; f2 = f(x2)

                    1   R is the ratio of the sides of a “golden rectangle,” considered by ancient Greeks to have the perfect
                        proportions.
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                December 16, 2009     15:4




               379     10.2 Minimization along a Line

                         # Determine downhill direction and change sign of h if needed
                           if f2 > f1:
                                  h = -h
                                  x2 = x1 + h; f2 = f(x2)
                             # Check if minimum between x1 - h and x1 + h
                                  if f2 > f1: return x2,x1 - h
                         # Search loop
                           for i in range (100):
                                  h = c*h
                                  x3 = x2 + h; f3 = f(x3)
                                  if f3 > f2: return x1,x3
                                  x1 = x2; x2 = x3
                                  f1 = f2; f2 = f3
                           print ’’Bracket did not find a minimum’’


                      def search(f,a,b,tol=1.0e-9):
                           nIter = -2.078087*log(tol/abs(b-a)) # Eq. (10.4)
                           R = 0.618033989
                           C = 1.0 - R
                         # First telescoping
                           x1 = R*a + C*b; x2 = C*a + R*b
                           f1 = f(x1); f2 = f(x2)
                         # Main loop
                           for i in range(nIter):
                                  if f1 > f2:
                                      a = x1
                                      x1 = x2; f1 = f2
                                      x2 = C*a + R*b; f2 = f(x2)
                                  else:
                                      b = x2
                                      x2 = x1; f2 = f1
                                      x1 = R*a + C*b; f1 = f(x1)
                           if f1 < f2: return x1,f1
                           else: return x2,f2

                      EXAMPLE 10.1
                      Use goldSearch to find x that minimizes

                                                       f (x) = 1.6x 3 + 3x 2 − 2x

                      subject to the constraint x ≥ 0. Compare the result with the analytical solution.

                      Solution This is a constrained minimization problem. Either the minimum of f (x)
                      is a stationary point in x ≥ 0, or it is located at the constraint boundary x = 0.
                      We handle the constraint with the penalty function method by minimizing f (x) +
                                   2
                      µ min(0, x) .
P1: PHB

CUUS884-Kiusalaas    CUUS884-10         978 0 521 19132 6                              December 16, 2009       15:4




           380      Introduction to Optimization

                        Starting at x = 1 and choosing h = 0.01 for the first step size in bracket (both
                    choices being rather arbitrary), we arrive at the following program:

                    #!/usr/bin/python
                    ## example10_1
                    from goldSearch import *


                    def f(x):
                         mu = 1.0                  # Constraint multiplier
                         c = min(0.0, x)           # Constraint function
                         return 1.6*x**3 + 3.0*x**2 - 2.0*x + mu*c**2


                    xStart = 1.0
                    h = 0.01
                    x1,x2 = bracket(f,xStart,h)
                    x,fMin = search(f,x1,x2)
                    print ’’x =’’,x
                    print ’’f(x) =’’,fMin
                    raw_input (’’\nPress return to exit’’)

                        The result is

                    x = 0.27349402621
                    f(x) = -0.28985978555

                        Because the minimum was found to be a stationary point, the constraint was not
                    active. Therefore, the penalty function was superfluous, but we did not know that at
                    the beginning.
                        The locations of stationary points are obtained analytically by solving

                                                      f (x) = 4.8x 2 + 6x − 2 = 0

                    The positive root of this equation is x = 0.273 49 4. As this is the only positive root,
                    there are no other stationary points in x ≥ 0 that we must check out. The only other
                    possible location of a minimum is the constraint boundary x = 0. But here f (0) = 0
                    is larger than the function at the stationary point, leading to the conclusion that the
                    global minimum occurs at x = 0.273 49 4.

                    EXAMPLE 10.2
                    The trapezoid shown is the cross section of a beam. It is formed by removing the top
                    from a triangle of base B = 48 mm and height H = 60 mm. The problem is to find the
                    height y of the trapezoid that maximizes the section modulus

                                                               S = Ix¯ /c

                    where Ix¯ is the second moment of the cross-sectional area about the axis that passes
                    through the centroid C of the cross section. By optimizing the section modulus, we
P1: PHB

CUUS884-Kiusalaas    CUUS884-10   978 0 521 19132 6                               December 16, 2009   15:4




               381     10.2 Minimization along a Line




                                       H                                            c
                                             y                                          _
                                                                                        x
                                                              C
                                                                                    d
                                                                                        x
                                                      b         a          b
                                                                B

                      minimize the maximum bending stress σ max = M/S in the beam, M being the bend-
                      ing moment.

                      Solution Considering the area of the trapezoid as a composite of a rectangle and two
                      triangles, the section modulus is found through the following sequence of computa-
                      tions:

                              Base of rectangle                      a = B (H − y) /H
                              Base of triangle                       b = (B − a) /2
                              Area                                   A = (B + a) y/2
                              First moment of area about x-axis      Q x = (ay) y/2 + 2 by/2 y/3
                              Location of centroid                   d = Q x /A
                              Distance involved in S                 c = y −d
                              Second moment of area about x-axis     Ix = ay 3 /3 + 2 by 3 /12
                              Parallel axis theorem                  Ix¯ = Ix − Ad 2
                              Section modulus                        S = Ix¯ /c

                      We could use the formulas in the table to derive S as an explicit function of y, but
                      that would involve a lot of error-prone algebra and result in an overly complicated
                      expression. It makes more sense to let the computer do the work.
                           The program we used and its output are listed next. As we wish to maximize S
                      with a minimization algorithm, the merit function is −S. There are no constraints in
                      this problem.

                      #!/usr/bin/python
                      ## example10_2
                      from goldSearch import *


                      def f(y):
                           B = 48.0
                           H = 60.0
                           a = B*(H - y)/H
P1: PHB

CUUS884-Kiusalaas    CUUS884-10       978 0 521 19132 6                               December 16, 2009       15:4




           382      Introduction to Optimization

                         b = (B - a)/2.0
                         A = (B + a)*y/2.0
                         Q = (a*y**2)/2.0 + (b*y**2)/3.0
                         d = Q/A
                         c = y - d
                         I = (a*y**3)/3.0 + (b*y**3)/6.0
                         Ibar = I - A*d**2
                         return -Ibar/c


                    yStart = 60.0       # Starting value of y
                    h = 1.0             # Size of first step used in bracketing
                    a,b = bracket(f,yStart,h)
                    yOpt,fOpt = search(f,a,b)
                    print ’’Optimal y =’’,yOpt
                    print ’’Optimal S =’’,-fOpt
                    print ’’S of triangle =’’,-f(60.0)
                    raw_input(’’Press return to exit’’)


                    Optimal y = 52.1762738732
                    Optimal S = 7864.43094136
                    S of triangle = 7200.0


                        The printout includes the section modulus of the original triangle. The optimal
                    section shows a 9.2% improvement over the triangle.



          10.3 Powell’s Method
                    Introduction
                    We now look at optimization in n-dimensional design space. The objective is to min-
                    imize F (x), where the components of x are the n independent design variables. One
                    way to tackle the problem is to use a succession of one-dimensional minimizations
                    to close in on the optimal point. The basic strategy is

                     • Choose a point x0 in the design space.
                     • Loop with i = 1, 2, 3, . . .

                      Choose a vector vi .
                          Minimize F (x) along the line through xi−1 in the direction of vi . Let the mini-
                              mum point be xi .
                          if |xi − xi−1 | < ε exit loop
                          xi ← xi+1

                     • end loop
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                                     December 16, 2009     15:4




               383     10.3 Powell’s Method

                          The minimization along a line can be accomplished with any one-dimensional
                      optimization algorithm (such as the golden section search). The only question left
                      open is how to choose the vectors vi .



                      Conjugate Directions
                      Consider the quadratic function

                                                                            1
                                               F (x) = c −        bi xi +                  A ij xi x j
                                                                            2
                                                              i                 i     j

                                                                      1 T
                                                       = c − bT x +     x Ax                                                 (10.5)
                                                                      2

                      Differentiation with respect to xi yields

                                                          ∂F
                                                              = −bi +           A ij x j
                                                          ∂xi
                                                                            j


                      which can be written in vector notation as

                                                             ∇ F = −b + Ax                                                   (10.6)

                      where ∇ F is called the gradient of F .
                          Now consider the change in the gradient as we move from point x0 in the direc-
                      tion of a vector u. The motion takes place along the line

                                                              x = x0 + su

                      where s is the distance moved. Substitution into Eq. (10.6) yields the expression for
                      the gradient at x:

                                           ∇ F |x0 +su = −b + A (x0 + su) = ∇ F |x0 + s Au

                      Note that the change in the gradient is s Au. If this change is perpendicular to a vector
                      v, that is, if

                                                                  vT Au = 0                                                  (10.7)

                      the directions of u and v are said to be mutually conjugate (noninterfering). The im-
                      plication is that once we have minimized F (x) in the direction of v, we can move
                      along u without ruining the previous minimization.
                          For a quadratic function of n independent variables it is possible to construct
                      n mutually conjugate directions. Therefore, it would take precisely n line mini-
                      mizations along these directions to reach the minimum point. If F (x) is not a
                      quadratic function, Eq. (10.5) can be treated as a local approximation of the merit
P1: PHB

CUUS884-Kiusalaas     CUUS884-10      978 0 521 19132 6                                           December 16, 2009   15:4




           384      Introduction to Optimization

                    function, obtained by truncating the Taylor series expansion of F (x) about x0 (see
                    Appendix A1):
                                                                             1
                                    F (x) ≈ F (x0 ) + ∇ F (x0 )(x − x0 ) +     (x − x0 )T H(x0 )(x − x0 )
                                                                             2
                    Now the conjugate directions based on the quadratic form are only approximations,
                    valid in the close vicinity of x0 . Consequently, it would take several cycles of n line
                    minimizations to reach the optimal point.
                         The various conjugate gradient methods use different techniques for construct-
                    ing conjugate directions. The zero-order methods work with F (x) only, whereas the
                    first-order methods utilize both F (x) and ∇ F . The first-order methods are computa-
                    tionally more efficient, of course, but the input of ∇ F , if it is available at all, can be
                    very tedious.


                    Powell’s Algorithm
                    Powell’s method is a zero-order method, requiring the evaluation of F (x) only. The
                    basic algorithm is

                     • Choose a point x0 in the design space.
                     • Choose the starting vectors vi , 1, 2, . . . , n (the usual choice is vi = ei , where ei is
                       the unit vector in the xi -coordinate direction).
                     • cycle

                           do with i = 1, 2, . . . , n
                                Minimize F (x) along the line through xi−1 in the direction of vi . Let the
                                     minimum point be xi .
                           end do
                           vn+1 ← x0 − xn
                           Minimize F (x) along the line through x0 in the direction of vn+1 . Let the mini-
                               mum point be xn+1 .
                           if |xn+1 − x0 | < ε exit loop
                           do with i = 1, 2, . . . , n
                                vi ← vi+1 (v1 is discarded, the other vectors are reused)
                           end do

                     • end cycle

                         Powell demonstrated that the vectors vn+1 produced in successive cycles are mu-
                    tually conjugate, so that the minimum point of a quadratic surface is reached in pre-
                    cisely n cycles. In practice, the merit function is seldom quadratic, but as long as it can
                    be approximated locally by Eq. (10.5), Powell’s method will work. Of course, it usually
                    takes more than n cycles to arrive at the minimum of a nonquadratic function. Note
                    that it takes n line minimizations to construct each conjugate direction.
P1: PHB

CUUS884-Kiusalaas    CUUS884-10      978 0 521 19132 6                                     December 16, 2009    15:4




               385     10.3 Powell’s Method


                                        P0 (x0)                                                      P0



                                             s1v1
                                     s3v3
                             v3
                                                  P1 (x1)                                  P5              P1
                                                                                 P6
                                   P3(x3)                                                       P3
                                                    v1                                P4
                                         s2v2
                                                                                        P2
                                  P2 (x2)
                        v2
                                  (a)                                             (b)


                        Figure 10.3. The method of Powell



                           Figure 10.3(a) illustrates one typical cycle of the method in a two-dimensional
                      design space (n = 2). We start with point x0 and vectors v1 and v2 . Then we find the
                      distance s 1 that minimizes F (x0 + sv1 ), finishing up at point x1 = x0 + s 1 v1 . Next, we
                      determine s 2 that minimizes F (x1 + sv2 ), which takes us to x2 = x1 + s 2 v2 . The last
                      search direction is v3 = x2 − x0 . After finding s 3 by minimizing F (x0 + sv3 ), we get to
                      x3 = x0 + s 3 v3 , completing the cycle.
                           Figure 10.3(b) shows the moves carried out in two cycles superimposed on the
                      contour map of a quadratic surface. As explained before, the first cycle starts at point
                      P0 and ends up at P3 . The second cycle takes us to P6 , which is the optimal point. The
                      directions P0 P3 and P3 P6 are mutually conjugate.
                           Powell’s method does have a major flaw that has to be remedied – if F (x) is not
                      a quadratic, the algorithm tends to produce search directions that gradually become
                      linearly dependent, thereby ruining the progress toward the minimum. The source
                      of the problem is the automatic discarding of v1 at the end of each cycle. It has been
                      suggested that it is better to throw out the direction that resulted in the largest de-
                      crease of F (x), a policy that we adopt. It seems counterintuitive to discard the best
                      direction, but it is likely to be close to the direction added in the next cycle, thereby
                      contributing to linear dependence. As a result of the change, the search directions
                      cease to be mutually conjugate, so that a quadratic form is not minimized in n cy-
                      cles any more. This is not a significant loss because in practice F (x) is seldom a
                      quadratic.
                           Powell suggested a few other refinements to speed up convergence. Because they
                      complicate the book keeping considerably, we did not implement them.
P1: PHB

CUUS884-Kiusalaas    CUUS884-10      978 0 521 19132 6                                  December 16, 2009       15:4




           386      Introduction to Optimization

                      powell

                    The algorithm for Powell’s method is listed here. It utilizes two arrays: df contains the
                    decreases of the merit function in the first n moves of a cycle, and the matrix u stores
                    the corresponding direction vectors vi (one vector per row).

                    ## module powell
                    ’’’ xMin,nCyc = powell(F,x,h=0.1,tol=1.0e-6)
                          Powell’s method of minimizing user-supplied function F(x).
                          x     = starting point
                          h    = initial search increment used in ’bracket’
                          xMin = mimimum point
                          nCyc = number of cycles
                    ’’’
                    from numpy import identity,array,dot,zeros,argmax
                    from goldSearch import *
                    from math import sqrt


                    def powell(F,x,h=0.1,tol=1.0e-6):


                          def f(s): return F(x + s*v)             # F in direction of v


                          n = len(x)                              # Number of design variables
                          df = zeros(n)                           # Decreases of F stored here
                          u = identity(n)                         # Vectors v stored here by rows
                          for j in range(30):                     # Allow for 30 cycles:
                               xOld = x.copy()                    # Save starting point
                               fOld = F(xOld)
                              # First n line searches record decreases of F
                               for i in range(n):
                                    v = u[i]
                                    a,b = bracket(f,0.0,h)
                                    s,fMin = search(f,a,
                                    df[i] = fOld - fMin
                                    fOld = fMin
                                    x = x + s*v
                              # Last line search in the cycle
                               v = x - xOld
                               a,b = bracket(f,0.0,h)
                               s,fLast = search(f,a,b)
                               x = x + s*v
                              # Check for convergence
                               if sqrt(dot(x-xOld,x-xOld)/n) < tol: return x,j+1
                              # Identify biggest decrease & update search directions
                               iMax = argmax(df)
P1: PHB

CUUS884-Kiusalaas    CUUS884-10       978 0 521 19132 6                                       December 16, 2009     15:4




               387     10.3 Powell’s Method

                                   for i in range(iMax,n-1):
                                         u[i] = u[i+1]
                                   u[n-1] = v
                             print "Powell did not converge"

                      EXAMPLE 10.3
                      Find the minimum of the function2


                                                          F = 100(y − x 2 )2 + (1 − x)2

                      with Powell’s method starting at the point (−1, 1). This function has an interesting
                      topology. The minimum value of F occurs at the point (1, 1). As seen in the figure,
                      there is a hump between the starting and minimum points that the algorithm must
                      negotiate.


                                        1000

                                    z     500

                                               0

                                                   1


                                                          0                                          1
                                                   y                                      0
                                                                -1           -1               x
                      Solution The program that solves this unconstrained optimization problem is

                      #!/usr/bin/python
                      ## example10_3
                      from powell import *
                      from numpy import array


                      def F(x): return 100.0*(x[1] - x[0]**2)**2 + (1 - x[0])**2


                      xStart = array([-1.0, 1.0])
                      xMin,nIter = powell(F,xStart)

                      2   From T. E. Shoup and F. Mistree, Optimization Methods with Applications for Personal Computers
                          (Prentice-Hall, 1987).
P1: PHB

CUUS884-Kiusalaas    CUUS884-10         978 0 521 19132 6                              December 16, 2009       15:4




           388      Introduction to Optimization

                    print ’’x =’’,xMin
                    print ’’F(x) =’’,F(xMin)
                    print ’’Number of cycles =’’,nIter
                    raw_input (’’Press return to exit’’)

                        As seen in the printout, the minimum point was obtained after 12 cycles.

                    x = [ 1.      1.]
                    F(x) = 3.71750701585e-029
                    Number of cycles = 12
                    Press return to exit

                    EXAMPLE 10.4
                    Use powell to determine the smallest distance from the point (5, 8) to the curve
                    xy = 5.

                    Solution This is a constrained optimization problem: minimize F (x, y) = (x − 5)2 +
                    (y − 8)2 (the square of the distance) subject to the equality constraint xy − 5 = 0. The
                    following program uses Powell’s method with penalty function:

                    #!/usr/bin/python
                    ## example10_4
                    from powell import *
                    from numpy import array
                    from math import sqrt


                    def F(x):
                         mu = 1.0                                 # Penalty multiplier
                         c = x[0]*x[1] - 5.0                      # Constraint equation
                         return     distSq(x) + mu*c**2           # Penalized merit function


                    def distSq(x): return (x[0] - 5)**2 + (x[1] - 8)**2


                    xStart = array([1.0, 5.0])
                    x,numIter = powell(F,xStart,0.01)
                    print ’’Intersection point =’’,x
                    print ’’Minimum distance =’’, sqrt(distSq(x))
                    print ’’xy =’’, x[0]*x[1]
                    print ’’Number of cycles =’’,numIter
                    raw_input (’’Press return to exit’’)

                        As mentioned before, the value of the penalty function multiplier µ (called mu in
                    the program) can have profound effect on the result. We chose µ = 1 (as in the pro-
                    gram listing) with the following result:

                    Intersection point = [ 0.73306761             7.58776385]
                    Minimum distance = 4.28679958767
P1: PHB

CUUS884-Kiusalaas    CUUS884-10   978 0 521 19132 6                                   December 16, 2009   15:4




               389     10.3 Powell’s Method

                      xy = 5.56234387462
                      Number of cycles = 5

                          The small value of µ favored speed of convergence over accuracy. Because the
                      violation of the constraint xy = 5 is clearly unacceptable, we ran the program again
                      with µ = 10 000 and changed the starting point to (0.73306761, 7.58776385), the end
                      point of the first run. The results shown next are now acceptable:

                      Intersection point = [ 0.65561311             7.62653592]
                      Minimum distance = 4.36040970945
                      xy = 5.00005696357
                      Number of cycles = 5

                           Could we have used µ = 10 000 in the first run? In this case, we would be lucky
                      and obtain the minimum in 17 cycles. Hence, we save seven cycles by using two runs.
                      However, a large µ often causes the algorithm to hang up, so that it is generally wise
                      to start with a small µ.

                      Check
                      Because we have an equality constraint, the optimal point can readily be found by
                      calculus. The function in Eq. (10.2a) is (here λ is the Lagrangian multiplier)


                                           F ∗ (x, y, λ) = (x − 5)2 + (y − 8)2 + λ(xy − 5)

                      so that Eqs. (10.2b) become
                                                      ∂F∗
                                                          = 2(x − 5) + λy = 0
                                                      ∂x
                                                      ∂F∗
                                                          = 2(y − 8) + λx = 0
                                                       ∂y
                                                      g(x) = xy − 5 = 0

                      which can be solved with the Newton–Raphson method (the function newtonRaph-
                                                                                                              T
                      son2 in Section 4.6). In the following program we used the notation x   = x     y   λ       .

                      ## example10_4_check
                      from numpy import array
                      from newtonRaphson2 import *


                      def F(x):
                           return array([2.0*(x[0] - 5.0) + x[2]*x[1],                  \
                                              2.0*(x[1] - 8.0) + x[2]*x[0],             \
                                              x[0]*x[1] - 5.0])


                      xStart = array([1.0, 5.0, 1.0])
                      print "x = ", newtonRaphson2(F,xStart)
                      raw_input (’’Press return to exit’’)
P1: PHB

CUUS884-Kiusalaas    CUUS884-10           978 0 521 19132 6                                         December 16, 2009         15:4




           390      Introduction to Optimization

                          The result is

                    x =     [ 0.6556053           7.62653992           1.13928328]

                    EXAMPLE 10.5

                                                         u3



                                                     L 1                   3


                                                                       2
                                                                                          u2
                                                                                               u1
                                                                       L
                                                                                          P

                        The displacement formulation of the truss shown results in the following simul-
                    taneous equations for the joint displacements u:
                                     ⎡ √                                 ⎤⎡ ⎤ ⎡           ⎤
                                        2 2A 2 + A 3 −A 3        A3          u1        0
                                  E ⎢                                    ⎥⎢ ⎥ ⎢           ⎥
                                 √   ⎣      −A 3        A3      −A 3     ⎦ ⎣ u2 ⎦ = ⎣ −P ⎦
                               2 2L                           √
                                             A3        −A 3 2 2A 1 + A 3     u3        0

                    where E represents the modulus of elasticity of the material and A i is the cross-
                    sectional area of member i. Use Powell’s method to minimize the structural volume
                    (i.e., the weight) of the truss while keeping the displacement u2 below a given value δ.

                    Solution Introducing the dimensionless variables
                                                                 ui               Eδ
                                                          vi =             xi =      Ai
                                                                 δ                PL
                    the equations become
                                      ⎡ √                                          ⎤⎡ ⎤ ⎡          ⎤
                                       2 2x2 + x3             −x3           x3         v1        0
                                   1 ⎢                                             ⎥⎢ ⎥ ⎢          ⎥
                                   √ ⎣    −x3                  x3          −x3     ⎦ ⎣ v2 ⎦ = ⎣ −1 ⎦                    (a)
                                  2 2                                    √
                                           x3                 −x3       2 2x1 + x3     v3        0

                    The structural volume to be minimized is
                                                                 √              P L2           √
                                           V = L(A 1 + A 2 +         2A 3 ) =        (x1 + x2 + 2x3 )
                                                                                 Eδ
                    In addition to the displacement constraint |u2 | ≤ δ, we should also prevent the cross-
                    sectional areas from becoming negative by applying the constraints A i ≥ 0. Thus, the
                    optimization problem becomes: Minimize
                                                                     √
                                                     F = x1 + x2 + 2x3
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                      December 16, 2009   15:4




               391     10.3 Powell’s Method

                      with the inequality constraints

                                                   |v2 | ≤ 1   xi ≥ 0 (i = 1, 2, 3)

                      Note that in order to obtain v2 we must solve Eqs. (a).
                         Here is the program:

                      #!/usr/bin/python
                      ## example10_5
                      from powell import *
                      from numpy import array
                      from math import sqrt
                      from gaussElimin import *


                      def F(x):
                            global v, weight
                            mu = 100.0
                            c = 2.0*sqrt(2.0)
                            A = array([[c*x[1] + x[2], -x[2],             x[2]],                         \
                                          [-x[2],               x[2], -x[2]],                            \
                                          [ x[2],              -x[2],     c*x[0] + x[2]]])/c
                            b = array([0.0, -1.0, 0.0])
                            v = gaussElimin(A,b)
                            weight = x[0] + x[1] + sqrt(2.0)*x[2]
                            penalty = max(0.0,abs(v[1]) - 1.0)**2                \
                                     + max(0.0,-x[0])**2                              \
                                     + max(0.0,-x[1])**2                              \
                                     + max(0.0,-x[2])**2
                            return weight + penalty*mu


                      xStart = array([1.0, 1.0, 1.0])
                      x,numIter = powell(F,xStart)
                      print "x = ",x
                      print "v = ",v
                      print "Relative weight F = ",weight
                      print "Number of cycles = ",numIter
                      raw_input ("Press return to exit")

                                                                                          T
                          The first run of the program started with x = 1       1     1       and used µ = 100 for
                      the penalty multiplier. The results were

                      x =    [ 3.73870376       3.73870366     5.28732564]
                      v =    [-0.26747239 -1.06988953 -0.26747238]
                      Relative weight F =          14.9548150471
                      Number of cycles =         10
P1: PHB

CUUS884-Kiusalaas     CUUS884-10      978 0 521 19132 6                                          December 16, 2009   15:4




           392      Introduction to Optimization

                        Because the magnitude of v2 is excessive, the penalty multiplier was increased to
                    10,000 and the program run again using the output x from the last run as the input.
                    As seen next, v2 is now much closer to the constraint value.

                    x =    [ 3.99680758        3.9968077         5.65233961]
                    v =    [-0.25019968 -1.00079872 -0.25019969]
                    Relative weight F =           15.9872306185
                    Number of cycles =          11

                        In this problem, the use of µ = 10,000 at the outset would not work. You are in-
                    vited to try it.


          10.4 Downhill Simplex Method

                    The downhill simplex method is also known as the Nelder–Mead method. The idea
                    is to employ a moving simplex in the design space to surround the optimal point
                    and then shrink the simplex until its dimensions reach a specified error tolerance.
                    In n-dimensional space, a simplex is a figure of n + 1 vertices connected by straight
                    lines and bounded by polygonal faces. If n = 2, a simplex is a triangle; if n = 3, it is a
                    tetrahedron.
                         The allowed moves of the simplex are illustrated in Fig. 10.4 for n = 2. By applying
                    these moves in a suitable sequence, the simplex can always hunt down the minimum
                    point, enclose it, and then shrink around it. The direction of a move is determined by
                    the values of F (x) (the function to be minimized) at the vertices. The vertex with the
                    highest value of F is labeled Hi, and Lo denotes the vertex with the lowest value. The


                          Hi                              Hi                            Hi

                               d
                                                                  2d
                                                                                                       3d
                      Original simplex

                                                               Reflection

                          Hi
                                   0.5d
                                                                                                  Expansion

                                                       Lo
                          Contraction                       Shrinkage
                      Figure 10.4. A simplex in two dimensions illustrating the allowed moves.
P1: PHB

CUUS884-Kiusalaas    CUUS884-10     978 0 521 19132 6                                       December 16, 2009      15:4




               393     10.4 Downhill Simplex Method

                      magnitude of a move is controlled by the distance d measured from the Hi vertex
                      to the centroid of the opposing face (in the case of the triangle, the middle of the
                      opposing side).
                          The outline of the algorithm is:

                        • Choose a starting simplex.
                        • Cycle until d ≤ ε (ε being the error tolerance)
                          – Try reflection.
                            ∗ If the new vertex is lower than previous Hi, accept reflection.
                            ∗ If the new vertex is lower than previous Lo, try expansion.
                            ∗ If the new vertex is lower than previous Lo, accept expansion.
                            ∗ If reflection is accepted, start next cycle.
                          – Try contraction.
                            ∗ If the new vertex is lower than Hi, accept contraction and start next cycle.
                          – Shrinkage.
                        • end cycle

                          The downhill simplex algorithm is much slower than Powell’s method in most
                      cases, but makes up for it in robustness. It often works in problems where Powell’s
                      method hangs up.


                         downhill

                      The implementation of the downhill simplex method is given here. The starting sim-
                      plex has one of its vertices at x0 and the others at x0 + ei b (i = 1, 2, . . . , n), where ei is
                      the unit vector in the direction of the xi -coordinate. The vector x0 (called xStart in
                      the program) and the edge length b of the simplex are input by the user.

                      ## module downhill
                      ’’’ x = downhill(F,xStart,side=0.1,tol=1.0e-6)
                            Downhill simplex method for minimizing the user-supplied
                            scalar function F(x) with respect to the vector x.
                            xStart = starting vector x.
                            side      = side length of the starting simplex (default = 0.1).
                      ’’’
                      from numpy import zeros,dot,argmax,argmin,sum
                      from math import sqrt


                      def downhill(F,xStart,side,tol=1.0e-6):
                            n = len(xStart)                               # Number of variables
                            x = zeros((n+1,n))
                            f = zeros(n+1)


                         # Generate starting simplex
                            x[0] = xStart
P1: PHB

CUUS884-Kiusalaas    CUUS884-10      978 0 521 19132 6                          December 16, 2009   15:4




           394      Introduction to Optimization

                        for i in range(1,n+1):
                             x[i] = xStart
                             x[i,i-1] = xStart[i-1] + side
                     # Compute values of F at the vertices of the simplex
                        for i in range(n+1): f[i] = F(x[i])


                     # Main loop
                        for k in range(500):
                          # Find highest and lowest vertices
                             iLo = argmin(f)
                             iHi = argmax(f)
                          # Compute the move vector d
                             d = (-(n+1)*x[iHi] + sum(x))/n
                          # Check for convergence
                             if sqrt(dot(d,d)/n) < tol: return x[iLo]


                          # Try reflection
                             xNew = x[iHi] + 2.0*d
                             fNew = F(xNew)
                             if fNew <= f[iLo]:                # Accept reflection
                                  x[iHi] = xNew
                                  f[iHi] = fNew
                               # Try expanding the reflection
                                  xNew = x[iHi] + d
                                  fNew = F(xNew)
                                  if fNew <= f[iLo]:           # Accept expansion
                                         x[iHi] = xNew
                                         f[iHi] = fNew
                             else:
                               # Try reflection again
                                  if fNew <= f[iHi]:           # Accept reflection
                                         x[iHi] = xNew
                                         f[iHi] = fNew
                                  else:
                                      # Try contraction
                                         xNew = x[iHi] + 0.5*d
                                         fNew = F(xNew)
                                         if fNew <= f[iHi]: # Accept contraction
                                               x[iHi] = xNew
                                               f[iHi] = fNew
                                         else:
                                            # Use shrinkage
                                               for i in range(len(x)):
                                                     if i != iLo:
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                          December 16, 2009   15:4




               395     10.4 Downhill Simplex Method

                                                               x[i] = (x[i] - x[iLo])
                                                               f[i] = F(x[i])
                           print "Too many iterations in downhill"
                           print "Last values of x were"
                           return x[iLo]

                      EXAMPLE 10.6
                      Use the downhill simplex method to minimize

                                                   F = 10x12 + 3x22 − 10x1 x2 + 2x1

                      The coordinates of the vertices of the starting simplex are (0, 0), (0, −0.2), and (0.2, 0).
                      Show graphically the first four moves of the simplex.

                      Solution The figure shows the design space (the x1 -x2 plane). The numbers in the
                      figure are the values of F at the vertices. The move numbers are enclosed in circles.
                      The starting move (move 1) is a reflection, followed by an expansion (move 2). The
                      next two moves are reflections. At this stage, the simplex is still moving downhill.
                      Contraction will not start until move 8, when the simplex has surrounded the optimal
                      point at (−0.6, −1.0).

                                               -0.6        -0.4        -0.2        0
                                                                              0.12        0.2

                                                                               1
                                                                       0              0   0


                                                                                          -0.2
                                                                       2      -0.28
                                                               3
                                               -0.02               -0.4                   -0.4
                                                           4

                                                                                          -0.6


                                          -0.48                                           -0.8

                      EXAMPLE 10.7




                                                       θ                       θ          h

                                                                   b
P1: PHB

CUUS884-Kiusalaas    CUUS884-10     978 0 521 19132 6                                       December 16, 2009        15:4




           396      Introduction to Optimization

                        The figure shows the cross section of a channel carrying water. Use the downhill
                    simplex to determine h, b, and θ that minimize the length of the wetted perimeter
                    while maintaining a cross-sectional area of 8 m2 . (Minimizing the wetted perimeter
                    results in least resistance to the flow.) Check the answer by calculus.

                    Solution The cross-sectional area of the channel is
                                             1
                                        A=     b + (b + 2h tan θ ) h = (b + h tan θ)h
                                             2

                    and the length of the wetted perimeter is

                                                        S = b + 2(h sec θ)

                    The optimization problem is to minimize S subject to the constraint A − 8 = 0. Us-
                    ing the penalty function to take care of the equality constraint, the function to be
                    minimized is

                                                                                       2
                                         S∗ = b + 2h sec θ + µ (b + h tan θ)h − 8

                                                T                                      T
                        Letting x = b    h θ        and starting with x0 = 4   2   0       , we arrive at the fol-
                    lowing program:

                    #!/usr/bin/python
                    ## example10_7
                    from numpy import array
                    from math import cos,tan,pi
                    from downhill import *


                    def S(x):
                         global perimeter,area
                         mu = 10000.0
                         perimeter = x[0] + 2.0*x[1]/cos(x[2])
                         area = (x[0] + x[1]*tan(x[2]))*x[1]
                         return perimeter + mu*(area - 8.0)**2


                    xStart = array([4.0, 2.0, 0.0])
                    x = downhill(S,xStart)
                    area = (x[0] + x[1]*tan(x[2]))*x[1]
                    print "b = ",x[0]
                    print "h = ",x[1]
                    print "theta (deg) = ",x[2]*180.0/pi
                    print "area = ",area
                    print "perimeter = ",perimeter
                    raw_input("Finished. Press return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-10     978 0 521 19132 6                                        December 16, 2009   15:4




               397     10.4 Downhill Simplex Method

                            The results are

                      b =     2.4816069148
                      h =     2.14913738694
                      theta (deg) =           30.0000185796
                      area =      7.99997671775
                      perimeter =       7.44482803952


                      Check
                      Because we have an equality constraint, the problem can be solved by calculus with
                      help from a Lagrangian multiplier. Referring to Eqs. (10.2a), we have F = S and g =
                      A − 8, so that

                                               F ∗ = S + λ(A − 8)

                                                  = b + 2(h sec θ) + λ (b + h tan θ )h − 8

                      Therefore, Eqs. (10.2b) become
                                                 ∂F∗
                                                         = 1 + λh = 0
                                                  ∂b
                                                 ∂F∗
                                                         = 2 sec θ + λ(b + 2h tan θ) = 0
                                                 ∂h
                                                 ∂F∗
                                                         = 2h sec θ tan θ + λh2 sec2 θ = 0
                                                  ∂θ
                                                     g   = (b + h tan θ )h − 8 = 0

                      which can be solved with newtonRaphson2 as shown next.

                      #!/usr/bin/python
                      ## example10_7_check
                      from numpy import array,zeros
                      from math import tan,cos
                      from newtonRaphson2 import *


                      def f(x):
                            f = zeros(4)
                            f[0] = 1.0 + x[3]*x[1]
                            f[1] = 2.0/cos(x[2]) + x[3]*(x[0] + 2.0*x[1]*tan(x[2]))
                            f[2] = 2.0*x[1]*tan(x[2])/cos(x[2]) + x[3]*(x[1]/cos(x[2]))**2
                            f[3] = (x[0] + x[1]*tan(x[2]))*x[1] - 8.0
                            return f


                      xStart = array([3.0, 2.0, 0.0, 1.0])
                      print "x =",newtonRaphson2(f,xStart)
                      raw_input ("Press return to exit")
P1: PHB

CUUS884-Kiusalaas    CUUS884-10      978 0 521 19132 6                                                 December 16, 2009         15:4




           398      Introduction to Optimization

                                                               T
                        The solution x = b      h θ       λ        is

                    x = [ 2.48161296        2.14913986             0.52359878 -0.46530243]

                    EXAMPLE 10.8

                                                                                d2
                                             d1               θ1                                            θ2
                                                  L                                    L

                         The fundamental circular frequency of the stepped shaft is required to be higher
                    than ω0 (a given value). Use the downhill simplex to determine the diameters d1 and
                    d2 that minimize the volume of the material without violating the frequency con-
                    straint. The approximate value of the fundamental frequency can be computed by
                    solving the eigenvalue problem (obtainable from the finite element approximation)

                                4(d14 + d24 ) 2d24       θ1         4γ L 4 ω2       4(d12 + d22 )   −3d22        θ1
                                                               =
                                    2d24      4d24       θ2          105E             −3d22          4d22        θ2

                    where

                                               γ = mass density of the material

                                               ω = circular frequency

                                               E = modulus of elasticity

                                           θ 1 , θ 2 = rotations at the simple supports

                    Solution We start by introducing the dimensionless variables xi = di /d0 , where d0 is
                    an arbitrary “base” diameter. As a result, the eigenvalue problem becomes

                                    4(x14 + x24 ) 2x24        θ1              4(x12 + x22 ) −3x22           θ1
                                                                        =λ                                                 (a)
                                        2x24      4x24        θ2                −3x22        4x22           θ2

                    where
                                                                        4γ L 4 ω2
                                                               λ=
                                                                        105Ed02
                    In the program listed next we assume that the constraint on the frequency ω is equiv-
                    alent to λ ≥ 0.4.

                    ## example10_8
                    from numpy import array
                    from stdForm import *
                    from inversePower import *
                    from downhill import *
P1: PHB

CUUS884-Kiusalaas    CUUS884-10   978 0 521 19132 6                                    December 16, 2009   15:4




               399     10.4 Downhill Simplex Method

                      def F(x):
                            global eVal
                            mu = 1.0e6
                            eVal_min = 0.4
                            A = array([[4.0*(x[0]**4 + x[1]**4), 2.0*x[1]**4],                   \
                                          [2.0*x[1]**4, 4.0*x[1]**4]])
                            B = array([[4.0*(x[0]**2 + x[1]**2), -3.0*x[1]**2],                      \
                                          [-3*x[1]**2, 4.0*x[1]**2]])
                            H,t = stdForm(A,B)
                            eVal,eVec = inversePower(H,0.0)
                            return x[0]**2 + x[1]**2 + mu*(max(0.0,eVal_min - eVal))**2


                      xStart = array([1.0,1.0])
                      x = downhill(F,xStart,0.1)
                      print "x = ", x
                      print "eigenvalue = ",eVal
                      raw_input ("Press return to exit")



                           Although a 2 × 2 eigenvalue problem can be solved easily, we avoid the work in-
                      volved by employing functions that have been already prepared – stdForm to turn
                      the eigenvalue problem into standard form, and inversePower to compute the
                      eigenvalue closest to zero.
                            The results shown here were obtained with x1 = x2 = 1 as the starting values
                      and 106 for the penalty multiplier. The downhill simplex method is robust enough to
                      alleviate the need for multiple runs with increasing penalty multiplier.


                      x =    [ 1.07512696      0.79924677]
                      eigenvalue =     0.399997757238



                      PROBLEM SET 10.1

                       1.    The Lennard–Jones potential between two molecules is


                                                                  σ   12       σ   6
                                                        V = 4ε             −
                                                                  r            r


                          where ε and σ are constants, and r is the distance between the molecules. Use
                          the module goldSearch to find σ /r that minimizes the potential, and verify the
                          result analytically.
                       2.   One wave function of the hydrogen atom is


                                                      ψ = C 27 − 18σ + 2σ 2 e−σ /3
P1: PHB

CUUS884-Kiusalaas    CUUS884-10       978 0 521 19132 6                                              December 16, 2009   15:4




           400      Introduction to Optimization

                         where

                                                           σ = zr/a 0
                                                                                         2/3
                                                                       1             z
                                                          C =          √
                                                                     81 3π          a0
                                                           z = nuclear charge

                                                          a 0 = Bohr radius

                                                           r = radial distance

                         Find σ where ψ is at a minimum. Verify the result analytically.
                    3.     Determine the parameter p that minimizes the integral
                                                                 π
                                                                     sin x cos px dx
                                                             0

                         Hint: use numerical quadrature to evaluate the integral.
                    4.
                                                    R1 = 2Ω                       R 2 = 3.6Ω

                                               i1                             i2
                             E = 120 V                                        R                     R5 = 1.2Ω
                                                                     i1                        i2

                                                 R3 = 1.5Ω                        R4 = 1.8Ω

                         Kirchoff’s equations for the two loops of the electrical circuit are

                                                           R1i1 + R3i1 + R(i1 − i2 ) = E

                                                    R2i2 + R4i2 + R5i2 + R(i2 − i1 ) = 0

                         Find the resistance R that maximizes the power dissipated by R. Hint: Solve Kir-
                         choff’s equations numerically with one of the functions in Chapter 2.
                    5.


                                                                              a
                                                                                    r
                                                     T                    T



                         A wire carrying an electric current is surrounded by rubber insulation of outer
                         radius r . The resistance of the wire generates heat, which is conducted through
                         the insulation and convected into the surrounding air. The temperature of the
P1: PHB

CUUS884-Kiusalaas    CUUS884-10    978 0 521 19132 6                                      December 16, 2009   15:4




               401     10.4 Downhill Simplex Method

                          wire can be shown to be

                                                             q   ln(r/a)    1
                                                       T=                +       + T∞
                                                            2π      k      hr


                          where


                                        q = rate of heat generation in wire = 50 W/m

                                        a = radius of wire = 5 mm

                                        k = thermal conductivity of rubber = 0.16 W/m · K

                                        h = convective heat-transfer coefficient = 20 W/m2 · K

                                      T∞ = ambient temperature = 280 K


                          Find r that minimizes T .
                       6.   Minimize the function


                                                       F (x, y) = (x − 1)2 + (y − 1)2


                          subject to the constraints x + y ≥ 1 and x ≥ 0.6.
                       7.   Find the minimum of the function


                                                         F (x, y) = 6x 2 + y 3 + xy


                          in y ≥ 0. Verify the result analytically.
                       8.   Solve Prob. 7 if the constraint is changed to y ≥ −2.
                       9.   Determine the smallest distance from the point (1, 2) to the parabola y = x 2 .
                      10.


                                                                                      x
                                                                 0.2 m
                                                0.4 m            C
                                                                      d

                                                                  0.4 m

                          Determine x that minimizes the distance d between the base of the area shown
                          and its centroid C.
P1: PHB

CUUS884-Kiusalaas    CUUS884-10       978 0 521 19132 6                                 December 16, 2009       15:4




           402      Introduction to Optimization

                    11.
                                                                   r




                                                          C                         H
                                             x
                                                                         0.43H



                          The cylindrical vessel of mass M has its center of gravity at C. The water in the
                          vessel has a depth x. Determine x so that the center of gravity of the vessel–water
                          combination is as low as possible. Use M = 115 kg, H = 0.8 m, and r = 0.25 m.
                    12.
                                                               a
                                                          b

                                                  b
                                                                                 a




                          The sheet of cardboard is folded along the dashed lines to form a box with an
                          open top. If the volume of the box is to be 1.0 m3 , determine the dimensions a
                          and b that would use the least amount of cardboard. Verify the result analytically.
                    13.
                                                          a                     b
                                        A                                               C
                                                                         B
                                                                         v

                                                                             u B'
                                                                               P
                          The elastic cord A BC has an extensional stiffness k. When the vertical force P is
                          applied at B, the cord deforms to the shape A B C. The potential energy of the
P1: PHB

CUUS884-Kiusalaas    CUUS884-10     978 0 521 19132 6                                       December 16, 2009   15:4




               403     10.4 Downhill Simplex Method

                            system in the deformed position is
                                                              k(a + b)        k(a + b)
                                                 V = −Pv +             δA B +          δ BC
                                                                 2a              2b
                            where

                                                         δA B =    (a + u)2 + v 2 − a

                                                         δ BC =    (b − u)2 + v 2 − b

                            are the elongations of A B and BC. Determine the displacements u and v by min-
                            imizing V (this is an application of the principle of minimum potential energy: a
                            system is in stable equilibrium if its potential energy is at minimum). Use a = 150
                            mm, b = 50 mm, k = 0.6 N/mm, and P = 5 N.
                      14.
                                                                  b= 4 m
                                                        θ                            θ



                                                                      P = 50 kN
                            Each member of the truss has a cross-sectional area A . Find A and the angle θ
                            that minimize the volume
                                                                         bA
                                                                  V =
                                                                        cos θ
                            of the material in the truss without violating the constraints

                                                        σ ≤ 150 MPa         δ ≤ 5 mm

                            where
                                                 P
                                           σ =         = stress in each member
                                              2A sin θ
                                                     Pb
                                           δ=                   = displacement at the load P
                                              2E A sin 2θ sin θ
                          and E = 200 × 109 .
                      15.   Solve Prob. 14 if the allowable displacement is changed to 2.5 mm.
                      16.
                                                        r1                      r2

                                              L = 1.0 m            L = 1.0 m
                                                                                         P = 10 kN
                            The cantilever beam of circular cross section is to have the smallest volume pos-
                            sible subject to constraints

                                             σ 1 ≤ 180 MPa        σ 2 ≤ 180 MPa          δ ≤ 25 mm
P1: PHB

CUUS884-Kiusalaas    CUUS884-10        978 0 521 19132 6                                         December 16, 2009   15:4




           404      Introduction to Optimization

                          where
                                                 8P L
                                          σ1 =         = maximum stress in left half
                                                 πr 13
                                                 4P L
                                          σ2 =         = maximum stress in right half
                                                 πr 23
                                                 P L3       7    1
                                           δ=                  +        = displacement at free end
                                                 3π E      r 14 r 24
                          and E = 200 GPa. Determine r 1 and r 2 .
                    17.     Find the minimum of the function

                                              F (x, y, z) = 2x 2 + 3y 2 + z2 + xy + xz − 2y

                          and confirm the result analytically.
                    18.
                                                                        r

                                                                                  h

                                                                                  b

                          The cylindrical container has a conical bottom and an open top. If the volume V
                          of the container is to be 1.0 m3 , find the dimensions r , h, and b that minimize the
                          surface area S. Note that
                                                                        b
                                                           V = πr 2       +h
                                                                        3

                                                           S = πr 2h +        b2 + r 2

                    19.
                                                             3m


                                              4m                  2      1

                                                                             P = 200 kN
                                                              3
                                                                         P = 200 kN

                          The equilibrium equations of the truss shown are
                                                         4                   3
                                              σ 1A 1 +     σ 2A 2 = P          σ 2A 2 + σ 3A 3 = P
                                                         5                   5
P1: PHB

CUUS884-Kiusalaas    CUUS884-10      978 0 521 19132 6                                        December 16, 2009   15:4




               405     10.4 Downhill Simplex Method

                            where σ i is the axial stress in member i and A i are the cross-sectional areas. The
                            third equation is supplied by compatibility (geometrical constraints on the elon-
                            gations of the members):
                                                               16             9
                                                                  σ 1 − 5σ 2 + σ 3 = 0
                                                               5              5
                            Find the cross-sectional areas of the members that minimize the weight of the
                            truss without the stresses exceeding 150 MPa.
                      20.
                                                                          B
                                                    θ1

                                                    L1     y1
                                                                          y2                H
                                                                θ2
                                                     W1             L2
                                                                                     θ3
                                                                                L3
                                                                     W2

                            A cable supported at the ends carries the weights W1 and W2 . The potential en-
                            ergy of the system is

                                               V = −W1 y1 − W2 y2

                                                  = −W1 L 1 sin θ 1 − W2 (L 1 sin θ 1 + L 2 sin θ 2 )

                            and the geometric constraints are

                                                    L 1 cos θ 1 + L 2 cos θ 2 + L 3 cos θ 3 = B

                                                     L 1 sin θ 1 + L 2 sin θ 2 + L 3 sin θ 3 = H

                            The principle of minimum potential energy states that the equilibrium configu-
                            ration of the system is the one that satisfies geometric constraints and minimizes
                            the potential energy. Determine the equilibrium values of θ 1 , θ 2 , and θ 3 given that
                            L 1 = 1.2 m, L 2 = 1.5 m, L 3 = 1.0 m, B = 3.5 m, H = 0, W1 = 20 kN, and W2 = 30
                            kN.
                      21.
                                                                           2P
                                                                         v u
                                                                                P
                                                          L                          L
                                                                1         2     3
                                                         30o                        30o

                            The displacement formulation of the truss results in the equations
                                                              √        √
                                          E     3A + 3A         3A 1 + 3A 3       u       P
                                              √ 1 √ 3                                 =
                                         4L     3A 1 + 3A 3 A 1 + 8A 2 + A 3      v      2P
P1: PHB

CUUS884-Kiusalaas    CUUS884-10      978 0 521 19132 6                                        December 16, 2009   15:4




           406      Introduction to Optimization

                        where E is the modulus of elasticity, A i is the cross-sectional area of member i,
                        and u, v are the displacement components of the loaded joint. Letting A 1 = A 3 (a
                        symmetric truss), determine the cross-sectional areas that minimize the struc-
                        tural volume without violating the constraints u ≤ δ and v ≤ δ. Hint: nondimen-
                        sionalize the problem as in Example 10.5.
                    22.   Solve Prob. 21 if the three cross-sectional areas are independent.
                    23.    A beam of rectangular cross section is cut from a cylindrical log of diameter
                        d. Calculate the height h and the width b of the cross section that maximizes the
                        cross-sectional moment of inertia I = bh3 /12. Check the result by calculus.

          10.5 Other Methods

                    Simulated annealing methods have been successfully employed for complex prob-
                    lems involving many design variables. These methods are based on an analogy with
                    the annealing as a slowly cooled liquid metal solidifies into a crystalline, minimum
                    energy structure. One distinguishing feature of simulated annealing is its ability to
                    pass over local minima in its search for the global minimum.
                        A topic that we reluctantly omitted is the simplex method of linear programming.
                    Linear programming deals with optimization problems where the merit function and
                    the constraints are linear expressions of the independent variables. The general lin-
                    ear programming problem is to minimize the objective function
                                                                       n
                                                                F =         ai xi
                                                                      i=1
                    subject to the constraints
                                                  n
                                                        Bij x j ≤ bi , i = 1, 2, . . . , m1
                                                 j =1

                                                  n
                                                        Cij x j ≥ ci , i = 1, 2, . . . , m2
                                                 j =1

                                                  n
                                                        Dij x j = di , i = 1, 2, . . . , m3
                                                 j =1

                                                            xi ≥ 0, i = 1, 2, . . . n

                    where the constants bi , ci , and di are non-negative. The roots of linear programming
                    lie in cost analysis, operations research and related fields. We skip this topic because
                    there are very few engineering applications that can be formulated as linear program-
                    ming problems. In addition, a fail-safe implementation of the simplex method results
                    in a rather complicated algorithm. This is not to say that the simplex method has no
                    place in nonlinear optimization. There are several effective methods that rely in part
                    on the simplex method. For example, problems with nonlinear constraints can often
                    be solved by a piecewise application of linear programming. The simplex method is
                    also used to compute search directions in the method of feasible directions.
P1: PHB

CUUS884-Kiusalaas    CUUS884-App     978 0 521 19132 6                                            December 16, 2009     15:4




                      Appendices




              A1      Taylor Series
                      Function of a Single Variable
                      The Taylor series expansion of a function f (x) about the point x = a is the infinite
                      series
                                                                         (x − a)2         (x − a)3
                                 f (x) = f (a) + f (a)(x − a) + f (a)             + f (a)          + ···              (A1)
                                                                            2!               3!
                      In the special case a = 0, the series is also known as the MacLaurin series. It can be
                      shown that Taylor series expansion is unique in the sense that no two functions have
                      identical Taylor series.
                           The Taylor series is meaningful only if all the derivatives of f (x) exist at x = a and
                      the series converges. In general, convergence occurs only if x is sufficiently close to
                      a, that is, if |x − a| ≤ ε, where ε is called the radius of convergence. In many cases, ε is
                      infinite.
                           Another useful form of Taylor series is the expansion about an arbitrary value
                      of x:
                                                                                 h2        h3
                                        f (x + h) = f (x) + f (x)h + f (x)          + f (x) + · · ·                   (A2)
                                                                                 2!        3!
                      Because it is not possible to evaluate all the terms of an infinite series, the effect of
                      truncating the series in Eq. (A2) is of great practical importance. Keeping the first
                      n + 1 terms, we have

                                                                              h2                     hn
                                    f (x + h) = f (x) + f (x)h + f (x)           + · · · + f (n) (x)    + En          (A3)
                                                                              2!                     n!
                      where E n is the truncation error (sum of the truncated terms). The bounds on the
                      truncation error are given by Taylor’s theorem:

                                                                                hn+1
                                                         E n = f (n+1) (ξ )                                           (A4)
                                                                              (n + 1)!
                      where ξ is some point in the interval (x, x + h). Note that the expression for E n is
                      identical to the first discarded term of the series, but with x replaced by ξ . Because

               407
P1: PHB

CUUS884-Kiusalaas     CUUS884-App      978 0 521 19132 6                                                          December 16, 2009    15:4




           408      Appendices

                    the value of ξ is undetermined (only its limits are known), the most we can get out of
                    Eq. (A4) are the upper and lower bounds on the truncation error.
                         If the expression for f (n+1) (ξ ) is not available, the information conveyed by Eq.
                    (A4) is reduced to

                                                              E n = O(hn+1 )                                                    (A5)

                    which is a concise way of saying that the truncation error is of the order of hn+1 , or
                    behaves as hn+1 . If h is within the radius of convergence, then

                                                             O(hn ) > O(hn+1 )

                    that is, the error is always reduced if a term is added to the truncated series (this may
                    not be true for the first few terms).
                        In the special case n = 1, Taylor’s theorem is known as the mean value theorem:

                                              f (x + h) = f (x) + f (ξ )h,           x ≤ξ ≤x+h                                  (A6)


                    Function of Several Variables
                    If f is a function of the m variables x1 , x2 , . . . , xm , then its Taylor series expansion
                    about the point x = [x1 , x2 , . . . , xm ]T is
                                                       m                           m    m
                                                             ∂f               1                ∂2 f
                                 f (x + h) = f (x) +                   hi +                                  hi h j + · · ·     (A7)
                                                             ∂xi   x          2!              ∂xi ∂x j   x
                                                       i=1                         i=1 j =1

                    This is sometimes written as
                                                                                       1 T
                                          f (x + h) = f (x) + ∇ f (x) · h +              h H(x)h + . . .                        (A8)
                                                                                       2
                    The vector ∇ f is known as the gradient of f , and the matrix H is called the Hessian
                    matrix of f .

                    EXAMPLE A1
                    Derive the Taylor series expansion of f (x) = ln(x) about x = 1.

                    Solution The derivatives of f are
                                          1                   1                          2!                       3!
                                f (x) =          f (x) = −                f (x) =                  f (4) = −         etc.
                                          x                   x2                         x3                       x4
                    Evaluating the derivatives at x = 1, we get

                                 f (1) = 1        f (1) = −1            f (1) = 2!              f (4) (1) = −3! etc.

                    which, upon substitution into Eq. (A1) together with a = 1, yields

                                                      (x − 1)2      (x − 1)3       (x − 1)4
                                ln(x) = 0 + (x − 1) −          + 2!           − 3!          + ···
                                                         2!            3!             4!
                                                  1            1             1
                                       = (x − 1) − (x − 1)2 + (x − 1)3 − (x − 1)4 + · · ·
                                                  2            3             4
P1: PHB

CUUS884-Kiusalaas    CUUS884-App      978 0 521 19132 6                                                        December 16, 2009   15:4




               409     A1 Taylor Series

                      EXAMPLE A2
                      Use the first five terms of the Taylor series expansion of e x about x = 0:

                                                                          x2   x3   x4
                                                      ex = 1 + x +           +    +    + ···
                                                                          2!   3!   4!
                      together with the error estimate to find the bounds of e.

                      Solution
                                                                 1 1  1        65
                                               e =1+1+            + +   + E4 =    + E4
                                                                 2 6 24        24

                                                                         h5  eξ
                                                      E 4 = f (4) (ξ )      = , 0≤ξ ≤1
                                                                         5!  5!
                      The bounds on the truncation error are
                                                           e0    1                               e1    e
                                            (E 4 )min =       =                  (E 4 )max =        =
                                                           5!   120                              5!   120
                      Thus, the lower bound on e is
                                                                      65   1   163
                                                           emin =        +   =
                                                                      24 120   60
                      and the upper bound is given by
                                                                           65 emax
                                                                emax =        +
                                                                           24   120
                      which yields
                                                       119        65                           325
                                                           emax =                    emax =
                                                       120        24                           119
                      Therefore,
                                                                  163     325
                                                                      ≤e≤
                                                                  60      119
                      EXAMPLE A3
                      Compute the gradient and the Hessian matrix of

                                                               f (x, y) = ln        x2 + y 2

                      at the point x = −2, y = 1.

                      Solution

                                     ∂f        1           1      2x                      x          ∂f     y
                                        =                                       =                       = 2
                                     ∂x      x2   +   y2   2     x2   +   y2         x2   + y2       ∂y  x + y2

                                                                                                           T
                                               ∇ f (x, y) = x/(x 2 + y 2 )                y/(x 2 + y 2 )
                                                                                     T
                                              ∇ f (−2, 1) = −0.4               0.2
P1: PHB

CUUS884-Kiusalaas    CUUS884-App       978 0 521 19132 6                                           December 16, 2009   15:4




           410      Appendices

                                              ∂2 f   (x 2 + y 2 ) − x(2x)   −x 2 + y 2
                                                   =                      =
                                              ∂x 2        (x 2 + y 2 )2     (x 2 + y 2 )2
                                              ∂2 f   x2 − y 2
                                                   = 2
                                              ∂y 2  (x + y 2 )2
                                             ∂2 f    ∂2 f    −2xy
                                                  =       = 2
                                            ∂x∂y    ∂y∂x   (x + y 2 )2


                                                           −x 2 + y 2   −2xy            1
                                           H(x, y) =
                                                           −2xy         x2 − y 2   (x 2 + y 2 )2

                                                           −0.12    0.16
                                         H(−2, 1) =
                                                            0.16    0.12



          A2        Matrix Algebra

                    A matrix is a rectangular array of numbers. The size of a matrix is determined by the
                    number of rows and columns, also called the dimensions of the matrix. Thus, a matrix
                    of m rows and n columns is said to have the size m × n (the number of rows is always
                    listed first). A particularly important matrix is the square matrix, which has the same
                    number of rows and columns.
                         An array of numbers arranged in a single column is called a column vector, or
                    simply a vector. If the numbers are set out in a row, the term row vector is used. Thus,
                    a column vector is a matrix of dimensions n × 1, and a row vector can be viewed as a
                    matrix of dimensions 1 × n.
                         We denote matrices by boldface, uppercase letters. For vectors we use boldface,
                    lowercase letters. Here are examples of the notation:
                                                   ⎡                ⎤          ⎡ ⎤
                                                     A 11 A 12 A 13              b1
                                                   ⎢                ⎥          ⎢ ⎥
                                              A = ⎣ A 21 A 22 A 23 ⎦       b = ⎣ b2 ⎦                   (A9)
                                                     A 31 A 32 A 33              b3

                    Indices of the elements of a matrix are displayed in the same order as its dimen-
                    sions: The row number comes first, followed by the column number. Only one index
                    is needed for the elements of a vector.


                    Transpose
                    The transpose of a matrix A is denoted by AT and defined as

                                                               A ijT = A ji

                    The transpose operation thus interchanges the rows and columns of the matrix. If ap-
                    plied to vectors, it turns a column vector into a row vector and vice versa. For example,
P1: PHB

CUUS884-Kiusalaas    CUUS884-App    978 0 521 19132 6                                                December 16, 2009      15:4




               411     A2 Matrix Algebra

                      transposing A and b in Eq. (A9), we get
                                             ⎡                ⎤
                                               A 11 A 21 A 31
                                             ⎢                ⎥
                                       AT = ⎣ A 12 A 22 A 32 ⎦                    bT = b 1      b2    b3
                                               A 13 A 23 A 33

                          An n × n matrix is said to be symmetric if AT = A. This means that the elements in
                      the upper triangular portion (above the diagonal connecting A 11 and A nn ) of a sym-
                      metric matrix are mirrored in the lower triangular portion.


                      Addition
                      The sum C = A + B of two m × n matrices A and B is defined as

                                        Cij = A ij + Bij , i = 1, 2, . . . , m;         j = 1, 2, . . . , n              (A10)

                      Thus, the elements of C are obtained by adding elements of A to the elements of B.
                      Note that addition is defined only for matrices that have the same dimensions.


                      Vector Products
                      The dot or inner product c = a · b of the vectors a and b, each of size m, is defined as
                      the scalar
                                                                        m
                                                                  c=         a k bk                                      (A11)
                                                                       k=1


                      It can also be written in the form c = aT b. In NumPy, the function for the dot product
                      is dot(a,b)or inner(a,b).
                           The outer product C = a ⊗ b is defined as the matrix

                                                                   Cij = ai b j                                          (A12)

                      An alternative notation is C = abT . The NumPy function for the outer product is
                      outer(a,b).



                      Array Products
                      The matrix product C = AB of an l × m matrix A and an m × n matrix B is defined by
                                                m
                                        Cij =         A ik Bkj , i = 1, 2, . . . , l;   j = 1, 2, . . . , n              (A12)
                                                k=1

                      The definition requires the number of columns in A (the dimension m) to be equal to
                      the number of rows in B. The matrix product can also be defined in terms of the dot
                      product. Representing the ith row of A as the vector ai and the j th column of B as the
P1: PHB

CUUS884-Kiusalaas     CUUS884-App      978 0 521 19132 6                                     December 16, 2009     15:4




           412      Appendices

                    vector b j , we have
                                                    ⎡                                    ⎤
                                                    a1 · b1     a1 · b2   ···    a1 · bn
                                                  ⎢a · b        a2 · b2   ···    a2 · bn ⎥
                                                  ⎢ 2 1                                  ⎥
                                             AB = ⎢
                                                  ⎢ ..             ..     ..        .. ⎥ ⎥                (A13)
                                                  ⎣ .               .        .       . ⎦
                                                    a · b1      a · b2    ···    a · bn

                    NumPy treats the matrix product as the dot product for arrays, so that the function
                    dot(A,B) returns the matrix product of A and B.
                         NumPy defines the inner product of matrices A and B to be C = ABT . Equation
                    (A13) still applies, but now b represents the j th row of B.
                         NumPy’s definition of the outer product of matrices A (size k × ) and B (size m ×
                    n) is as follows. Let ai be the ith row of A, and let b j represent the j th row of B. Then
                    the outer product is of A and B is
                                                    ⎡                                    ⎤
                                                      a1 ⊗ b1 a1 ⊗ b2 · · · a1 ⊗ bm
                                                    ⎢a ⊗ b a ⊗ b ··· a ⊗ b ⎥
                                                    ⎢ 2      1    2    2          2    m⎥
                                          A⊗B=⎢     ⎢     ..        ..     ..       ..   ⎥
                                                                                         ⎥                 (A14)
                                                    ⎣      .         .        .      .   ⎦
                                                      ak ⊗ b1 ak ⊗ b2 · · · ak ⊗ bm

                    The submatrices ai ⊗ b j are of dimensions × n. As you can see, the size of the outer
                    product is much larger than either A or B.


                    Identity Matrix
                    A square matrix of special importance is the identity or unit matrix
                                                     ⎡                      ⎤
                                                        1 0 0 ··· 0
                                                     ⎢0 1 0 ··· 0⎥
                                                     ⎢                      ⎥
                                                     ⎢                      ⎥
                                                  I=⎢⎢  0  0   1   · · · 0  ⎥                             (A15)
                                                                            ⎥
                                                     ⎢ .. .. .. . .      .. ⎥
                                                     ⎣. . .            . . ⎦
                                                        0 0 0 0          1

                    It has the property AI = IA = A.


                    Inverse
                    The inverse of an n × n matrix A, denoted by A−1 , is defined to be an n × n matrix that
                    has the property

                                                           A−1 A = AA−1 = I                               (A16)


                    Determinant
                    The determinant of a square matrix A is a scalar denoted by |A| or det(A). There is no
                    concise definition of the determinant for a matrix of arbitrary size. We start with the
P1: PHB

CUUS884-Kiusalaas    CUUS884-App     978 0 521 19132 6                                             December 16, 2009       15:4




               413     A2 Matrix Algebra

                      determinant of a 2 × 2 matrix, which is defined as

                                                     A 11   A 12
                                                                 = A 11 A 22 − A 12 A 21                                (A17)
                                                     A 21   A 22

                      The determinant of a 3 × 3 matrix is then defined as

                              A 11   A 12   A 13
                                                        A 22      A 23        A 21        A 23        A 21       A 22
                              A 21   A 22   A 23 = A 11                − A 12                  + A 13
                                                        A 32      A 33        A 31        A 33        A 31       A 32
                              A 31   A 32   A 33

                      Having established the pattern, we can now define the determinant of an n × n ma-
                      trix in terms of the determinant of an (n − 1) × (n − 1) matrix:
                                                                 n
                                                         |A| =         (−1)k+1 A 1k M1k                                 (A18)
                                                                 k=1

                      where Mik is the determinant of the (n − 1) × (n − 1) matrix obtained by deleting the
                      ith row and kth column of A. The term (−1)k+i Mik is called a cofactor of A ik .
                            Equation (A18) is known as Laplace’s development of the determinant on the
                      first row of A. Actually, Laplace’s development can take place on any convenient row.
                      Choosing the ith row, we have
                                                                  n
                                                         |A| =         (−1)k+i A ik Mik                                 (A19)
                                                                 k=1

                          The matrix A is said to be singular if |A| = 0.


                      Positive Definiteness
                      An n × n matrix A is said to be positive definite if

                                                                  xT Ax > 0                                             (A20)

                      for all nonvanishing vectors x. It can be shown that a matrix is positive definite if the
                      determinants of all its leading minors are positive. The leading minors of A are the n
                      square matrices
                                            ⎡                       ⎤
                                               A 11 A 12 · · · A 1k
                                            ⎢A                      ⎥
                                            ⎢ 12 A 22 · · · A 2k ⎥
                                            ⎢.                      ⎥ , k = 1, 2, . . . , n
                                            ⎢.      ..     ..    .  ⎥
                                            ⎣.       .        . ..  ⎦
                                               A k1 A k2 · · · A kk

                      Therefore, positive definiteness requires that

                                                                      A 11   A 12   A 13
                                            A 11   A 12
                              A 11 > 0,                 > 0,          A 21   A 22   A 23 > 0, . . . , |A | > 0          (A21)
                                            A 21   A 22
                                                                      A 31   A 32   A 33
P1: PHB

CUUS884-Kiusalaas    CUUS884-App      978 0 521 19132 6                                        December 16, 2009    15:4




           414      Appendices

                    Useful Theorems
                    We list without proof a few theorems that are utilized in the main body of the text.
                    Most proofs are easy and could be attempted as exercises in matrix algebra.

                                          (AB)T = BT AT                                                    (A22a)
                                                −1       −1 −1
                                         (AB)        =B A                                                 (A22b)

                                            AT = |A|                                                       (A22c)

                                           |AB| = |A| |B|                                                 (A22d)

                                            if C = AT BA where B = BT , then C = CT                        (A22e)

                    EXAMPLE A4
                    Letting                ⎡               ⎤          ⎡     ⎤           ⎡  ⎤
                                            1        2   3                1              8
                                          ⎢                ⎥           ⎢    ⎥         ⎢    ⎥
                                      A = ⎣1         2   1⎦        u = ⎣ 6⎦       v = ⎣ 0⎦
                                            0        1   2               −2             −3

                    compute u + v, u · v, Av, and uT Av.

                    Solution
                                                               ⎡      ⎤ ⎡    ⎤
                                                                1+8        9
                                                             ⎢        ⎥ ⎢    ⎥
                                                     u + v = ⎣ 6 + 0⎦ = ⎣ 6⎦
                                                               −2 − 3     −5

                                               u · v = 1(8)) + 6(0) + (−2)(−3) = 14
                                           ⎡       ⎤ ⎡                       ⎤ ⎡     ⎤
                                             a1 ·v       1(8) + 2(0) + 3(−3)      −1
                                           ⎢       ⎥ ⎢                       ⎥ ⎢     ⎥
                                      Av = ⎣ a2 ·v ⎦ = ⎣ 1(8) + 2(0) + 1(−3) ⎦ = ⎣ 5 ⎦
                                             a3 ·v       0(8) + 1(0) + 2(−3)      −6

                                       uT Av = u · (Av) = 1(−1) + 6(5) + (−2)(−6) = 41

                    EXAMPLE A5
                    Compute |A|, where A is given in Example A4. Is A positive definite?

                    Solution Laplace’s development of the determinant on the first row yields

                                                          2    1    1     1    1    2
                                            |A| = 1              −2         +3
                                                          1    2    0     2    0    1
                                                     = 1(3) − 2(2) + 3(1) = 2

                    Development on the third row is somewhat easier because of the presence of the zero
                    element:
                                                          2    3    1     3    1    2
                                            |A| = 0              −1         +2
                                                          2    1    1     1    1    2
                                                     = 0(−4) − 1(−2) + 2(0) = 2
P1: PHB

CUUS884-Kiusalaas    CUUS884-App     978 0 521 19132 6                                        December 16, 2009   15:4




               415     A2 Matrix Algebra

                         To verify positive definiteness, we evaluate the determinants of the leading
                      minors:

                                                            A 11 = 1 > 0    O.K.

                                                    A 11   A 12   1       2
                                                                =           =0     Not O.K.
                                                    A 21   A 22   1       2

                      A is not positive definite.

                      EXAMPLE A6
                      Evaluate the matrix product AB, where A is given in Example A4 and
                                                            ⎡          ⎤
                                                              −4     1
                                                            ⎢          ⎥
                                                        B = ⎣ 1 −4 ⎦
                                                               2 −2
                      Solution
                                     ⎡               ⎤
                                       a1 ·b1 a1 ·b2
                                     ⎢               ⎥
                               AB = ⎣ a2 ·b1 a2 ·b2 ⎦
                                       a3 ·b1 a3 ·b2
                                     ⎡                                          ⎤ ⎡         ⎤
                                       1(−4) + 2(1) + 3(2) 1(1) + 2(−4) + 3(−2)       4 −13
                                     ⎢                                          ⎥ ⎢         ⎥
                                   = ⎣ 1(−4) + 2(1) + 1(2) 1(1) + 2(−4) + 1(−2) ⎦ = ⎣ 0  −9 ⎦
                                       0(−4) + 1(1) + 2(2) 0(1) + 1(−4) + 2(−2)       5  −8
                      EXAMPLE A7
                      Compute A ⊗ b, where

                                                             5 −2                    1
                                                      A=                     b=
                                                            −3  4                    3

                      Solution
                                                             a1 ⊗ b
                                                A⊗b =
                                                             a2 ⊗ b

                                                              5                   5 15
                                               a1 ⊗ b =               1   3 =
                                                             −2                  −2 −6

                                                             −3                −3 −9
                                               a2 ⊗ b =               1   3 =
                                                              4                  4 12
                                                                      ⎡         ⎤
                                                                    5        15
                                                                 ⎢ −2        −6 ⎥
                                                                 ⎢              ⎥
                                                           ∴ A⊗b=⎢              ⎥
                                                                 ⎣ −3        −9 ⎦
                                                                    4        12
P1: PHB

CUUS884-Kiusalaas    CUUS884-Program    978 0 521 19132 6                                December 16, 2009   15:4




                    List of Program Modules (by Chapter)




                    Chapter 1

                    1.7   error                    Error handling routine



                    Chapter 2

                    2.2   gaussElimin     Gauss elimination
                    2.3   LUdecomp        LU decomposition
                    2.3   choleski        Choleski decomposition
                    2.4   LUdecomp3       LU decomposition of tridiagonal matrices
                    2.4   LUdecomp5       LU decomposition of pentadiagonal matrices
                    2.5   swap            Interchanges rows or columns of a matrix
                    2.5   gaussPivot      Gauss elimination with row pivoting
                    2.5   LUpivot         LU decomposition with row pivoting
                    2.7   gaussSeidel     Gauss–Seidel method with relaxation
                    2.7   conjGrad        Conjugate gradient method



                    Chapter 3

                    3.2   newtonPoly      Newton’s method of polynomial interpolation
                    3.2   neville         Neville’s method of polynomial interpolation
                    3.2   rational        Rational function interpolation
                    3.3   cubicSpline     Cubic spline interpolation
                    3.4   polyFit         Polynomial curve fitting



                    Chapter 4

                    4.2   rootsearch     Brackets a root of an equation
                    4.3   bisection      Method of bisection

           416
P1: PHB

CUUS884-Kiusalaas    CUUS884-Program     978 0 521 19132 6                                December 16, 2009   15:4




               417     List of Program Modules (by Chapter)

                        4.4   ridder                Ridder’s method
                        4.5   newtonRaphson         Newton–Raphson method
                        4.6   newtonRaphson2        Newton–Raphson method for systems of equations
                        4.7   evalPoly              Evaluates a polynomial and its derivatives
                        4.7   polyRoots             Laguerre’s method for roots of polynomials



                      Chapter 6

                        6.2   trapezoid          Recursive trapezoidal rule
                        6.3   romberg            Romberg integration
                        6.4   gaussNodes         Nodes and weights for Gauss-Legendre quadrature
                        6.4   gaussQuad          Gauss–Legendre quadrature
                        6.5   gaussQuad2         Gauss–Legendre quadrature over a quadrilateral
                        6.5   triangleQuad       Gauss–Legendre quadrature over a triangle



                      Chapter 7

                        7.2   taylor         Taylor series method for solution of initial value problems
                        7.2   printSoln      Prints solution of initial value problem in tabular form
                        7.3   run kut4       Fourth-order Runge–Kutta method
                        7.5   run kut5       Adaptive (fifth-order) Runge–Kutta method
                        7.6   midpoint       Midpoint method with Richardson extrapolation
                        7.6   bulStoer       Simplified Bulirsch–Stoer method



                      Chapter 8

                      8.2     linInterp        Linear interpolation
                      8.2     example8 1       Shooting method example for second-order
                                                 differential eqs.
                      8.2     example8 3       Shooting method example for third-order linear
                                                 differential eqs.
                      8.2     example8 4       Shooting method example for fourth-order
                                                 differential eqs.
                      8.2     example8 5       Shooting method example for fourth-order
                                                 differential eqs.
                      8.3     example8 6       Finite difference example for second-order linear
                                                 differential eqs.
                      8.3     example8 7       Finite difference example for second-order
                                                 differential eqs.
                      8.4     example8 8       Finite difference example for fourth-order linear
                                                 differential eqs.
P1: PHB

CUUS884-Kiusalaas    CUUS884-Program      978 0 521 19132 6                                 December 16, 2009   15:4




           418      List of Program Modules (by Chapter)

                    Chapter 9

                     9.2    jacobi         Jacobi’s method
                     9.2    sortJacobi     Sorts eigenvectors in ascending order of eigenvalues
                     9.2    stdForm        Transforms eigenvalue problem into standard form
                     9.3    inversePower       Inverse power method with eigenvalue shifting
                     9.3    inversePower5      As above for pentadiagonal matrices
                     9.4    householder        Householder reduction to tridiagonal form
                     9.5    sturmSeq           Sturm sequence for tridiagonal matrices
                     9.5    gerschgorin        Computes global bounds on eigenvalues
                     9.5    lamRange           Brackets m smallest eigenvalues of a tridiagonal matrix
                     9.5    eigenvals3         Finds m smallest eigenvalues of a tridiagonal matrix
                     9.5    inversePower3      Inverse power method for tridiagonal matrices



                    Chapter 10

                     10.2   goldSearch      Golden section search for the minimum of a function
                     10.3   powell          Powell’s method of minimization
                     10.4   downhill        Downhill simplex method of minimization


                    Available on Website
                       xyPlot                Unsophisticated plotting routine
                       plotPoly              Plots data points and the fitting polynomial
P1: PHB

CUUS884-Kiusalaas    CUUS884-Program        978 0 521 19132 6                                       December 16, 2009        15:4




                      Index




                      adaptive Runge–Kutta method, 275–283             eigenvals3, 364
                      arithmetic operators, in Python, 6               eigenvalue problems. See symmetric matrix
                      arrays                                                     eigenvalue problems
                        accessing/changing, 20                         elementary operations, linear algebra, 30
                        copying, 23                                    embedded integration formula, 269
                        creating, 19                                   equivalent linear equation, 30
                        functions, 21–22                               error
                        operations on, 20–21                             control in Python, 14–15
                      augmented assignment operators, 7                  in finite difference approximations, 181–182
                      augmented coefficient matrix, 28                 Euhler’s method, 250
                                                                       evalPoly, 168–169
                      backward finite difference approximations,       evaluation of polynomials, 167–169
                               179                                     exponential functions, fitting, 129–130
                      banded matrix, 54–63
                      bisection, 142–143                               false position method, roots of equations, 145–146
                      bisection method, for equation root, 142–145     finite difference approximations, 177–181
                      Brent’s method, 175                                 errors in, 181–182
                      Bulirsch–Stoer method, 278–279, 283                 first central difference approximations, 178–179
                        algorithm, 280–284                                first noncentral, 179–180
                        midpoint method, 277–278                          second noncentral, 180–181
                        Richardson extrapolation, 278–279              finite elements, 228
                      bulStoer, 281                                    first central difference approximations, 182–183
                                                                       fourth-order Runge–Kutta method, 252–253
                      choleski(a), 46–47                               functions, in Python, 15–16
                      Choleski’s decomposition, 44–47
                      cmath module, 18                                 gaussElimin, 37
                      coefficient matrices, symmetric/banded, 54–63    Gauss elimination method, 33–38
                        symmetric, 57–58                                 algorithm for, 35–38
                        symmetric/pentadiagonal, 58–61                     back substitution phase, 36
                        tridiagonal, 55–57                                 elimination phase, 35–36
                      comparison operators, in Python, 7                 multiple sets of equations, 37, 38
                      composite Simpson’s 1/3 rule, 148                Gauss elimination with scaled row pivoting, 65–68
                      composite trapezoidal rule, 195–197              Gaussian integration, 211–221
                      conditionals, in Python, 8                         abscissas/weights for Guaussian quadratures,
                      conjGrad, 86, 87                                          216–219
                      conjugate gradient method, 84–87                     Gauss–Chebyshev quadrature, 217
                        conjugate directions, 383–384                      Gauss–Hermite quadrature, 218
                        Powell’s method, 382–387                           Gauss–Laguerre quadrature, 217–218
                      continuation character, 6                            Gauss–Legendre quadrature, 216–217
                      cubicSpline, 117–118                                 Gauss quadrature with logarithmic singularity,
                      cubic splines, 114–118, 195                               219
                      curve fitting. See interpolation/curve fitting     determination of nodal abscissas/weights,
                                                                                214–216
                      deflation of polynomials, 169                      orthogonal polynomials, 212–214
                      diagonal dominance, 65                           Gauss–Jordan elimination, 31–32
                      docstring, 26                                    Gauss–Legendre quadrature over quadrilateral
                      Doolittle’s decomposition, 41–44                          element, 228–231
                      downhill simplex method, 392–395                 gaussNodes, 219–220
                      downhill, 393–395                                gaussPivot, 67–68




               419
P1: PHB

CUUS884-Kiusalaas     CUUS884-Program         978 0 521 19132 6                                        December 16, 2009   15:4




           420      Index

                    gaussQuad, 220–221                               Jacobi method, 321–327
                    gaussQuad2, 230–231                                Jacobi diagonalization, 323–326
                    gaussSeidel, 84                                    Jacobi rotation, 322–323
                    Gauss–Seidel method, 82–84                         similarity transformation, 322
                    gerschgorin, 361                                   transformation to standard form, 328–330
                    Gerschgorin’s theorem, 361                       Jenkins–Traub algorithm, 176
                    golden section search, 377–379
                    goldSearch, 378–379                              knots of spline, 115
                                                                     Lagrange’s method, 99–101
                    Higher-order equations, shooting method, 296     Laguerre’s method, 169–171
                    householder, 355–356
                                                                     lamRange, 362–363
                      householder reduction to tridiagonal form,     least-squares fit, 124–135
                            351–356                                     fitting linear forms, 125–126
                        accumulated transformation matrix, 354–355      fitting straight line, 125
                        householder matrix, 351–352                     polynomial fit, 126–128
                        householder reduction of symmetric matrix,      weighting data, 128–130
                            352–359                                        fitting exponential functions, 129–130
                                                                           weighted linear regression, 128–129
                    Idle (code editor), 3                            linear algebraic equations systems. See also matrix
                    ill-conditioning, 28–29                                      algebra
                    incremental search method, roots of equations,      back substitution, 32
                               140–141                                  direct methods overview, 31–33
                    indirect methods. See iterative methods             elementary operations, 30
                    initial value problems                              equivalent equations, 30
                       adaptive Runge–Kutta method, 269–273             forward substitution, 32
                       Bulirsch–Stoer method, 277–281                   Gauss elimination method, 33–40
                         algorithm, 280–281                                algorithm for, 35–37
                         midpoint method, 277–278                             back substitution phase, 36
                         Richardson extrapolation, 278–279                    elimination phase, 35–36
                       multistep methods, 289                              multiple sets of equations, 37–38
                       Runge–Kutta methods, 249–253                     ill-conditioning, 28–30
                         fourth-order, 252–253                          LU decomposition methods, 40–47
                         second-order, 250–252                             Choleski’s decomposition, 44–47
                       stability/stiffness, 266–268                        Doolittle’s decomposition, 41–44
                         stability of Euhler’s method, 266–267          matrix inversion, 79–80
                         stiffness, 267–268                             pivoting, 64–70
                       Taylor series method, 244–246                       diagonal dominance, 65
                    Input/output                                           Gauss elimination with scaled row pivoting,
                       printing, 12–13                                           65–68
                       reading, 13–14                                      when to pivot, 70
                       writing, 14                                      QR decomposition, 98
                    integration order, 229                              singular value decomposition, 98
                    interpolation, derivatives by, 185–186              symmetric/banded coefficient matrices, 54–61
                       cubic spline interpolant, 186                       symmetric coefficient, 59–60
                       polynomial interpolant, 185–186                     symmetric/pentadiagonal coefficient, 58–61
                    interpolation/curve fitting                            tridiagonal coefficient, 55–57
                       interpolation with cubic spline, 114–118         uniqueness of solution, 28
                       least–squares fit, 124–129                    linear forms, fitting, 125–126
                         fitting a straight line, 125                linear systems, 30
                         fitting linear forms, 125–126               linInterp, 292
                         polynomial fit, 126–128                     lists, 5–6
                         weighting of data, 128–130                  loops, 8–10
                            fitting exponential functions, 129–130   LR algorithm, 373
                            weighted linear regression, 128–129      LUdecomp, 43–44
                       polynomial interpolation, 99–107              LUdecomp3, 56–57
                         Lagrange’s method, 99–101                   LUdecomp5, 61
                         limits of, 106–107                          LU decomposition methods, 40–49
                         Neville’s method, 104–106                      Choleski’s decomposition, 44–47
                         Newton’s method, 101–103                       Doolittle’s decomposition, 41–44
                       rational function interpolation, 110–112      LUpivot, 68–70
                    interval halving method. See bisection method
                    inversePower, 340                                mathematical functions, 11
                    inversePower3, 365–366                           math module, 17–18
                    iterative methods, 85–96                         MATLAB, 2–3
                       conjugate gradient method, 84–87              matrix algebra, 410–415
                       Gauss–Seidel method, 82–84                     addition, 411
                                                                      determinant, 412–413
                    jacobi, 326–327                                   inverse, 412
                    Jacobian matrix, 230                              multiplication, 411–412
P1: PHB

CUUS884-Kiusalaas    CUUS884-Program        978 0 521 19132 6                                          December 16, 2009   15:4




               421     Index

                        positive definiteness, 413                           Newton–Cotes formulas, 194–199
                        transpose, 410                                         composite trapezoidal rule, 195–197
                        useful theorems, 414                                   recursive trapezoidal rule, 197–198
                      matrix inversion, 79–80                                  Simpson’s rules, 198–199
                      methods of feasible directions, 406                      trapezoidal rule, 195
                      midpoint, 277–278                                      Romberg integration, 202–205
                      minimization along line, 376–379
                        bracketing, 376–377                                operators
                        golden section search, 377–379                       arithmetic, 6–7
                      modules, in Python, 16–17                              comparison, 7
                      multiple integrals, 227                              optimization
                        Gauss–Legendre quadrature over quadrilateral            conjugate directions, 383–384
                               element, 228–231                                 Powell’s method, 382–387
                        quadrature over triangular element, 234–237          minimization along line, 376–379
                      multistep methods, for initial value problems, 289        bracketing, 376–377
                                                                                golden section search, 377–379
                      Namespace, 24                                          Nelder–Mead method. See simplex method
                      natural cubic spline, 115                              simplex method, 392–395
                      Nelder–Mead method, 392                                simulated annealing method, 406
                      neville, 105                                         orthogonal polynomials, 212–214
                      Neville’s method, 104–105                            relaxation factor, 83
                      Newton–Cotes formulas, 194–199
                        composite trapezoidal rule, 195–197                pivoting, 64
                        recursive trapezoidal rule, 197                      diagonal dominance, 65–70
                        Simpson’s rules, 198–199                             Gauss elimination with scaled row pivoting,
                        trapezoidal rule, 195                                        65–68
                      newtonPoly, 103                                        when to pivot, 70
                      newtonRaphson, 152                                   polyFit, 127–128
                      newtonRaphson2, 156–157                              polynomial fit, 126–128
                      Newton–Raphson method, 150–152, 155–157              polynomial interpolation, 99–107
                      norm of matrix, 29                                     Lagrange’s method, 99–101
                      notation, 27–28                                        limits of, 106–107
                      numpy module, 18–24                                    Neville’s method, 104–106
                        accessing/changing array, 20                         Newton’s method, 101–103
                        array functions, 21–22                             polynomials, zeroes of, 166–172
                        copying arrays, 23                                   deflation of polynomials, 169
                        creating an array, 19                                evaluation of polynomials, 167–169
                        linear algebra module, 22–23                       Laguerre’s method, 169–172
                        operations on arrays, 20–21                        polyRoots, 171–172
                        vecturization, 23                                  powell, 386–387
                      numerical differentiation                            Powell’s method, 382–387
                        derivatives by interpolation, 185–186              printing input, 12–13
                           cubic spline interpolant, 186                   printSoln, 246
                           polynomial interpolant, 185–186                 Python
                        finite difference approximations, 177–182            arithmetic operators, 6–7
                           errors in, 181–182                                cmath module, 18–19
                           first central difference approximations,          comparison operators, 7–8
                                 178–179                                     conditionals, 8
                           first noncentral, 179–180                         error control, 14–15
                           second noncentral, 180–181                        functions, 15–16
                        Richardson extrapolation, 182–183                    general information, 1–3
                      numerical instability, 257                                obtaining Python, 3
                      numerical integration                                     overview, 1–3
                        Gaussian integration, 211–221                        linear algebra module, 22–23
                           abscissas/weights for Guaussian quadratures,      lists, 5–6
                                 216–219                                     loops, 8–10
                              Gauss–Chebyshev quadrature, 217                mathematical functions, 11
                              Gauss–Hermite quadrature, 218                  math module, 17–18
                              Gauss–Laguerre quadrature, 217–218             modules, 16–17
                              Gauss–Legendre quadrature, 216–217             numpy module, 18–24
                              Gauss quadrature with logarithmic                 accessing/changing array, 21
                                 singularity, 219                               array functions, 21–22
                           determination of nodal abscissas/weights,            copying arrays, 23
                                 214–216                                        creating an array, 19
                           orthogonal polynomials, 212–214                      operations on arrays, 20–21
                        multiple integrals, 227–237                          printing output, 12–13
                           Gauss–Legendre quadrature over quadrilateral      reading input, 11–12
                                 element, 228–231                            scoping of variables, 24–25
                           quadrature over triangular element, 234–237       strings, 4
P1: PHB

CUUS884-Kiusalaas     CUUS884-Program          978 0 521 19132 6                                       December 16, 2009   15:4




           422      Index

                    Python (Continued )                              symmetric/banded coefficient matrices, 54–62
                      tuples, 4–5                                      symmetric coefficient matrix, 57–58
                      type conversion, 10–11                           symmetric/pentadiagonal coefficient, 61–66
                      variables, 3–4                                   tridiagonal coefficient, 55–57
                      vectorization, 23–24                           symmetric matrix eigenvalue problems
                      writing/running programs, 25–26                  eigenvalues of symmetric tridiagonal matrices,
                    Python interpreter, 1                                     358–366
                                                                          bracketing eigenvalues, 362–363
                    QR algorithm, 380                                     computation of eigenvalues, 364
                    quadrature. See numerical integration                 computation of eigenvectors, 365–366
                    quadrature over triangular element, 240–245           Gerschgorin’s theorem, 361
                                                                          Strum sequence, 358–360
                    rational function interpolation, 110–112           householder reduction to tridiagonal form,
                    reading input, 11–12                                      351–356
                    recursive trapezoidal rule, 197–198                   accumulated transformation matrix, 354–355
                    relaxation factor, 89                                 householder matrix, 351–352
                    Richardson extrapolation, 182–183, 278–279            householder reduction of symmetric matrix,
                    Ridder’s method, 146–150                                  352–354
                    ridder, 147–148                                    inverse power/power methods, 337–340
                    romberg, 204–205                                      eigenvalue shifting, 338–339
                    Romberg integration, 202–205                          inverse power method, 337–339
                    rootsearch, 141                                       power method, 339
                    roots of equations                                 Jacobi method, 321–330
                      Brent’s method, 175                                 Jacobi diagonalization, 323–328
                      false position method, 145                          Jacobi rotation, 322–323
                      incremental search method, 140–141                  similarity transformation/diagonalization,
                      Jenkins–Traub algorithm, 176                            321–322
                      method of bisection, 142–143                        transformation to standard form, 328–330
                      Newton–Raphson method, 153–158                   LR algorithm, 373
                    Ridder’s method, 146–150                           QR algorithm, 373
                      secant method, 145                               Shur’s factorization, 373
                      systems of equations, 155–157                  symmetric/pentadiagonal coefficient matrix, 58–61
                         Newton–Raphson method, 155–157              synthetic division, 169
                      zeroes of polynomials, 166–172                 systems of equations
                         deflation of polynomials, 169                 Newton–Raphson method, 155–157
                         evaluation of polynomials, 167–168
                         Laguerre’s method, 169–172                  taylor, 245–246
                    Runge–Kutta–Fehlberg formulas, 270               Taylor series, 244, 407–408
                    Runge–Kutta methods, 249–253                        function of several variables, 408
                      fourth-order, 252–253                             function of single variable, 407–408
                      second-order, 250–251                          transpose, 410
                    run kut4, 252–253                                trapezoid, 197–198
                    run kut5, 272–273                                trapezoidal rule, 195
                                                                     triangleQuad, 236–237
                    scaled row pivoting, 65–68                       tridiagonal coefficient matrix, 55–57
                    second noncentral finite difference              tuples, 5
                               approximations, 180–181               two-point boundary value problems
                    second-order Runge–Kutta method, 250–251            finite difference method, 305–314
                    shape functions, 229                                   fourth-order differential equation, 310–314
                    shooting method, 291–296                               second-order differential equation, 306–310
                       higher-order equations, 296                      shooting method, 291–301
                       second-order differential equation, 291–292         higher-order equations, 296–301
                    Shur’s factorization, 373                              second-order differential equation, 291–296
                    similarity transformation, 322                   type(a), 12
                    Simpson’s 3/8 rule, 199                          type conversion, 10–11
                    Simpson’s rules, 198–199
                    slicing operator, 3                              variables
                    sortJacobi, 327–328                                Python, 3–4
                    sparsely populated matrix, 54                      scoping, 24–25
                    stability/stiffness, 266–268                     vectorizing, 23–24
                       stability of Euler’s method, 266–267
                       stiffness, 267–268                            weighted linear regression, 128–129
                    stdForm, 329–330                                 writing/running programs, in Python, 25–26
                    straight line, fitting, 125
                    strings, 5                                       zeroes of polynomials, 166–172
                    Strum sequence, 358–360                            deflation of polynomials, 169
                    sturmSeq, 359–360                                  evaluation of polynomials, 167–169
                    swapCols, 67                                       Laguerre’s method, 169–172
                    swapRows, 67                                     zero offset, 3
